[
    {
        "uri": "/about",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: about us\n---\nVamp is developed by Magnetic.io in the heart of Amsterdam.  \nWe’re dedicated to helping companies of all sizes increase efficiency, save time and reduce costs. We provide powerful and easy-to-use solutions to optimise systems.\n\nVision\nTime is precious.  \nWe believe people should focus their efforts on the things they excel at and that matter to them.  \nLet computers do the repeatable and automatable tasks - they’re far more suited to that than us.\n\n Community\nWe encourage anyone to join the Vamp community and pitch in with pull requests, bug reports etc. to make Vamp even more awesome.\n",
        "tags": []
    },
    {
        "uri": "/contact",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: contact\n---\n\nVamp is being developed by Magnetic.io in the heart of Amsterdam.\n\nOur Amsterdam offices:\n\nSint Antoniesbreestraat 16  \n1011 HB Amsterdam  \nThe Netherlands  \n+31(0)88 555 33 99  \ninfo@magnetic.io\n\nProfessional services and consultancy\nWe can provide professional services and consultancy around the implementation and use of Vamp. Send us an email on info@magnetic.io or call +31(0)88 555 33 99.\n\n Vamp Enterprise Edition (EE)\nWe also provide a commercial Enterprise Edition of Vamp with features specifically tuned to enterprise usage. Contact us to get more information.\n\nSupport\nCheck our support page for SLA's and other forms of support.\n\n Community\nJoin the Vamp community to make Vamp even better.\n",
        "tags": []
    },
    {
        "uri": "/documentation/api/api-reference",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: API Reference\n---\n\nThis page gives full details of all available API calls. See using the Vamp API for details on pagination, json and yaml content types and effective use of the API.\n\nBlueprints\n\n List blueprints\n\nLists all blueprints without any pagination or filtering.\n\n    GET /api/v1/blueprints\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. 400 Bad Request in case some breeds are not yet fully defined.\n| only_references   | true or false     | false            | all breeds will be replaced with their references.\n\nGet a single blueprint\n\nLists all details for one specific blueprint.\n\n    GET /api/v1/blueprints/{blueprint_name}\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. 400 Bad Request in case some breeds are not yet fully defined.\n| only_references   | true or false     | false            | all breeds will be replaced with their references.\n\n Create blueprint\n\nCreates a new blueprint. Accepts JSON or YAML formatted blueprints. Set the Content-Type request header to application/json or application/x-yaml accordingly.\n\n    POST /api/v1/blueprint\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 201 Created if the blueprint is valid.This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON.     \n\nUpdate a blueprint\n\nUpdates the content of a specific blueprint.\n\n    PUT /api/v1/blueprints/{blueprint_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 200 OK if the blueprint is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a blueprint\n\nDeletes a blueprint.        \n\n    DELETE /api/v1/blueprints/{blueprint_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the blueprint.\n\n------------\n\nBreeds\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\n List breeds\n\nLists all breeds without any pagination or filtering.\n\n    GET /api/v1/breeds\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed dependencies will be replaced (recursively) with full breed definitions. 400 Bad Request in case some dependencies are not yet fully defined.\n| only_references   | true or false     | false            | all full breed dependencies will be replaced with their references.\n\nGet a single breed\n\nLists all details for one specific breed.\n\n    GET /api/v1/breeds/{breed_name}\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed dependencies will be replaced (recursively) with full breed definitions. 400 Bad Request in case some dependencies are not yet fully defined.\n| only_references   | true or false     | false            | all full breed dependencies will be replaced with their references.\n\n Create breed\n\nCreates a new breed. Accepts JSON or YAML formatted breeds. Set the Content-Type request header to application/json or application/x-yaml accordingly.    \n\n    POST /api/v1/breeds\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 201 Created if the breed is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate a breed\n\nUpdates the content of a specific breed.\n\n    PUT /api/v1/breeds/{breed_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the breed and returns a 200 OK if the breed is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a breed\n\nDeletes a breed.        \n\n    DELETE /api/v1/breeds/{breed_name}\n    \n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the breed.\n\n---------\n\nConditions\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\n List conditions\n\nLists all conditions without any pagination or filtering.\n\n    GET /api/v1/conditions\n\nGet a single condition\n\nLists all details for one specific condition.\n\n    GET /api/v1/conditions/{condition_name}\n\n Create condition\n\nCreates a new condition. Accepts JSON or YAML formatted conditions. Set the Content-Type request header to application/json or application/x-yaml accordingly.\n\n    POST /api/v1/conditions\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 201 Created if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate a condition\n\nUpdates the content of a specific condition.\n\n    PUT /api/v1/conditions/{condition_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 200 OK if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a condition\n\nDeletes a condition.\n\n    DELETE /api/v1/conditions/{condition_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the escalation.\n\n-------------\n\nDeployments\n\nDeployments are non-static entities in the Vamp eco-system. They represent runtime structures so any changes to them will take time to execute and can possibly fail. Most API calls to the /deployments endpoint will therefore return a 202: Accepted return code, indicating the asynchronous nature of the call.\n\nDeployments have a set of sub resources: SLA's, scales and gateways. These are instantiations of their static counterparts.\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\n List deployments\n\n\tGET /api/v1/deployments\n\n| parameter         | options           | default          | description      |\n| ----------------- |:-----------------:|:----------------:| ----------------:|\n| as_blueprint      | true or false     | false            | exports each deployment as a valid blueprint. This can be used together with the header Accept: application/x-yaml to export in YAML format instead of the default JSON. |\n| expandreferences | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. It will be applied only if ?asblueprint=true.\n| onlyreferences  | true or false     | false            | all breeds will be replaced with their references. It will be applied only if ?asblueprint=true.\n\nGet a single deployment\n\nLists all details for one specific deployment.\n\n    GET /api/v1/deployments/{deployment_name}\n\n| parameter         | options           | default          | description      |\n| ----------------- |:-----------------:|:----------------:| ----------------:|\n| as_blueprint     | true or false     | false            | exports the deployment as a valid blueprint. This can be used together with the header Accept: application/x-yaml to export in YAML format instead of the default JSON. |\n| expandreferences | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. It will be applied only if ?asblueprint=true.\n| onlyreferences   | true or false     | false            | all breeds will be replaced with their references. It will be applied only if ?asblueprint=true.\n\n Create deployment using a blueprint\n\nCreates a new deployment\n\n\tPOST /api/v1/deployments\n\nCreate a named (non UUID) deployment\n\n\tPUT /api/v1/deployments/{deployment_name}\n\t\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted if the blueprint is valid for deployment. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON.     \n\nUpdate a deployment using a blueprint\n\nUpdates the settings of a specific deployment.\n\n    PUT /api/v1/deployments/{deployment_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted if the deployment after the update would be still valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a deployment using a blueprint\n\nDeletes all or parts of a deployment.        \n\n    DELETE /api/v1/deployments/{deployment_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 202 Accepted if the deployment after the (partial) deletion would be still valid. Actual delete is not performed.\n\nIn contrast to most API's, doing a DELETE in Vamp takes a request body that designates what part of the deployment should be deleted. This allows you to remove specific services, clusters of the whole deployment.\n\n{{ note title=\"Note!\" }}\nDELETE on deployment with an empty request body will not delete anything.\n{{ /note }}\n\nThe most common way to specify what you want to delete is by exporting the target deployment as a blueprint using the ?as_blueprint=true parameter. You then either programmatically or by hand edit the resulting blueprint and specify which of the services you want to delete. You can also use the blueprint as a whole in the DELETE request. The result is the removal of the full deployment. \n\nexample - delete service\n\nThis is our (abbreviated) deployment in YAML format. We have two clusters. The first cluster 'frontend' has two services.\nWe have left out some keys like scale among others as they have no effect on this specific use case.\n\n\t\tGET /api/v1/deployment/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcd?as_blueprint=true\n\nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nendpoints:\n  frontend.port: '9050'\nclusters:\n  frontend:\n    services:\n    breed:\n        name: monarch_front:0.1\n        deployable: magneticio/monarch:0.1\n        ports:\n          port: 8080/http\n        constants: {}\n        dependencies:\n          backend:\n            ref: monarch_backend:0.3\n    breed:\n        name: monarch_front:0.2\n        deployable: magneticio/monarch:0.2\n        ports:\n          port: 8080/http\n        dependencies:\n          backend:\n            ref: monarch_backend:0.3\n  backend:\n    services:\n    breed:\n        name: monarch_backend:0.3\n        deployable: magneticio/monarch:0.3\n        ports:\n          jdbc: 8080/http\n        environment_variables: {}\n\nIf we want to delete the first service in the frontend cluster, we use the following blueprint as the request body in the DELETE action.\n\n\tDELETE /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\n\t\t\nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nclusters:\n  frontend:\n    services:\n    breed:\n        ref: monarch_front:0.1\n\nIf we want to delete the whole deployment, we just specify all the clusters and services.\n\n\tDELETE /api/v1/deployments/3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\n\t\t  \n\t\t  \nname: 3df5c37c-5137-4d2c-b1e1-1cb3d03ffcdd\nclusters:\n  frontend:\n    services:\n    breed:\n        ref: monarch_front:0.1\n    breed:\n        ref: monarch_front:0.2\n  backend:\n    services:\n    breed:\n        ref: monarch_backend:0.3\n\n Deployment SLAs\n\nGet a deployment SLA\n\nLists all details for a specific SLA that's part of a specific cluster.\n\n\tGET /api/v1/deployments/{deploymentname}/clusters/{clustername}/sla\n\t\n Set a deployment SLA\n\nCreates or updates a specific deployment SLA.\n\n\tPOST|PUT /api/v1/deployments/{deploymentname}/clusters/{clustername}/sla\n\t\nDelete a deployment SLA\n\nDeletes as specific deployment SLA.\n\n\tDELETE /api/v1/deployments/{deploymentname}/clusters/{clustername}/sla\n\n Deployment scales\n\nDeployment scales are singular resources: you only have one scale per service. Deleting a scale is not a meaningfull action.\n\nGet a deployment scale\n\nLists all details for a specific deployment scale that's part of a service inside a cluster.\n\n\tGET /api/v1/deployments/{deploymentname}/clusters/{clustername}/services/{service_name}/scale\n\t\n Set a deployment scale\t\n\nUpdates a deployment scale.\n\n\tPOST|PUT /api/v1/deployments/{deploymentname}/clusters/{clustername}/services/{service_name}/scale\n\n----------\n\nEscalations\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\n List escalations\n\nLists all escalations without any pagination or filtering.\n\n    GET /api/v1/escalations\n\nGet a single escalation\n\nLists all details for one specific escalation.\n\n    GET /api/v1/escalations/{escalation_name}\n\n Create escalation\n\nCreates a new escalation. Accepts JSON or YAML formatted escalations. Set the Content-Type request header to application/json or application/x-yaml accordingly.   \n\n    POST /api/v1/escalations\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 201 Created if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate an escalation\n\nUpdates the content of a specific escalation.\n\n    PUT /api/v1/escalations/{escalation_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the escalation and returns a 200 OK if the escalation is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete an escalation\n\nDeletes an escalation.        \n\n    DELETE /api/v1/escalations/{escalation_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the escalation.\n\n---------\n\nEvents\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\n List events\n\nLists metrics and/or events without any pagination or filtering.\n\n    GET /api/v1/events/get\n\n| parameter     | description      |\n| ------------- |:----------------:|\n| tag           | Event tag, e.g. GET /api/v1/events?tag=archiving&tag=breeds\n\n{{ note title=\"Note!\" }}\nsearch criteria can be set in request body, checkout examples for event stream.\n{{ /note }}\n\nCreate events\n\n    POST /api/v1/events    \n    \n Server-sent events (SSE)\n\n    GET  /api/v1/events/stream\n\n--------------\n\nGateways\n\n List gateways\n\n    GET /api/v1/gateways\n\nGet a single gateway\n\n    GET /api/v1/gateways/{gateway_name}\n\n Create gateway\nAccepts JSON or YAML formatted gateways. Set the Content-Type request header to application/json or application/x-yaml accordingly.    \n\n    POST /api/v1/gateways\n\n| parameter     | options           | default          | description       |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the gateway and returns a 201 Created if the gateway is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nUpdate a gateway\n\n    PUT /api/v1/gateways/{gateway_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the gateway and returns a 200 OK if the gateway is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a gateway     \n\n    DELETE /api/v1/gateways/{gateway_name}\n    \n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the gateway.\n\n-----------\n\nMetrics\n\nMetrics can be defined on gateways and deployment ports and retrieved:\n\n/api/v1/metrics/gateways/{gateway}/{metrics}\n/api/v1/metrics/gateways/{gateway}/routes/$route/{metrics}\n\n/api/v1/metrics/deployments/{deployment}/clusters/{cluster}/ports/{port}/{metrics}\n/api/v1/metrics/deployments/{deployment}/clusters/{cluster}/services/{service}/ports/{port}/{metrics}\n\n Example\n    /api/v1/metrics/deployments/sava/clusters/frontend/ports/api/response-time\n\n{{ note title=\"Note!\" }}\nMetrics are calculated using external services, e.g. Vamp workflows.\n{{ /note }}\n\nHealth\n\nHealth can be defined on gateways and deployment ports and retrieved:\n\n/api/v1/health/gateways/{gateway}\n/api/v1/health/gateways/{gateway}/routes/$route\n\n/api/v1/health/deployments/{deployment}\n/api/v1/health/deployments/{deployment}/clusters/{cluster}\n/api/v1/health/deployments/{deployment}/clusters/{cluster}/services/{service}\n\nHealth is value between 1 (100% healthy) and 0.\n\n{{ note title=\"Note!\" }}\nHealth is calculated using external services, e.g. Vamp workflows.\n{{ /note }}\n\n-----------\n\n Scales\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\nList scales\n\nLists all scales without any pagination or filtering.\n\n    GET /api/v1/scales\n\n Get a single scale\n\nLists all details for one specific scale.\n\n    GET /api/v1/scales/{scale_name}\n\nCreate scale\n\nCreates a new scale. Accepts JSON or YAML formatted scales. Set the Content-Type request header to application/json or application/x-yaml accordingly.    \n\n    POST /api/v1/scales\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the scale and returns a 201 Created if the scale is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Update a scale\n\nUpdates the content of a specific scale.\n\n    PUT /api/v1/scales/{scale_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the scale and returns a 200 OK if the scale is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nDelete a scale\n\nDeletes a scale.        \n\n    DELETE /api/v1/scales/{scale_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the scale.\n\n-------------\n\n SLAs\n\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n\nList SLAs\n\nLists all slas without any pagination or filtering.\n\n    GET /api/v1/slas\n\n Get a single SLA\n\nLists all details for one specific breed.\n\n    GET /api/v1/slas/{sla_name}\n\nCreate an SLA\n\nCreates a new SLA\n\n    POST /api/v1/slas   \n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| -----------------:|\n| validate_only | true or false     | false            | validates the SLA and returns a 201 Created if the SLA is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Update an SLA\n\nUpdates the content of a specific SLA.\n\n    PUT /api/v1/slas/{sla_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the SLA and returns a 200 OK if the SLA is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\nDelete an SLA\n\nDeletes an SLA.        \n\n    DELETE /api/v1/slas/{sla_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the SLA.\n\n---------------\n\n System\n\nVamp provides a set of API endpoints that help with getting general health/configuration status.\n\nGet runtime info\n\nLists information about Vamp's JVM environment and runtime status. \nAlso lists info for configured persistence layer and container driver status.\n\n\tGET /api/v1/info\n\t\nSections are jvm, persistence, keyvalue, pulse, gatewaydriver, containerdriver and workflowdriver:\n\n{\n    \"message\": \"...\",\n    \"version\": \"...\",\n    \"uuid\": \"...\",\n    \"running_since\": \"...\",\n    \"jvm\": {...},\n    \"persistence\": {...},\n    \"key_value\": {...},\n    \"pulse\": {...},\n    \"gateway_driver\": {...},\n    \"container_driver\": {...},\n    \"workflow_driver\": {...}\n}\n\n Example - explicitly request specific sections\nExplicitly requesting jvm and persistence using parameter(s) on:\n\n\tGET /api/v1/info?on=jvm&on=persistence\n\nGet Vamp configuration\n\n\tGET /api/v1/config\n\n Get HAProxy configuration\n\n\tGET /api/v1/haproxy\n\nDebug \n\n Force sync\n\nForces Vamp to perform a synchronization cycle, regardless of the configured default interval.\n\n\tGET /api/v1/sync\n\t\nForce SLA check\t\n\nForces Vamp to perform an SLA check, regardless of the configured default interval.\n\n\tGET /api/v1/sla\n\n Force escalation\t\n\nForces Vamp to perform an escalation check, regardless of the configured default interval.\n\n\tGET /api/v1/escalation\n\n---------------\nSee using the Vamp API for details on pagination, json and yaml content types and effective use of the API\n",
        "tags": []
    },
    {
        "uri": "/documentation/api/using-the-api",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Using the Vamp API\n---\nVamp has one REST API. This page explains how to specify pagination, and json and yaml content types, and how to effectively use the Vamp REST API.\n\nSee also\nFull details of all available API calls\n\n Content types\n\nVamp requests can be in YAML format or JSON format. Set the Content-Type request header to application/x-yaml or application/json accordingly.\nVamp responses can be in YAML format or JSON format. Set the Accept request header to application/x-yaml or application/json accordingly.\n\nPagination\n\nVamp API endpoints support pagination with the following scheme:\n\nRequest parameters page (starting from 1, not 0) and per_page (by default 30) e.g:\n\nGET http://vamp:8080/api/v1/breeds?page=5&per_page=20\n\nResponse headers X-Total-Count giving the total amount of items (e.g. 349673) and a Link header for easy traversing, e.g.\nX-Total-Count: 5522\nLink: \n  http://vamp:8080/api/v1/events/get?page=1&per_page=5; rel=first, \n  http://vamp:8080/api/v1/events/get?page=1&per_page=5; rel=prev, \n  http://vamp:8080/api/v1/events/get?page=2&per_page=5; rel=next, \n  http://vamp:8080/api/v1/events/get?page=19&per_page=5; rel=last\n\nSee Github's implementation for more info.\n\n Return codes\n\nCreate & Delete operations are idempotent: sending the second request with the same content will not result to an error response (4xx).\nAn update will fail (4xx) if a resource does not exist.\nA successful create operation has status code 201 Created and the response body contains the created resource.\nA successful update operation has status code 200 OK or 202 Accepted and the response body contains the updated resource.\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\nSending multiple artifacts (documents) - POST, PUT and DELETE\n \nIt is possible to send YAML document containing more than 1 artifact definition:\n\nGET /api/v1\n\nSupported methods are POST, PUT and DELETE. Example:\n\n---\nname: ...\nkind: breed\n breed definition ...\n---\nname: ...\nkind: blueprint\nblueprint definition ...\n\nAdditional kind field is required and it always correspond (singular form) to type of the artifact.\nFor instance if specific endpoint would be /api/v1/deloyments then the same deployment request can be sent to api/v1 with additional kind: deployment.\nIf specific endpoints are used (e.g. /api/v1/blueprints) then kind needs to be ommited.\n\n-------------\n\n See also\nFull details of all available API calls",
        "tags": []
    },
    {
        "uri": "/documentation/cli/cli-reference",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: CLI reference\n---\n\nThe VAMP CLI supports the following commands:  \ncreate, deploy, generate, help, info, inspect, list, merge, remove, undeploy, update, version  \nSee using the Vamp CLI for details on installation, configuration and effective use of the CLI\n\nFor details about a specific command, use vamp COMMAND --help\n\n-------------\nCreate\n\nCreate an artifact read from the specified filename or read from stdin.\n\nvamp create blueprint|breed|deployment|escalation|condition|scale|sla [--file|--stdin]\n\nParameter | purpose\n----------|--------\n--file        |       Name of the yaml file [Optional]\n--stdin        |      Read file from stdin [Optional]\n  \n Example\n vamp create scale --file my_scale.yaml\nname: my_scale\ncpu: 2.0\nmemory: 2GB\ninstances: 2\n\n-------------\nDeploy\n\nDeploys a blueprint\n\nvamp deploy NAME --deployment [--file|--stdin]\n\nParameter | purpose\n----------|--------\n--file      |         Name of the yaml file [Optional]\n--stdin     |         Read file from stdin [Optional]\n--deployment|         Name of the deployment to update [Optional]\n\n Example\n vamp deploy --deployment 1111-2222-3333-4444 --file mynewblueprint.yaml\n\n-------------\nGenerate\n\nGenerates an artifact\n\nvamp generate breed|blueprint|condition|scale [NAME] [--file|--stdin]\n| Parameter | purpose |\n|-----------|---------|\n--file    |           Name of the yaml file to preload the generation [Optional]\n--stdin   |           Read file from stdin [Optional]\n\n generate breed\n\n| Parameter | purpose |\n|-----------|---------|\n--deployable  |       Deployable specification [Optional]\n\nExample\n vamp generate breed mynewbreed --json\n{\n  \"name\":\"mynewbreed\",\n  \"deployable\":\"docker://\",\n  \"ports\":{\n    \n  },\n  \"environment_variables\":{\n    \n  },\n  \"constants\":{\n    \n  },\n  \"dependencies\":{\n    \n  }\n}\n\n generate blueprint\n\n| Parameter | purpose |\n|-----------|---------|\n--cluster   |         Name of the cluster\n--breed     |         Name of the breed   [Optional, requires --cluster]\n--scale     |         Name of the scale   [Optional, requires --breed]\n\n-------------\nHelp\n\nDisplays the Vamp help message\n\n Example\n vamp help\nUsage: vamp COMMAND [args..]\n\nCommands:\n  create              Create an artifact\n  deploy              Deploys a blueprint\n  help                This message\n  generate            Generates an artifact\n  info                Information from Vamp\n  inspect             Shows the details of the specified artifact\n  list                Shows a list of artifacts\n  merge               Merge a blueprint with an existing deployment or blueprint\n  remove              Removes an artifact\n  undeploy            Removes (part of) a deployment\n  update              Update an artifact\n  version             Shows the version of the VAMP CLI client\n  \nRun vamp COMMMAND --help  for additional help about the different command options\n\n-------------\nInfo\n\nDisplays the Vamp Info message\n\n Example\n vamp info\nmessage: Hi, I'm Vamp! How are you?\njvm:\n  operating_system:\n    name: Mac OS X\n    architecture: x86_64\n    version: 10.9.5\n    available_processors: 8.0\n    systemloadaverage: 4.8095703125\n  runtime:\n    process: 12871@MacMatthijs-4.local\n    virtualmachinename: Java HotSpot(TM) 64-Bit Server VM\n    virtualmachinevendor: Oracle Corporation\n    virtualmachineversion: 25.31-b07\n    start_time: 1433415167162\n    up_time: 1305115\n...    \n\n-------------\nInspect\nShows the details of the specified artifact\n\nvamp inspect blueprint|breed|deployment|escalation|condition|scale|sla NAME --json\n\n| Parameter | purpose |\n|-----------|---------|\n--as_blueprint | Returns a blueprint (only for inspect deployment) [Optional]|\n--json    |  Output Json instead of Yaml [Optional]|\n\n Example\n vamp inspect breed sava:1.0.0\nname: sava:1.0.0\ndeployable: magneticio/sava:1.0.0\nports:\n  port: 80/http\nenvironment_variables: {}\nconstants: {}\ndependencies: {}\n\n-------------\nList\nShows a list of artifacts\n\nvamp list blueprints|breeds|deployments|escalations|conditions|gateways|scales|slas\n\n Example \n vamp list deployments\nNAME                                    CLUSTERS\n80b310eb-027e-44e8-b170-5bf004119ef4    sava\n06e4ace5-41ce-46d7-b32d-01ee2c48f436    sava\na1e2a68b-295f-4c9b-bec5-64158d84cd00    sava, backend1, backend2\n\n-------------\nMerge\n\nMerges a blueprint with an existing deployment or blueprint.\nEither specify a deployment or blueprint in which the blueprint should be merged\nThe blueprint can be specified by NAME, read from the specified filename or read from stdin.\n\nvamp merge --deployment|--blueprint [NAME] [--file|--stdin] \n      \n| Parameter | purpose |\n|-----------|---------|\n--file     | Name of the yaml file [Optional]\n--stdin    | Read file from stdin [Optional]\n\n Example\nvamp merge --blueprint myexistingblueprint -- file addthisblueprint.yaml\n\n-------------\nRemove\n\nRemoves artifact\n\nvamp remove blueprint|breed|escalation|condition|scale|sla NAME\n\n Example\n vamp remove scale my_scale\n-------------\nUndeploy\n\nRemoves (part of) a deployment.\nBy only specifying the name, the whole deployment will be removed. To remove part of a deployment, specify a blueprint. The contents of the blueprint will be subtracted from the active deployment.\n\nvamp undeploy NAME [--blueprint|--file|--stdin] \n\nParameter | purpose\n----------|--------\n--blueprint|    Name of the stored blueprint to subtract from the deployment\n--file   |       Name of the yaml file [Optional]\n--stdin  |      Read file from stdin [Optional]\n\n Example\n vamp undeploy 9ec50a2a-33d7-4dd3-a027-9eeaeaf925c1 --blueprint sava:1.0\n-------------\nUpdate\n\nUpdates an existing artifact read from the specified filename or read from stdin.\n\nvamp update blueprint|breed|deployment|escalation|condition|scale|sla NAME [--file] [--stdin]\n\nParameter | purpose\n----------|--------\n--file   |      Name of the yaml file [Optional]\n--stdin  |      Read file from stdin [Optional]\n\n-------------\n Version\n\nDisplays the Vamp CLI version information \n\nExample\n vamp version\nCLI version: 0.7.9\n-------------\n\n See also\n\nUsing the Vamp CLI - installation, configuration and effective use of the CLI",
        "tags": []
    },
    {
        "uri": "/documentation/cli/using-the-cli",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Using the Vamp CLI\n---\n\nVamp's command line interface (CLI) can be used to perform basic actions against the Vamp API. The CLI was\nprimarily developed to work in continuous delivery situations. In these setups, the CLI takes care of automating (canary) releasing new artifacts to Vamp deployments and clusters.\n\nSee also\nFull list of available CLI commands\n\n Installation\n\nCheck run vamp for details on how to install the Vamp CLI on your platform. \n\nConfiguration\n\nAfter installation, set Vamp's host location. This location can be specified as a command line option (--host)\n\nvamp list breeds --host=http://192.168.59.103:8080\n\n...or via the environment variable VAMP_HOST\nexport VAMP_HOST=http://192.168.59.103:8080\n\n Simple commands\n\nThe basic commands of the CLI, like list, allow you to do exactly what you would expect:\n\n vamp list breeds\nNAME                     DEPLOYABLE\ncatalog                  docker://zutherb/catalog-frontend\ncheckout                 docker://zutherb/monolithic-shop\nproduct                  docker://zutherb/product-service\nnavigation               docker://magneticio/navigation-service:latest\ncart                     docker://zutherb/cart-service\nredis                    docker://redis:latest\nmongodb                  docker://mongo:latest\nmonarch_front:0.1        docker://magneticio/monarch:0.1\nmonarch_front:0.2        docker://magneticio/monarch:0.2\nmonarch_backend:0.3      docker://magneticio/monarch:0.3\n\n vamp list deployments\nNAME                                    CLUSTERS\n1272c91b-ba29-4ad1-8d09-33cbaa8f6ac2    frontend, backend\n\nCI and chaining\n\nIn more complex continuous integration situations you can use the CLI with the --stdin flag to chain a bunch of commands together. You could for instance:\n\nget an \"old\" version of a breed with inspect\ngenerate a new breed based on the previous one, while inserting a new deployable\ncreate the breed in the backend\n\nvamp inspect breed frontend:${OLD} | \\\nvamp generate breed --deployable mycompany/frontend:${NEW} frontend:${NEW} --stdin | \\\nvamp create breed --stdin\n\nOnce you have the new breed stored, you can insert it into a running deployment at the right position, i.e:\n\nget a blueprint from a running deployment with inspect and --as_blueprint\ngenerate a new blueprint with generate while inserting a new breed\ndeploying the result with deploy\n\nvamp inspect deployment $DEPLOYMENT --as_blueprint | \\\nvamp generate blueprint --cluster frontend --breed frontend:${NEW} --stdin | \\\nvamp deploy --deployment $DEPLOYMENT --stdin\n\n---------\n See also\nFull list of available CLI commands",
        "tags": []
    },
    {
        "uri": "/documentation/how vamp works/architecture-and-components",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Architecture and components\n---\n\nArchitecture \nVamp and the Vamp Gateway Agent require specific elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation. There is no set architecture required for running Vamp and every use case or specific combination of tools and platforms can have its own set up.\n\n Example topology\nThe below diagram should be used more as an overview than required architecture. For example, Mesos/Marathon stack is included even though it is not a hard dependency.\n\nVamp components\n\nVamp consists of server- and client-side components that work together with elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation.\n\n Vamp UI  \nThe Vamp UI is a graphical web interface for managing Vamp in a web browser. Packaged with Vamp.\n\nVamp CLI  \nThe Vamp CLI is a command line interface for managing Vamp and providing integration with (shell) scripts.\n\n Vamp  \nVamp is the main API endpoint, business logic and service coordinator. Vamp talks to the configured container manager (Docker, Marathon, Kubernetes etc.) and synchronizes it with Vamp Gateway Agent (VGA)  via ZooKeeper, etcd or Consul. Vamp uses Elasticsearch for artifact persistence and to store events (e.g. changes in deployments). Typically, there should be one Vamp instance and one or more VGA instances.  \n\nVamp workflows\nVamp workflows are small applications (for example using JavaScript or your own containers) that automate changes of the running system, and its deployments and gateways. We have included a few workflows out of the box, such as health and metrics, which are used by the Vamp UI to report system status and to enable autoscaling and self-healing.\n\n Vamp Gateway Agent (VGA)  \nVamp Gateway Agent (VGA) reads the HAProxy configuration from ZooKeeper, etcd or Consul and reloads HAProxy on each configuration change with as close to zero client request interruptions as possible. Typically, there should be one Vamp instance and one or more VGA instances.     \nLogs from HAProxy are read over socket and pushed to Logstash over UDP.  VGA will handle and recover from ZooKeeper, etcd, Consul and Logstash outages without interrupting the HAProxy process and client requests.  ",
        "tags": []
    },
    {
        "uri": "/documentation/how vamp works/events-and-metrics",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Events and metrics\n---\nHAProxy (VGA) generates logs and makes them accessible via open socket - check the HAProxy configuration (github.com/magneticio - haproxy.cfg) of log.\n\nVGA listens on log socket and any new messages are forwarded to the Logstash instance.\nLog format is configurable in Vamp configuration vamp.gateway-driver.haproxy (github.com/magneticio - reference.conf).\n\nFor an effective feedback loop, HTTP/TCP logs should be collected, stored and analyzed\nCollection and storing is done by a combination of HAProxy, VGA and Logstash setup\nLogs can be stored in Elasticsearch and later analysed and visualised by Kibana.\n\nLogstash\n\n{{ note title=\"Note!\" }}\nLogstash is listening on UDP port, but in principle any other listener can receive logs forwarded by VGA. \nDifferent VGAs can use different Logstash instances.\n{{ /note }}\n\nIn general, for each HTTP/TCP request to HAProxy, several log messages are created (e.g. for gateway, service and instance level).\nUsing one of the simplest Logstash configurations should be sufficient for dozens of requests per second - or even more.\nThis also depends on whether ELK is used for custom application/service logs etc.\n\nYou can transform logs to plain JSON, which can be parsed easily later on (e.g. for Kibana visualization), with this Logstash configuration together with the default vamp.gateway-driver.haproxy log format (github.com/magneticio - logstash.conf).\n\n{{ tip title=\"Examples\" }}\n\nDifferent Logstash/Elasticsearch setups: (elastic.co - Deploying and Scaling Logstash).\nLogstash command line parameter (/github.com/magneticio - Logstash section).\n{{ /tip }}\n\n Kibana\n   \n  Vamp can be configured to create Kibana searches, visualisations and dashboards automatically with the vamp.gateway-driver.kibana.enabled configuration parameter.\n  Vamp will do this by inserting ES documents to the Kibana index, so only the URL to access ES is needed (by default reusing the same as for persistence).  \n\n{{ note title=\"What next?\" }}\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/documentation/how vamp works/persistence-key-value-store",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Persistence and key-value (KV) store\n---\n\nPersistence \nVamp uses Elasticsearch (ES) as main persistence (e.g. for artifacts and events). \nVamp is not demanding in ES resources, so a small ES installation is sufficient for Vamp indexes (index names are configurable). Vamp can also use an existing ES cluster.\n\n Key-value (KV) store\nVamp depends on a key-value (KV) store for non-direct communication between Vamp and instances of the Vamp Gateway Agent (VGA). There is no direct connection between Vamp and the VGA instances - all communication is done by managing specific KV in the store.  Currently we support:\n\nZooKeeper (apache.org - zookeeper).  \nVamp and VGAs can use an existing DC/OS ZooKeeper cluster.\netcd (coreos.com - etcd)  \nVamp and VGAs can use an existing Kubernetes etcd cluster.\nConsul (consul.io)\n\n{{ note title=\"What next?\" }}\nRead about routing and load balancing\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/how vamp works/requirements",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Requirements\n---\n\nVamp's components work together with elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation. To achieve this, Vamp requires access to a container scheduler, key value store, Elastic Search and HAproxy.\n\nContainer scheduler  (orchestration)\nVamp talks directly to your choice of container scheduler. Currently we support DC/OS, Kubernetes and Rancher.\n\n Key value store\nVamp depends on a key-value (KV) store for non-direct communication between Vamp and instances of the Vamp Gateway Agent (VGA). There is no direct connection between Vamp and the VGA instances, all communication is done by managing specific KV in the store.  When Vamp needs to update the HAProxy configuration (e.g. when a new service has been deployed) Vamp will generate the new configuration and store it in the KV store. The VGAs read specific valuea and reload HAProxy instances accordingly.\nCurrently we support:\n\nZooKeeper (apache.org - zookeeper).  \nVamp and VGAs can use an existing DC/OS ZooKeeper cluster.\netcd (coreos.com - etcd)  \nVamp and VGAs can use an existing Kubernetes etcd cluster.\nConsul (consul.io)\n\nElastic Search (persistence and metrics)\nVamp uses Elastic Search (ES) for persistence (e.g. for artifacts and events) and for aggregating the metrics used by Vamp workflows and the Vamp UI. As Vamp is not demanding in ES resources, it can comfortably work with an existing ES cluster.  \nCurrently we use Logstash to format and send data to Elastic Search, but you could also opt for an alternative solution.\n\n HAproxy  (routing)\nEach Vamp Gateway Agent (VGA) requires its own instance of HAproxy. This is a hard requirement, so to keep things simple we provide a Docker container with both Vamp Gateway Agent (VGA) and HAproxy (hub.docker.com - magneticio/vamp-gateway-agent).  \n\n{{ note title=\"What next?\" }}\nFind out about using Vamp\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/how vamp works/routing-and-load-balancing",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Routing and load balancing\n---\n\n{{ note title=\"What next?\" }}\nRead about how Vamp works with events and metrics\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/documentation/how vamp works/service-discovery",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Service discovery\n---\n\nHow does Vamp do service discovery?\n\nVamp uses a service discovery pattern called server-side service discovery. This pattern allows service discovery without the need to change your code or run any other daemon or agent. The provided link explains the general pro’s and cons in good detail. In addition to service discovery, Vamp also functions as a service registry.\n\nFor Vamp, we recognise the following benefits of this pattern:\n\nNo code injection needed.\nNo extra libraries or agents needed.\nplatform/language agnostic: it’s just HTTP.\nEasy integration using ENV variables.\n\n Create and publish a service\n\n{{ note title=\"Note\" }}\nServices do not register themselves. They are explicitly created, registered in the Vamp database and provisioned on the load balancer.\n{{ /note }}\n\nServices are created and published as follows:\n\nThe user describes a service and its desired endpoint port in the Vamp DSL.\nThe service is deployed to the configured container manager by Vamp.\nVamp instructs Vamp Gateway Agent (via ZooKeeper, etcd or Consul) to set up service endpoints.\nVamp Gateway Agent takes care of configuring HAProxy, making the services available.\n\nAfter this, you can scale the service up/down or in/out either by hand or using Vamp’s auto scaling functionality. The endpoint is stable.\n\nDiscovering a service\n\nSo, how does one service find a dependent service? Services are found by just referencing them in the DSL. Take a look at the following example:\n---\nname: my_blueprint:1.0\nclusters:\n  myfrontendcluster:\n    services:\n      breed:\n        name: myfrontendservice:0.1\n        deployable: company/frontend:0.1\n        ports:\n          port: 8080/http\n        dependencies:\n          backend: mybackendservice:0.3\n        environment_variables:\n         BACKEND_HOST: $backend.host\n         BACKEND_PORT: $backend.ports.jdbc\n      scale:\n        instances: 3         \n  mybackendcluster:\n    services:\n      breed:\n        name: mybackendservice:0.3\n        deployable: company/backend:0.3\n        ports:\n          jdbc: 8080/tcp\n      scale:\n        instances: 4\n\nWe have a frontend cluster and a backend cluster. These are just organisational units.\nThe frontend cluster runs just one version of our service, consisting of three instances.\nThe frontend service has a hard dependency on a backend (tcp) service.\nWe reference the backend by name, my_backend:0.3, and assign it a label, in this case just backend\nWe use the label backend to get the host and a specific port (jdbc) from this backend.\nWe assign these values to environment variables that are exposed in the container runtime.\nAny frontend service now has access to the location of the dependent backend service.\n\n{{ note title=\"Note\" }}\nThere is no point-to-point wiring. The $backend.host and $backend.ports.jdbc variables resolve to service endpoints Vamp automatically sets up and exposes.\n{{ /note }}\n\nEven though Vamp provides this type of service discovery, it does not put any constraint on other possible solutions. For instance services can use their own approach specific approach using service registry, self-registration etc.\n\n{{ note title=\"What next?\" }}\nRead about how Vamp works with routing and load balancing\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/documentation/how vamp works/what-to-choose",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: What to choose?\n---\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/installation/azure-container-service",
        "content": "---\ndate: 2016-09-30T12:00:00+00:00\ntitle: Azure Container Service\n---\n\nTested against\nThis guide has been tested on \n\n Requirements\n\nBefore we begin...\n\n Standard install\n\n{{ note title=\"What next?\" }}\n\nOnce you have Vamp up and running you can jump into the getting started tutorials.\nThings still not running? We're here to help →\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/documentation/installation/configure-elastic-stack",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Configure Elastic Stack\n---\n\nConfigure Logstash\nIn general, for each HTTP/TCP request to HAProxy, several log messages are created (e.g. for gateway, service and instance level).\nUsing one of the simplest Logstash configurations should be sufficient for dozens of requests per second - or even more.\nThis also depends on whether ELK is used for custom application/service logs etc.\n\nYou can transform logs to plain JSON, which can be parsed easily later on (e.g. for Kibana visualization), with this Logstash configuration together with the default vamp.gateway-driver.haproxy log format (github.com/magneticio - logstash.conf).\n\n{{ tip title=\"Examples\" }}\n\nDifferent Logstash/Elasticsearch setups: (elastic.co - Deploying and Scaling Logstash).\nLogstash command line parameter (/github.com/magneticio - Logstash section).\n{{ /tip }}\n\n Configure Vamp for \n\n{{ note title=\"What next?\" }}\nConfigure Vamp\nSet container driver\nFollow the getting started tutorials\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/installation/configure-vamp",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Configure Vamp\n---\n\nVamp can be configured using one or a combination of:\n\nthe Vamp application.conf HOCON file (github.com/typesafehub - config)\nenvironment variables\nsystem properties\n\nFor example:\n\nexport VAMPINFOMESSAGE=Hello # overriding Vamp info message (vamp.info.message)\n\njava -Dvamp.gateway-driver.host=localhost \\\n     -Dlogback.configurationFile=logback.xml \\\n     -Dconfig.file=application.conf \\\n     -jar vamp.jar\n\nThe Vamp application.conf file\n\nThe Vamp application.conf consists of the following sections. All sections are nested inside a parent vamp {} tag.\n\nrest-api\npersistence\ncontainer-drivers\ngateway-driver\noperation\n\n rest-api\nConfigure the port, host name and interface that Vamp runs on using the rest-api.port \n\nvamp {\n  rest-api {\n    interface = 0.0.0.0\n    host = localhost\n    port = 8080\n    response-timeout = 10 seconds # HTTP response time out\n  }\n}    \n\npersistence\n\nVamp uses Elasticsearch for persistence and ZooKeeper (apache.org - ZooKeeper), etcd (coreos.com  - etcd documentation) or Consul (consul.io) for key-value store (keeping HAProxy configuration). \n\nvamp {\n  persistence {\n    response-timeout = 5 seconds\n\n    database {\n      type: \"elasticsearch\"  elasticsearch or in-memory (no persistence)\n      elasticsearch.url = ${vamp.pulse.elasticsearch.url}\n    }\n\n    key-value-store {\n    \n      type = \"zookeeper\"    # zookeeper, etcd or consul\n      base-path = \"/vamp\"   # base path for keys, e.g. /vamp/...\n\n      zookeeper {\n        servers = \"192.168.99.100:2181\"\n      }\n\n      etcd {\n        url = \"http://192.168.99.100:2379\"\n      }\n\n      consul {\n        url = \"http://192.168.99.100:8500\"\n      }\n    }\n  }\n}\n\nzookeeper, etcd or consul configuration is needed based on the type, e.g. if type = \"zookeeper\" then only zookeeper.servers should be set.\n\nContainer drivers\n\nVamp can be configured to work with the following container drivers:\n\nDocker\nMesos/Marathon\nKubernetes\nRancher\n\n Docker\nVamp can talk directly to a Docker daemon and its driver is configured by default. This is useful for local testing, Docker Swarm support is coming soon.\nVamp can even run inside Docker while deploying to Docker.\n\nInstall Docker as per Docker's installation manual (docs.docker.com - install Docker engine)\nCheck the DOCKER_* environment variables Vamp uses to connect to Docker, i.e.\n\n        DOCKER_HOST=tcp://192.168.99.100:2376\n    export DOCKERMACHINENAME=default\n    DOCKERTLSVERIFY=1\n    DOCKERCERTPATH=/Users/tim/.docker/machine/machines/default\n    \nIf Vamp can't find these environment variables, it falls back to using the unix:///var/run/docker.sock Unix socket for communicating with Docker.\nUpdate the container-driver section in Vamp's config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf:\n\n        ...\n    container-driver {\n      type = \"docker\"\n      response-timeout = 30 # seconds, timeout for container operations\n    }\n    ...\n        See Vamp configuration for details of the the Vamp application.conf file\n(Re)start Vamp by restarting the Java process by hand.   \n\nMesos/Marathon\nVamp can use the full power of Marathon running on either a DCOS cluster or custom Mesos cluster. You can use Vamp's DSL, or you can pass native Marathon options by using a dialect in a blueprint.  \n\nSet up a DCOS cluster using Mesosphere's assisted install on AWS (mesosphere.com - product).  \nIf you prefer, you can build your own Mesos/Marathon cluster. Here are some tutorials and scripts to help you get started:\n\n  Mesos Ansible playbook (github.com/mhamrah - ansible mesos playbook)\n  Mesos Vagrant (github.com/everpeace - vagrant-mesos)\n  Mesos Terraform (github.com/ContainerSolutions - Terraform Mesos)\n\nWhichever way you set up Marathon, in the end you should be able to see something like this:  \n\nMake a note of the Marathon endpoint (host:port) and update the container-driver section in Vamp's config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf. Set the \"url\" option to the Marathon endpoint.\n\n        ...\n    container-driver {\n      type = \"marathon\"\n      url = \"http://marathonhost:marathonport\" \n      response-timeout = 30  seconds, timeout for container operations\n    }\n    ...\n    (Re)start Vamp by restarting the Java process by hand.   \n\nKubernetes\nSpecify Kubernetes as the container driver in the Vamp application.conf file.   \n\nTaken from the example application.conf file for Kubernetes (github.com/magneticio - vamp-kubernetes):\n\n  ...\n  container-driver {\n\n    type = \"kubernetes\"\n\n    kubernetes {\n      url = \"https://kubernetes\"\n      service-type = \"LoadBalancer\"\n    }\n    ...\n\n Rancher\n\nSpecify Rancher as the container driver in the Vamp application.conf file.   \n\nTaken from the example application.conf file for Rancher (github.com/magneticio - vamp-rancher):\n\n  ...\n  container-driver.type = \"rancher\"\n  ...\n\ngateway-driver\n\nThe gateway-driver section configures how traffic should be routed through Vamp Gateway Agent. See the below example on how to configure this:\n\nvamp {\n  gateway-driver {\n    host: \"10.193.238.26\"               Vamp Gateway Agent / Haproxy, internal IP.\n    response-timeout: 30 seconds\n\n    haproxy {\n      ip: 127.0.0.1                    # HAProxy backend server IP\n\n      template: \"\"                     # Path to template file, if not specified default will be used\n\n      virtual-hosts {\n        ip: \"127.0.0.1\"                # IP, if virtual hosts are enabled\n        port: 40800                    # Port, if virtual hosts are enabled\n      }\n    }\n  }\n}  \n\nThe reason for the need to configure vamp.gateway-driver.host is that when services are deployed, they need to be able to find Vamp Gateway Agent in their respective networks. This can be a totally different network than where Vamp is running.\nLet's use an example: frontend and backend service, frontend depends on backend - in Vamp DSL that would be 2 clusters (assuming the same deployment).\nThere are different ways how frontend can discover its dependency backend, and to make things simpler Vamp supports using specific environment parameters.\n \n---\nname: my-web-app\nclusters:\n  frontend:\n    services:\n      breed:\n        name: my-frontend:1.0.0\n        deployable: magneticio/my-frontend:1.0.0\n        ports:\n          port: 8080/http\n        environment_variables:\n          BACKEND: http://$backend.host:$backend.ports.port\n        dependencies:\n          backend: my-backend:1.0.0\n  backend:\n    services:\n      breed:\n        name: my-backend:1.0.0\n        deployable: magneticio/my-backend:1.0.0\n        ports:\n          port: 8080/http\n\nIn this example $backend.host will have the value of the vamp.gateway-driver.host configuration parameter, while $backend.ports.port the next available port from vamp.operation.gateway.port-range.\nfrontend doesn't connect to backend directly but via Vamp Gateway Agent(s) - given on these host and port parameters.\nThis is quite simmilar to common pattern to access any clustered application. \nFor instance if you want to access DB server, you will have an address string based on e.g. DNS name or something simmilar.\nNote that even without Vamp, you would need to setup access to backend in some similar way. \nWith Vamp, access is via VGA's and that allows specific routing (conditions, weights) needed for A/B testing and canary releasing.\nAdditional information can be found on service discovery page.\n\noperation\n\nThe operation section holds all parameters that control how Vamp executes against \"external\" services: this also includes Vamp Pulse and Vamp Gateway Agent.\n\noperation {\n  sla.period = 5 seconds         controls how often an SLA checks against metrics\n  escalation.period = 5 seconds # controls how often Vamp checks for escalation events\n\tsynchronization {\n    period = 4 seconds          # controls how often Vamp performs\n                                # a sync between Vamp and the container driver.\n    timeout {\n      ready-for-deployment: 600\tseconds   # controls how long Vamp waits for a\n                                          # service to start. If the service is not started\n                                          # before this time, the service is registered as \"error\"\n      ready-for-undeployment: 600 seconds # similar to \"ready-for-deployment\", but for\n                                          # the removal of services.\n    }\n   }\n  \n  gateway {\n    port-range = 40000-45000\n    response-timeout = 5 seconds # timeout for container operations\n\n    virtual-hosts.formats {      # name format\n      gateway                 = \"$gateway.vamp\"\n      deployment-port         = \"$port.$deployment.vamp\"\n      deployment-cluster-port = \"$port.$cluster.$deployment.vamp\"\n    }\n  }\n  \n  deployment {\n    scale {         # default scale, if not specified in blueprint\n      instances: 1\n      cpu: 1\n      memory: 1GB\n    }\n\n    arguments: []   # split by first '=', \n                    # Docker command line arguments, e.g. \"security-opt=seccomp:unconfined\"\n  }\n}\n\nFor each cluster and service port within the same cluster a gateway is created - this is exactly as one that can be created using Gateway API.\nThat means specific conditions and weights can be applied on traffic to/from cluster services - A/B testing and canary releases support.\nvamp.operation.gateway.port-range is range of port values that can be used for these cluster/port gateways. \nThese ports need to be available on all Vamp Gateway Agent hosts.\n\nEnvironment variables\n\nEach configuration parameter can be replaced by an environment variable.\nEnvironment variable names are based on the configuration parameter name converted to upper case and with all non-alphanumerics replaced by an underscore _ \n\n Example - environment variable names\n\nvamp.info.message           ⇒ VAMPINFOMESSAGE\nvamp.gateway-driver.timeout ⇒ VAMPGATEWAYDRIVER_TIMEOUT\n\n{{ note title=\"Note!\" }}\nEnvironment variables have precedence over application.conf or system properties.\n{{ /note }}\n\nRead more about environment variables.\n\n{{ note title=\"What next?\" }}\nSet container driver\nConfigure Elastic Stack\nFollow the getting started tutorials\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/documentation/installation/dcos",
        "content": "---\ndate: 2016-09-30T12:00:00+00:00\ntitle: DC/OS 1.7 and 1.8\n---\n\nOverview\n\nThere are different ways to install Vamp on DC.OS. On this page we start out with the most common setup, but if you are interested in doing a custom install or working with public and private nodes you should jump to that section.\n\nStandard install\nCustom install\nPublic and private nodes\n\n Standard install\nThis setup will run Vamp, Mesos and Marathon, together with Zookeeper, Elasticsearch and Logstash on DC/OS. If you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\nTested against\nThis guide has been tested on both 1.7 and the latest 1.8 version of DC/OS.\n\n Requirements\nBefore you start you need to have a DC/OS cluster up and running, as well as the its CLI configured to use it. We assume you have it up and running on http://dcos.example.com/.\nSetting up DC/OS is outside the scope of this document, for that you need to refer to the official documentation:\n\nhttps://dcos.io/docs/1.7/administration/installing/\nhttps://dcos.io/docs/1.7/usage/cli/\nhttps://dcos.io/docs/1.8/administration/installing/\nhttps://dcos.io/docs/1.8/usage/cli/\n\nStep 1: Install Elasticsearch + Logstash\n\nMesos, Marathon and ZooKeeper are all installed by DC/OS. In addition to these, Vamp requires Elasticsearch and Logstash for metrics collection and aggregation.\n\nYou could install Elasticsearch on DC/OS by following the Mesos Elasticsearch documentation (mesos-elasticsearch - Elasticsearch Mesos Framework).\nHowever, Vamp will also need Logstash (not currently available as a DC/OS package) with a specific Vamp Logstash configuration (github.com/magneticio - Vamp Docker logstash.conf).  \n\nTo make life easier, we have created compatible Docker images for a Vamp Elastic Stack (hub.docker.com - magneticio elastic) that you can use with the Mesos elasticsearch documentation (mesos-elasticsearch - How to install on Marathon).\nOur advice is to use our custom Elasticsearch+Logstash Docker image. Let's get started!\n\nCreate elasticsearch.json with the following content:\n\n{\n  \"id\": \"elasticsearch\",\n  \"instances\": 1,\n  \"cpus\": 0.2,\n  \"mem\": 1024.0,\n  \"container\": {\n    \"docker\": {\n      \"image\": \"magneticio/elastic:2.2\",\n      \"network\": \"HOST\",\n      \"forcePullImage\": true\n    }\n  },\n  \"healthChecks\": [\n    {\n      \"protocol\": \"TCP\",\n      \"gracePeriodSeconds\": 30,\n      \"intervalSeconds\": 10,\n      \"timeoutSeconds\": 5,\n      \"port\": 9200,\n      \"maxConsecutiveFailures\": 0\n    }\n  ]\n}\n\nThis will run the container with 1G of RAM and a basic health check on the elasticsearch port.\n\nUsing the CLI we can install this in our cluster:\n\n$ dcos marathon app add elasticsearch.json\n\nIf you get no error message you should now be able to see it being deployed:\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    0/1    0/0      scale       DOCKER   None  \n\nOnce it's fully up and running you should see all tasks and health checks being up:\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    1/1    1/1       ---        DOCKER   None  \n\n Step 2: Deploy Vamp\n\nOnce you have elasticsearch up and running it's time to move on to Vamp.\n\nCreate vamp.json with the following content:\n\n{\n  \"id\": \"vamp/vamp\",\n  \"instances\": 1,\n  \"cpus\": 0.5,\n  \"mem\": 1024,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"magneticio/vamp:katana-dcos\",\n      \"network\": \"BRIDGE\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8080,\n          \"hostPort\": 0,\n          \"name\": \"vip0\",\n          \"labels\": {\n            \"VIP_0\": \"10.20.0.100:8080\"\n          }\n        }\n      ],\n      \"forcePullImage\": true\n    }\n  },\n  \"labels\": {\n    \"DCOSSERVICENAME\": \"vamp\",\n    \"DCOSSERVICESCHEME\": \"http\",\n    \"DCOSSERVICEPORT_INDEX\": \"0\"\n  },\n  \"env\": {\n    \"VAMPWAITFOR\": \"http://elasticsearch.marathon.mesos:9200/.kibana\",\n    \"VAMPPERSISTENCEDATABASEELASTICSEARCHURL\": \"http://elasticsearch.marathon.mesos:9200\",\n    \"VAMPGATEWAYDRIVERLOGSTASHHOST\": \"elasticsearch.marathon.mesos\",\n    \"VAMPWORKFLOWDRIVERVAMPURL\": \"http://10.20.0.100:8080\",\n    \"VAMPPULSEELASTICSEARCH_URL\": \"http://elasticsearch.marathon.mesos:9200\"\n  },  \n  \"healthChecks\": [\n    {\n      \"protocol\": \"TCP\",\n      \"gracePeriodSeconds\": 30,\n      \"intervalSeconds\": 10,\n      \"timeoutSeconds\": 5,\n      \"portIndex\": 0,\n      \"maxConsecutiveFailures\": 0\n    }\n  ]\n}\n\nThis service definition will download our Vamp container and spin it up in your DC/OS cluster on a private node in bridge networking mode. It will also configure the apporiate labels for the AdminRouter to expose the UI through DC/OS, as well as an internal VIP for other applications to talk to Vamp, adjusting some defaults to work inside DC/OS, and finally a health check for monitoring.\n\nDeploy it with the CLI, like with did with elasticsearch:\n\n$ dcos marathon app add vamp.json\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    1/1    1/1       ---        DOCKER   None  \n/vamp/vamp      1024  0.5    0/1    0/0      scale       DOCKER   None  \n\nIt will take a minute for Vamp to deploy all its components, you can see that by looking in the \"tasks\" column, where Vamp is listed as 0/1. Run the list command again and you should see all the components coming online:\n\n$ dcos marathon app list\nID                        MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD\n/elasticsearch            1024  0.2    1/1    1/1       ---        DOCKER   None\n/vamp/vamp                1024  0.5    1/1    1/1       ---        DOCKER   None\n/vamp/vamp-gateway-agent  256   0.2    3/3    ---       ---        DOCKER   ['--storeType=zookeeper', '--storeConnection=zk-1.zk:2181', '--storeKey=/vamp/haproxy/1.6', '--logstash=elasticsearch.marathon.mesos:10001']\n/vamp/workflow-health      64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-kibana      64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-metrics     64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-vga         64   0.1    1/1    ---       ---        DOCKER   None\n\nVamp has now spun up all it's components and you should be able to access the ui by opening http://dcos.example.com/service/vamp/ in your browser.\n\nNow you're ready to follow our Vamp getting started tutorials.\nThings still not running? We're here to help →\n\n NB If you need help you can also find us on [Gitter] (https://gitter.im/magneticio/vamp)\n\nCustom install\n\nThe Vamp DC/OS Docker image (github.com/magneticio - Vamp DC/OS) contains configuration (github.com/magneticio - Vamp DC/OS configuration) that can be overridden for specific needs by:\n\nMaking a new Docker image based on the Vamp DC/OS image\nUsing environment variables\n\n Example 1 - Remove the metrics and health workflows by Vamp configuration and keep the kibana workflow:\n\nvamp.lifter.artifact.resources = [\n    \"breeds/kibana.js\", \"workflows/kibana.yml\"\n  ]\n\nor doing the same using Marathon JSON\n\n\"env\": {\n  \"VAMPLIFTERARTIFACT_RESOURCES\": \"[\\\"breeds/kibana.js\\\",\\\"workflows/kibana.yml\\\"]\"\n}\n\nExample 2 - Avoid automatic deployment of Vamp Gateway Agent\n\nRemove vga-marathon breed and workflow from vamp.lifter.artifact.files:\n\nvamp.lifter.artifact.files = []\n\nor using Marathon JSON\n\n\"env\": {\n  \"VAMPLIFTERARTIFACT_FILES\": \"[]\"\n}\n\n Public and private nodes\n\nRunning Vamp on public Mesos agent node(s) and disabling automatic Vamp Gateway Agent deployments (but keeping other default workflows) can be done with the following Marathon JSON:\n\n{\n  \"id\": \"vamp/vamp\",\n  \"instances\": 1,\n  \"cpus\": 0.5,\n  \"mem\": 1024,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"magneticio/vamp:katana-dcos\",\n      \"network\": \"BRIDGE\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8080,\n          \"hostPort\": 0,\n          \"name\": \"vip0\",\n          \"labels\": {\n            \"VIP_0\": \"10.20.0.100:8080\"\n          }\n        }\n      ],\n      \"forcePullImage\": true\n    }\n  },\n  \"labels\": {\n    \"DCOSSERVICENAME\": \"vamp\",\n    \"DCOSSERVICESCHEME\": \"http\",\n    \"DCOSSERVICEPORT_INDEX\": \"0\"\n  },\n  \"env\": {\n    \"VAMPLIFTERARTIFACT_FILES\": \"[\\\"breeds/health.js\\\",\\\"workflows/health.yml\\\",\\\"breeds/metrics.js\\\",\\\"workflows/metrics.yml\\\",\\\"breeds/kibana.js\\\",\\\"workflows/kibana.yml\\\"]\",\n    \"VAMPWAITFOR\": \"http://elasticsearch.marathon.mesos:9200/.kibana\",\n    \"VAMPPERSISTENCEDATABASEELASTICSEARCHURL\": \"http://elasticsearch.marathon.mesos:9200\",\n    \"VAMPGATEWAYDRIVERLOGSTASHHOST\": \"elasticsearch.marathon.mesos\",\n    \"VAMPWORKFLOWDRIVERVAMPURL\": \"http://vamp-vamp.marathon.mesos:8080\",\n    \"VAMPPULSEELASTICSEARCH_URL\": \"http://elasticsearch.marathon.mesos:9200\"\n  },\n  \"acceptedResourceRoles\": [\n    \"slave_public\"\n  ],\n  \"healthChecks\": [\n    {\n      \"protocol\": \"TCP\",\n      \"gracePeriodSeconds\": 30,\n      \"intervalSeconds\": 10,\n      \"timeoutSeconds\": 5,\n      \"portIndex\": 0,\n      \"maxConsecutiveFailures\": 0\n    }\n  ]\n}\n\nDeploying Vamp Gateway Agent on all public and private Mesos agent nodes through Marathon JSON - NB replace $INSTANCES (e.g. to be the same as total number of Mesos agent nodes) and optionally other parameters:\n\n{\n  \"id\": \"vamp/vamp-gateway-agent\",\n  \"instances\": $INSTANCES,\n  \"cpus\": 0.2,\n  \"mem\": 256.0,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"magneticio/vamp-gateway-agent:katana\",\n      \"network\": \"HOST\",\n      \"privileged\": true,\n      \"forcePullImage\": true\n    }\n  },\n  \"args\": [\n    \"--storeType=zookeeper\",\n    \"--storeConnection=zk-1.zk:2181\",\n    \"--storeKey=/vamp/gateways/haproxy/1.6\",\n    \"--logstash=elasticsearch.marathon.mesos:10001\"\n  ],\n  \"constraints\": [\n    [\n      \"hostname\",\n      \"UNIQUE\"\n    ]\n  ],\n  \"acceptedResourceRoles\": [\n    \"slave_public\",\n    \"*\"\n  ]\n}\n\n{{ note title=\"What next?\" }}\n\nOnce you have Vamp up and running you can follow our getting started tutorials.\nChcek the Vamp documentation\nThings still not running? We're here to help →\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/documentation/installation/hello-world",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Hello world\n\n---\n\nThe Vamp hello world setup will run Mesos, Marathon (mesosphere.github.io - Marathon) and Vamp inside a local Docker container with Vamp's Marathon driver.  We will do this in three simple steps (although it's really just one docker run command). You can use the hello world setup to work through the getting started tutorials and try out some of Vamp's core features.\n\n{{ note }}\nThis hello world set up is designed for demo purposes only - it is not production grade.\n{{ /note }}\n\nStep 1: Get Docker\n\nPlease install one of the following for your platform/architecture\n\nDocker 1.9.x (Linux) or higher (Vamp works with Docker 1.12 too), OR\n[Docker Toolbox 1.12.x] (https://github.com/docker/toolbox/releases) if on Mac OS X 10.8+ or Windows 7+ \n\nVamp hello world on Docker for Mac or Windows is currently not supported. We're working on this so please check back. \n\n Step 2: Run Vamp\n\nUse the instructions below to start the magneticio/vamp-docker:0.9.0-marathon container, taking care to pass in the right parameters. \n\nLinux\n\nA typical command would be:\ndocker run --privileged \\\n           --net=host \\\n           -v /var/run/docker.sock:/var/run/docker.sock \\\n           -v $(which docker):/bin/docker \\\n           -v \"/sys/fs/cgroup:/sys/fs/cgroup\" \\\n           -e \"DOCKERHOSTIP=hostname -I | awk '{print $1;}'\" \\\n           magneticio/vamp-docker:0.9.0\n\nMounting volumes is important. Read this great article about starting Docker containers from/within another Docker container.\n\n Mac OS X 10.8+ or Windows 7+\n\nIf you installed Docker Toolbox, please use the Docker Quickstart Terminal. We don't currently support Kitematic. A typical command on Mac OS X running Docker Toolbox would be:\ndocker run --net=host \\\n           -v /var/run/docker.sock:/var/run/docker.sock \\\n           -v docker-machine ssh default \"which docker\":/bin/docker \\\n           -v \"/sys/fs/cgroup:/sys/fs/cgroup\" \\\n           -e \"DOCKERHOSTIP=docker-machine ip default\" \\\n           magneticio/vamp-docker:0.9.0\n\nStep 3: Check Vamp is up and running\n\nAfter some downloading and booting, your Docker log will show Vamp has launched and report something like:\n\n...Bound to /0.0.0.0:8080\n\nNow check if Vamp is home on http://{docker-machine ip default}:8080/ and you're ready for the Vamp getting started tutorials\n\nExposed services:\n\nHAProxy statistics | http://localhost:1988 (username/password: haproxy) \nHAProxy statistics http://localhost:1988 (username/password: haproxy)\nElasticsearch HTTP http://localhost:9200\nKibana http://localhost:5601\nSense http://localhost:5601/app/sense\nMesos http://localhost:5050\nMarathon http://localhost:9090 (Note that the Marathon port is 9090 and not the default 8080).\nChronos http://localhost:4400\nVamp http://localhost:8080\n\nIf you run on Docker machine, use docker-machine ip default instead of localhost.\n\n{{ note title=\"Note\" }}\nThis set up runs all of Vamp's components in one container. This is definitely not ideal, but works fine for kicking the tires.\nYou will run into cpu, memory and storage issues pretty soon though. Also, random ports are assigned by Vamp which you might not have exposed on either Docker or your Docker Toolbox Vagrant box.  \n{{ /note }}\n\n What next?\n\nNow you're all set to follow our getting started tutorials.\nThings still not running? We're here to help →\nRemember, this is not a production grade setup!\n\nIf you need help you can also find us on [Gitter] (https://gitter.im/magneticio/vamp)",
        "tags": []
    },
    {
        "uri": "/documentation/installation/index",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Installation\n---\nBefore you get Vamp up and running on your architecture, it is helpful to understand how vamp works and the role of each component and its preferred location in a typical architecture.  \n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n\nRequirements\n\nVamp requirements\n\n Install Vamp\n\nDC/OS 1.7 and 1.8\nMesos/Marathon\nKubernetes 1.2\nRancher\nAzure Container Service\n\nConfiguration\n\nConfigure Vamp\nSet container driver\nConfigure Elastic Stack \n\n Try Vamp\n\nWe've put together a hello world walkthrough to let you try out some of Vamp's core features in a local docker container. You can use this to work through the getting started tutorials.\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/installation/kubernetes",
        "content": "---\ndate: 2016-10-04T09:00:00+00:00\ntitle: Kubernetes 1.x\n---\n\n{{ note title=\"Note!\" }}\nKubernetes support is still in Alpha.\n{{ /note }}\n\nTested against\nThis guide has been tested on Kubernetes 1.2 and 1.3. Minikube can also be used. (github.com - minikube) \n\n Requirements\n\nGoogle Container Engine cluster\nKey-value store (like ZooKeeper, Consul or etcd)\nElasticsearch and Logstash\n\nBefore we begin...\nIt is advisable to try out the official Quickstart for Google Container Engine tutorial first (google.com - container engine quickstart).  \n\n Standard install\nThe standard install will run Vamp together with etcd, Elasticsearch and Logstash on Google container engine and kubernetes. (We will also deploy our demo Sava application to give you something to play around on).   \n\nStep 1: Create a new GKE cluster:\n\nThe simple way to create a new GKE cluster:\n\nopen Google Cloud Shell\nset a zone, e.g. gcloud config set compute/zone europe-west1-b\ncreate a cluster vamp using default parameters: gcloud container clusters create vamp\n\nAfter the (new) Kubernetes cluster is setup, we are going to continue with the installation using the Kubernetes CLI kubectl.\nYou can use kubectl directly from the Google Cloud Shell, e.g. to check the Kubernetes client and server version:\n\n$ kubectl version\n\n Step 2: Deploy etcd, Elasticsearch and Logstash\n\nLet's deploy etcd - the installation is based on this tutorial (github.com/coreos - etcd on Kubernetes).\nExecute: \n\n$ kubectl create \\\n        -f https://raw.githubusercontent.com/magneticio/vamp-docker/master/vamp-kubernetes/etcd.yml\n\nDeploy Elasticsearch and Logstash with a proper Vamp Logstash configuration (github.com/magneticio - elastic):\n\n$ kubectl run elastic --image=magneticio/elastic:2.2\n$ kubectl expose deployment elastic --protocol=TCP --port=9200 --name=elasticsearch\n$ ubectl expose deployment elastic --protocol=UDP --port=10001 --name=logstash\n$ kubectl expose deployment elastic --protocol=TCP --port=5601 --name=kibana\n{{ note title=\"Note!\" }}\nThis is not a production grade setup. You would also need to take care of persistence and running multiple replicas of each pod.\n{{ /note }}\n\nStep 3: Run Vamp\n\nLet's run Vamp gateway agent as a daemon set first:\n$ kubectl create \\\n        -f https://raw.githubusercontent.com/magneticio/vamp-docker/master/vamp-kubernetes/vga.yml\n\nTo deploy Vamp, execute:\n\n$ kubectl run vamp --image=magneticio/vamp:0.9.0-kubernetes\n$ kubectl expose deployment vamp --protocol=TCP --port=8080 --name=vamp --type=\"LoadBalancer\"\n\nThe Vamp image uses the following configuration (github.com/magneticio - Vamp kubernetes configuration).\n\nWait a bit until Vamp is running and check out the Kubernetes services:\n\n$ kubectl get services\n\nThe output should be similar to this:\n\nNAME                 CLUSTER-IP     EXTERNAL-IP      PORT(S)             AGE\nelasticsearch        10.3.242.188   none           9200/TCP            4m\netcd-client          10.3.247.112   none           2379/TCP            4m\netcd0                10.3.251.13    none           2379/TCP,2380/TCP   4m\netcd1                10.3.251.103   none           2379/TCP,2380/TCP   4m\netcd2                10.3.250.20    none           2379/TCP,2380/TCP   4m\nkubernetes           10.3.240.1     none           443/TCP             5m\nlogstash             10.3.254.16    none           10001/UDP           4m\nvamp                 10.3.242.93    146.148.118.45   8080/TCP            2m\nvamp-gateway-agent   10.3.254.234   146.148.22.145   80/TCP              2m\n\nNotice that the Vamp UI is exposed (in this example) on http://146.148.118.45:8080\n\n Step 4: Deploy the Sava demo application\n\n---\nname: sava:1.0\ngateways:\n  9050: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava:1.0.0\n        deployable: magneticio/sava:1.0.0\n        ports:\n          port: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1\n\n{{ note title=\"Note!\" }}\nBe sure that the cluster has enough resources (CPU, memory), otherwise deployments will be in pending state.\n{{ /note }}\n\nOnce it's running we can check if all Vamp Gateway Agent services are up:\n\n$ kubectl get services --show-labels -l vamp=gateway\n\nWe can see that for each gateway a new service is created:\n\nNAME                      CLUSTER-IP     EXTERNAL-IP     PORT(S)     AGE       LABELS\nhex1f8c9a0157c9fe3335e9   10.3.243.199   104.155.24.47   9050/TCP    2m        lookup_name=a7ad6869e65e9c047f956cf7d1b4d01a89e\nef486,vamp=gateway\nhex26bb0695e9a85ec34b03   10.3.245.85    23.251.143.62   40000/TCP   2m        lookup_name=6ace45cb2c155e85bd0c84123d1dab5a6cb\n12c97,vamp=gateway\n\n{{ note title=\"Note!\" }}\nIn this setup Vamp is deliberately configured to initiate exposure of all gateway and VGA ports. This would not be the case if the default and recommended setting are used.\n{{ /note }}\n\nNow we can access our sava service on http://104.155.24.47:9050\n\nThe default Kubernetes service type can be set in configuration: vamp.container-driver.kubernetes.service-type, possible values are LoadBalancer or NodePort. \n\nWe can also access gateways using virtual hosts. Vamp Gateway Agent service is on IP 146.148.22.145 in this example, so:\n$ curl --resolve 9050.sava-1-0.vamp:80:146.148.22.145 -v http://9050.sava-1-0.vamp\n\n{{ note title=\"Note!\" }}\nDon't forget to clean up your Kubernetes cluster and firewall rules  if you don't want to use them anymore (google.com - container engine quickstart: clean up).\n{{ /note }}\n\n{{ note title=\"What next?\" }}\n\nOnce you have Vamp up and running you can jump into the getting started tutorials.\nThings still not running? We're here to help →\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/documentation/installation/mesos-marathon",
        "content": "---\ndate: 2016-09-30T12:00:00+00:00\ntitle: Mesos/Marathon\n---\n\nVamp can use the full power of Marathon running on either a DCOS cluster or custom Mesos cluster. You can use Vamp's DSL, or you can pass native Marathon options by using a dialect in a blueprint.  \n\nInstall\nThe instructions included on the DC/OS installation page will also work with Mesos/Marathon.\n\n set container driver\nSee set Mesos/Marathon as the container driver\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/installation/rancher",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Rancher\n---\n\n{{ note title=\"Note!\" }}\nRancher support is still in Alpha.\n{{ /note }}\n\nTested against\nThis guide has been tested on Rancher version 1.1.x.\n\n Requirements\n\nRancher up and running\nKey-value store like ZooKeeper, Consul or etcd\nElasticsearch and Logstash\n\nBefore we begin...\nIt is advisable to try out the official Rancher Quick Start Guide tutorial first (rancher.com - quick start guide).  \n\n Standard install\nThe standard install will run Vamp together with Consul, Elasticsearch and Logstash on Rancher. (We'll also deploy our demo Sava application to give you something to play around on).\n\nIf you want to make a setup on your local VM based Docker, it's advisable to increase default VM memory size from 1GB to 4GB.\n\nStep 1: Run Rancher locally\nBased on the official Rancher quickstart tutorial, these are a few simple steps to run Rancher locally:\n$ docker run -d --restart=always -p 8080:8080 rancher/server\nThe Rancher UI is exposed on port 8080, so go to http://SERVER_IP:8080 - for instance http://192.168.99.100:8080, http://localhost:8080 or something similar depending on your Docker setup.\n  \nFollow the instructions on the screen to add a new Rancher host:\n\nclick on \"Add Host\" and then on \"Save\". \nYou should get instructions (bullet point 5) to run an agent Docker image:\n$ docker run \\\n  -d --privileged \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /var/lib/rancher:/var/lib/rancher \\\n  rancher/agent:v1.0.1 \\\n  http://192.168.99.100:8080/v1/scripts/E78EF5848B989FD4DA77:1466265600000:SYqIvhPgzKLonp8r0erqgpsi7pQ\n\nGo to Add Stack and create a new stack vamp (lowercase).   \n\n Step 2: Install Consul, Elasticsearch and Logstash\nWe can now install the other dependencies.\n\nConsul\n\nUse your newly created vamp stack and go to Add Service:\n\nName ⇒ consul\nSelect Image ⇒ gliderlabs/consul-server\nSet Command ⇒ -server -bootstrap\nGo to Networking tab\nUnder Hostname select Set a specific hostname: and enter consul\nClick the Create button\n\n Elasticsearch and Logstash\n\n Our custom Docker image magneticio/elastic:2.2 contains Elasticsearch, Logstash and Kibana with the proper Logstash configuration for Vamp. More details can be found on the github project page (github.com/magneticio - elastic).\n\nUse the vamp stack and go to Add Service:\n\nName ⇒ elastic\nSelect Image ⇒ magneticio/elastic:2.2\nGo to Networking tab\nUnder Hostname select Set a specific hostname: and enter elastic\nClick on Create button\n\nStep 3: Run Vamp\n\nFirst we'll run the Vamp Gateway Agent: \n\nUse the vamp stack and go to Add Service:\n\nSet scale to Always run one instance of this container on every host\nName ⇒ vamp-gateway-agent\nSelect Image ⇒ magneticio/vamp-gateway-agent:0.9.0\nSet Command ⇒ --storeType=consul --storeConnection=consul:8500 --storeKey=/vamp/haproxy/1.6 --logstash=elastic:10001\nGo to Networking tab\nUnder Hostname select Set a specific hostname: and enter vamp-gateway-agent\nClick on Create button\n\nNow let's find a Rancher API endpoint that can be accessed from running container:\n\nGo to the API page and find the endpoint, e.g. http://192.168.99.100:8080/v1/projects/1a5\nGo to the Infrastructure/Containers and find the IP address of rancher/server, e.g. 172.17.0.2\nThe Rancher API endpoint should be then http://IP_ADDRESS:PORT/PATH based on values we have, e.g. http://172.17.0.2:8080/v1/projects/1a5\n\nNow we can deploy Vamp:\n\nUse the vamp stack and go to Add Service:\n\nName ⇒ vamp\nSelect Image ⇒ magneticio/vamp:0.9.0-rancher\nGo to Add environment variable VAMPCONTAINERDRIVERRANCHERURL with value of Rancher API endpoint, e.g. http://172.17.0.2:8080/v1/projects/1a5\nGo to Networking tab\nUnder Hostname select Set a specific hostname: and enter vamp\nClick the Create button\nGo to Add Load Balancer (click arrow next to Add Service button label)\nChoose a name (e.g. vamp-lb), Source IP/Port ⇒ 9090, Default Target Port ⇒ 8080 and Target Service ⇒ vamp\n\nIf you go to http://SERVER_IP:9090 (e.g http://192.168.99.100:9090), you should get the Vamp UI.  \nYou should also notice that Vamp Gateway Agent is running (one instance on each node) and additional Vamp workflows.\n\nTo access HAProxy stats:\n\nGo to Add Load Balancer (click arrow next to Add Service)\nChoose aname (e.g. vamp-gateway-agent-lb), Source IP/Port ⇒ 1988, Default Target Port ⇒ 1988 and Target Service ⇒ vamp-gateway-agent\nUse the following username/password: haproxy for the HAProxy stats page\n\n Step 4: Deploy the Sava demo application\n\nLet's deploy our sava demo application:\n\n---\nname: sava:1.0\ngateways:\n  9050: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava:1.0.0\n        deployable: magneticio/sava:1.0.0\n        ports:\n          port: 8080/http\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n\nIf you want to the gateway port to be exposed outside of the cluster via Rancher Load Balancer:\n\nGo to Add Load Balancer (click arrow next to Add Service)\nChoose name (e.g. gateway-9050), Source IP/Port ⇒ 9050, Default Target Port ⇒ 9050 and Target Service ⇒ vamp-gateway-agent\n \n\n{{ note title=\"What next?\" }}\n\nOnce you have Vamp up and running you can jump into the getting started tutorials\nThings still not running? We're here to help →\nRemember, this is not a production grade setup!\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/documentation/installation/set-container-driver",
        "content": "---\ndate: 2016-09-30T12:00:00+00:00\ntitle: Set container driver\n---\n\nVamp can be configured to work with the following container drivers:\n\nDocker\nMesos/Marathon\nKubernetes\nRancher\n\nDocker\nVamp can talk directly to a Docker daemon and its driver is configured by default. This is useful for local testing, Docker Swarm support is coming soon.\nVamp can even run inside Docker while deploying to Docker.\n\nInstall Docker as per Docker's installation manual (docs.docker.com - install Docker engine)\nCheck the DOCKER_* environment variables Vamp uses to connect to Docker, i.e.\n\n        DOCKER_HOST=tcp://192.168.99.100:2376\n    export DOCKERMACHINENAME=default\n    DOCKERTLSVERIFY=1\n    DOCKERCERTPATH=/Users/tim/.docker/machine/machines/default\n    \nIf Vamp can't find these environment variables, it falls back to using the unix:///var/run/docker.sock Unix socket for communicating with Docker.\nUpdate the container-driver section in Vamp's config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf:\n\n        ...\n    container-driver {\n      type = \"docker\"\n      response-timeout = 30  seconds, timeout for container operations\n    }\n    ...\n        See Vamp configuration for details of the the Vamp application.conf file\n(Re)start Vamp by restarting the Java process by hand.   \n\nMesos/Marathon\nVamp can use the full power of Marathon running on either a DCOS cluster or custom Mesos cluster. You can use Vamp's DSL, or you can pass native Marathon options by using a dialect in a blueprint.  \n\nSet up a DCOS cluster using Mesosphere's assisted install on AWS (mesosphere.com - product).  \nIf you prefer, you can build your own Mesos/Marathon cluster. Here are some tutorials and scripts to help you get started:\n\n  Mesos Ansible playbook (github.com/mhamrah - ansible mesos playbook)\n  Mesos Vagrant (github.com/everpeace - vagrant-mesos)\n  Mesos Terraform (github.com/ContainerSolutions - Terraform Mesos)\n\nWhichever way you set up Marathon, in the end you should be able to see something like this:  \n\nMake a note of the Marathon endpoint (host:port) and update the container-driver section in Vamp's config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf. Set the \"url\" option to the Marathon endpoint.\n\n        ...\n    container-driver {\n      type = \"marathon\"\n      url = \"http://marathonhost:marathonport\" \n      response-timeout = 30  seconds, timeout for container operations\n    }\n    ...\n    (Re)start Vamp by restarting the Java process by hand.   \n\nKubernetes\nSpecify Kubernetes as the container driver in the Vamp application.conf file.   \n\nTaken from the example application.conf file for Kubernetes (github.com/magneticio - vamp-kubernetes):\n\n  ...\n  container-driver {\n\n    type = \"kubernetes\"\n\n    kubernetes {\n      url = \"https://kubernetes\"\n      service-type = \"LoadBalancer\"\n    }\n    ...\n\n Rancher\n\nSpecify Rancher as the container driver in the Vamp application.conf file.   \n\nTaken from the example application.conf file for Rancher (github.com/magneticio - vamp-rancher):\n\n  ...\n  container-driver.type = \"rancher\"\n  ...\n\n{{ note title=\"What next?\" }}\nConfigure Vamp\nConfigure Elastic Stack\nFollow the getting started tutorials\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/documentation/release-notes",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Release notes\n---",
        "tags": []
    },
    {
        "uri": "/documentation/tutorials/deploy-your-first-blueprint",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Deploy your first blueprint\n---\nOverview\n\nIf everything went to plan, you should have your Vamp installation up and running. If not, please follow the Vamp hello world quick setup steps. Now we're ready to check out some of Vamp's features. \n\n In this tutorial we will:  \n\nDeploy a monolith, using either the Vamp UI or the Vamp API\nCheck out the deployed application  \nGet some metrics on the running application  \nChange the scale and load-balancing\nChaos monkey!    \n\nIn depth\n\n Step 1: Deploy a monolith\n\nImagine you or the company you work for still use monolithic applications. I know, it sounds far fetched...\nThis application is conveniently called Sava monolith and is at version 1.0.  \n\nYou've managed to wrap your monolith in a Docker container, which lives in the Docker hub under magneticio/sava:1.0.0. Your app normally runs on port 8080 but you want to expose it under port 9050 in this case. Let's deploy this through Vamp using the following simple blueprint. Don't worry too much about what means what: we'll get there.\n\n---\nname: sava:1.0\ngateways:\n  9050: sava/webport\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava:1.0.0\n        deployable: magneticio/sava:1.0.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1\n\nDeploy using the Vamp UI\n\nIn the Vamp UI, go to the blueprints tab, click the + Create button. Paste in the above blueprint and press + Create.  \n  Vamp will store the blueprint and make it available for deployment. \n  \nPress the Deploy as button.\n  You'll be prompted to give your deployment a name, let's call it sava\n  \nPress Deploy.\n\n Deploy using the Vamp API\n\nYou can use your favorite tools like Postman (getpostman.com), HTTPie (github.com/jakubroztocil - httpie) or Curl to post this blueprint directly to the api/v1/deployments endpoint of Vamp.\n\n{{ note title=\"Note!\" }}\nTake care to set the correct Content-Type: application/x-yaml header on the POST request. Vamp is kinda\nstrict with regard to content types, because we support JSON and YAML so we need to know what you are sending.   \nIf you run on Docker machine, use docker-machine ip default instead of localhost, e.g.\n    http POST http://docker-machine ip default:8080/api/v1/deployments Content-Type:application/x-yaml < sava_1.0.yaml\n  {{ /note }}\n\nUsing curl\ncurl -v -X POST --data-binary @sava_1.0.yaml -H \"Content-Type: application/x-yaml\" http://localhost:8080/api/v1/deployments\n\nUsing httpie\nhttp POST http://localhost:8080/api/v1/deployments Content-Type:application/x-yaml < sava_1.0.yaml\n\nAfter POST-ing, Vamp should respond with a 202 Accepted message and return a JSON blob. This means Vamp is trying to deploy your container. You'll notice some parts are filled in for you, like a default scale, a default routing and of course a UUID as a name.\n\n{{ tip }}\nYou can use the RESTful API to create a deployment with a custom name - simple PUT request to http://localhost:8080/api/v1/deployments/DEPLOYMENTCUSTOMNAME\n{{ /tip }}\n\nStep 2: Check out the deployed application \n\nYou can follow the deployment process of our container by checking the /api/v1/deployments endpoint and checking when the state field changes from ReadyForDeployment to Deployed. You can also check Marathon's GUI.\n\nWhen the application is fully deployed you can check it out at Vamp host address + the port that was assigned in the blueprint, e.g: http://10.26.184.254:9050/. It should report a refreshing hipster lorem ipsum (hipsterjesus.com) upon each reload.\n\n Step 3: Get some metrics on the running application\n\nUsing a simple tool like Apache Bench (apache - ab) we can put some load on our application and see some of the metrics flowing into the dashboard. Use the following command to send 10000 requests using 15 threads to our Sava app.\n\nab -k -c 15 -n 10000 http://localhost:9050/\nor\nab -k -c 15 -n 10000 http://docker-machine ip default:9050/\n\nYou should see the metrics spike and some pretty charts being drawn:\n\nStep 4: Change scale and load-balancing\n\nVamp will automatically load-balance services. Let's change the scale of the service by selecting \"3\" in the instances field. Now Vamp will automatically scale up the number of running instances (of course permitting underlying resources) and load-balance these to the outside world using the gateway feature.\n\n Step 5: Chaos monkey\n\nNow let's try something fun. Go to the Marathon UI (on port 9090) and find the Sava container running. Now select destroy to kill the container. Watch Vamp detecting that issue and making sure that the defined number of instances is spun up again as soon as possible, while making sure the loadbalancing routing rules are also updated to reflect the changed IP's and ports of the instances.\n\n{{ note title=\"What next?\" }}\nLet's run a canary release in the second part of this getting started tutorial →\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/tutorials/index",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Tutorials\n---\n\nGetting started\nWe’ve created a set of showcase applications, services and corresponding blueprints that demonstrate Vamp’s core features - together we call them “Sava”. Sava is a mythical vampire from Serbia (wiki), but in our case it is a Github repo full of examples to help us demonstrate Vamp.\nYou can work with Sava in the Vamp hello world setup (or any other Vamp installation).\n\nDeploy your first blueprint\nRun a canary release\nSplit a monolith into services\nMerge a changed topology\n",
        "tags": []
    },
    {
        "uri": "/documentation/tutorials/merge-and-delete",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Merge and delete\n---\n\nOverview\n\nIn the previous tutorial we \"over-engineered\" our service based solution a bit (on purpose of course). We don't really need two backends services, so in this tutorial we will introduce our newly engineered solution and transition to it using Vamp blueprints and canary releasing methods.\n\n In this tutorial we will:\nGet some background and theory on merging services\nPrepare our blueprint\nTransition from blueprints to deployments (and back)\nDelete parts of the deployment\nAnswer the all important question when would I use this?\n\nIn depth\n\n step 1: Get some background and theory\n\nWhat we are going to do is create a new blueprint that is completely valid by itself and merge it\nwith our already running deployment. This might sound strange at first, but it makes sense. Why? Merging will enable us to slowly move from the previous solution to the next solution. Once moved over, we can\nremove any parts we no longer need, i.e. the former \"over-engineered\" topology.\n\nIn the diagram above, this is visualized as follows:\n\nWe have a running deployment (the blue circle with the \"1\"). To this we introduce a new blueprint\nwhich is merged with the running deployment (the pink circle with the \"2\").\nAt a point, both are active as we are transitioning from blue to pink.\nOnce we are fully on pink, we actively remove/decommission the blue part.\n\nIs this the same as a blue/green release? Yes, but we like pink better ;o)\n\nStep 2: Prepare our blueprint\n\nThe below blueprint describes our more reasonable service topology. Again, this blueprint is completely\nvalid by itself. You could just deploy it somewhere separately and not merge it with our over-engineered\ntopology. Notice the following:\n\nThe blueprint only has one backend cluster with one service.\nThe blueprint does not specify a gateway using the gateways key because we are going to use the gateway already present and configured in the running deployment. However, it would be perfectly correct to specify the old gateway - the gateway would be updated as well.\n\n---\nname: sava:1.3\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava-frontend:1.3.0\n        deployable: magneticio/sava-frontend:1.3.0\n        ports:\n          webport: 8080/http\n        environment_variables:\n          BACKEND: http://$backend.host:$backend.ports.webport/api/message\n        dependencies:\n          backend: sava-backend:1.3.0\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n  backend:\n    services:\n      breed:\n        name: sava-backend:1.3.0\n        deployable: magneticio/sava-backend:1.3.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n\nUpdating our deployment using the UI or a PUT to the /api/v1/deployments/{deployment_name} should yield a deployment with the following properties (we left some irrelevant\nparts out):\n\nTwo services in the sava cluster: the old one at 100% and the new one at 0% weight.\nThree backends in the cluster list: two old ones and one new one.\n\nSo what happened here? Vamp has worked out what parts were already there and what parts should be merged or added. This is done based on naming, i.e. the sava cluster already existed, so Vamp added a service to it at 0% weight. A cluster named \"backend\" didn't exist yet, so it was created. Effectively, we have merged\nthe running deployment with a new blueprint.\n\n Step 3: Transition from blueprints to deployments and back\n\nMoving from the old to the new topology is now just a question of \"turning the weight dial\". You\ncould do this in one go, or slowly adjust it. The easiest and neatest way is to just update the deployment as you go.\n\nVamp's API has a convenient option for this: you can export any deployment as a blueprint! By appending ?as_blueprint=true to any deployment URI, Vamp strips all runtime info and outputs a perfectly valid blueprint of that specific deployment.\n\nThe default output will be in JSON format, but you can also get a YAML format. Just set the header Accept: application/x-yaml and Vamp will give you a YAML format blueprint of that deployment.\n\n{{ note title=\"Note!\" }}\nWhen using the graphical UI, this is all taken care of.\n{{ /note }}\n\nIn this specific example, we could export the deployment as a blueprint and update the weight to a 50% to\n50% split. Then we could do this again, but with a 80% to 20% split and so on. See the abbreviated example\nbelow where we set the weight keys to 50% in both routing sections.\n\n---\nname: eb2d505e-f5cf-4aed-b4ae-326a8ca54577\nclusters:\n  sava:\n    services:\n    breed:\n        name: sava-frontend:1.2.0\n        deployable: magneticio/sava-frontend:1.2.0\n        ports:\n          webport: 8080/http\n        environment_variables:\n          BACKEND_1: http://$backend1.host:$backend1.ports.webport/api/message\n          BACKEND_2: http://$backend2.host:$backend2.ports.webport/api/message\n        constants: {}\n        dependencies:\n          backend1: sava-backend1:1.2.0\n          backend2: sava-backend2:1.2.0\n      environment_variables: {}\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n      dialects: {}\n    breed:\n        name: sava-frontend:1.3.0\n        deployable: magneticio/sava-frontend:1.3.0\n        ports:\n          webport: 8080/http\n        environment_variables:\n          BACKEND: http://$backend.host:$backend.ports.webport/api/message\n        constants: {}\n        dependencies:\n          backend: sava-backend:1.3.0\n      environment_variables: {}\n      scale:\n        cpu: 0.2\n        memory: 64MB\n        instances: 1\n      dialects: {}\n    gateways:\n      port:\n        sticky: none\n        routes:\n          sava-frontend:1.2.0:\n            weight: 50%\n          sava-frontend:1.3.0:\n            weight: 50%\n\nStep 4: Delete parts of the deployment\n\nVamp helps you transition between states and avoid \"hard\" switches, so deleting parts of a deployment is somewhat different than you might expect.\n\nIn essence, a delete is just another update of the deployment: you specify what you want to remove using a blueprint and send it to the deployment's URI using the DELETEHTTP verb: yes, it is HTTP Delete with a body, not just a URI and some id.\n\nThis means you can specifically target parts of your deployment to be removed instead of deleting the whole thing. For this tutorial we are going to delete the \"over-engineered\" old part of our deployment.\n\nCurrently, deleting works in two steps:\nSet all routings to weight: 0% of the services you want to delete with a simple update.\nExecute the delete.\n\n{{ note title=\"Note!\" }}\nYou need to explicitly set the routing weight of the service you want to deploy to zero before deleting. Here is why: When you have, for example, four active services divided in a 25/25/20/30 split and you delete the one with 30%, Vamp doesn't know how you want to redistribute the \"left over\" 30% of traffic. For this reason the user should first explicitly divide this and then perform the delete.\n{{ /note }}\n\nSetting to zero\n\nWhen you grab the YAML version of the deployment, just like above, you can set all the weight entries for the Sava 1.2.0 versions to 0 and update the deployment as usual. See the cleaned up example and make sure to adjust the name to your specific situation.\n\n---\nname: 125fd95c-a756-4635-8e1a-361085037870\nclusters:\n  backend1:\n    services:\n    breed:\n        ref: sava-backend1:1.2.0\n    gateways:\n      routes:\n        sava-backend1:1.2.0:\n          weight: 0%\n  backend2:\n    services:\n    breed:\n        ref: sava-backend2:1.2.0\n    gateways:\n      routes:\n        sava-backend2:1.2.0:\n          weight: 0%\n  sava:\n    services:\n    breed:\n        ref: sava-frontend:1.3.0\n\n    breed:\n        ref: sava-frontend:1.2.0\n    gateways:\n      routes:\n        sava-frontend:1.3.0:\n          weight: 100%\n        sava-frontend:1.2.0:\n          weight: 0%\n\nDoing the delete\n\nNow, you can take the exact same YAML blueprint or use one that's a bit cleaned up for clarity and send it in the body of the DELETE to the deployment resource, e.g. /api/v1/deployments/125fd95c-a756-4635-8e1a-361085037870.\n\n{{ note title=\"Note!\" }}\nUsing our UI you can delete parts of your deployment by using the “Remove from” function under the Blueprint tab.\n{{ /note }}\n\n---\nname: sava:1.2\nclusters:\n  sava:\n    services:\n      breed:\n        ref: sava-frontend:1.2.0\n  backend1:\n    services:\n      breed:\n        ref: sava-backend1:1.2.0\n  backend2:\n    services:\n      breed:\n        ref: sava-backend2:1.2.0\n\n{{ note title=\"Note!\" }}\nWe removed the deployable, environment_variables, ports and some other parts of the blueprint. These are actually not necessary for updating or deletion. Besides that, this is actually exactly the same blueprint we used to initially deploy the \"old\" topology.\n{{ /note }}\n\nYou can check the result in the UI: you should be left with just one backend and one frontend:\n\n Step 5: When would I use this?\n\nSounds cool, but when would I use this in practice? Well, basically anytime you release something new!\nFor example a bugfix release for a mobile API that \"didn't change anything significantly\"? You could test\nthis separately and describe it in its own blueprint. After testing, you would merge that exact same blueprint\nwith your already running production version (the one without the bugfix) and slowly move over to new version.\n\nNew major release of your customer facing app? You probably also have some new dependencies that come with that\nrelease. You create some containers and write up a blueprint that describes this new situation, run it in acceptance and test and what have you. Later, you merge it into your production setup, effectively putting it next to it and then slowly move from the old situation to the new situation, including dependencies.\n\n{{ note title=\"What next?\" }}\nThis is the end of this initial getting started tutorial. We haven't done anything with Vamp's SLA's yet, scaling or dictionary system, so there is much more to come!\nVamp use cases\nFind out how to install a production-grade set up of Vamp\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/tutorials/run-a-canary-release",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Run a canary release\n---\n\nOverview\n\nIn the previous tutorial we deployed our app sava 1.0. If you haven't walked through that part already, please do so before continuing. \n\nNow let's say we have a new version of this great application that we want to canary release into production. We have it containerised as magneticio/sava:1.1.0 and are ready to go. \n\n In this tutorial we will:\n\nPrepare our blueprint\nDeploy the new version of our application next to the old one\nUse conditions to target specific groups\nLearn a bit more about conditions\n\nIn depth\n\n Step 1: Prepare our blueprint\n\nVamp allows you to do canary releases using blueprints. Take a look at the YAML example below. It is quite similar to the blueprint we initially used to deploy sava 1.0.0. However, there are two big differences.\n\nThe services key holds a list of breeds: one for v1.0.0 and one for v1.1.0 of our app. Breeds are Vamp's way of describing static artifacts that can be used in blueprints.\nWe've added the routing key which holds the weight of each service as a percentage of all requests. Notice we assigned 50% to our current version 1.0.0 and 50% to the new version 1.1.0 We could also start with a 100% to 0% split, a 99% to 1% split or whatever combination you want as long as all percentages add up to 100% in total.\n\n---\nname: sava:1.0\ngateways:\n  9050: sava/webport\nclusters:\n  sava:\n    gateways:\n      routes:\n        sava:1.0.0:\n          weight: 50%  # weight in percentage\n        sava:1.1.0:\n          weight: 50%\n    services: # services is now a list of breeds\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\n      breed:\n          name: sava:1.1.0 # a new version of our service\n          deployable: magneticio/sava:1.1.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\nYou could also just leave out the whole routing section and use the UI to change the weights after we've done the deployment.\n\n{{ tip }}\nThere is nothing stopping you from deploying three or more versions and distributing the weight\namong them. Just make sure that when doing a straight three-way split you give one service 34% as 33%+33%+33%=99%.\n{{ /tip }}\n\nStep 2: Deploy the new version of our application next to the old one\n\nIt is our goal to update the already running deployment with the new blueprint. Vamp will figure out that v1.0.0\nis already there and just add v1.1.0 while setting the correct routing between these services.\n\n{{ tip }}\nYou could also create a second blueprint with the new service, and merge this new service to the Sava-cluster so it becomes available for routing traffic to it.\n{{ /tip }}\n\n Deploy using the UI\n\nGo to the deployment detail screen and press the Edit button. \nCopy the blueprint above and paste it over the the deployment that is there. \nPress Save \n\nVamp will start working out the differences and update the deployment accordingly.\n\nDeploy using the API\n\nGet the running deployment's name (the UUID) from /api/v1/deployments (or use the explicit name that you used for the deployment).\nPUT the blueprint to that resource, e.g: /api/v1/deployments/e1c99ca3-dc1f-4577-aa1b-27f37dba0325\n\n Check the deployment and routing\nWhen Vamp has finished deploying, you can start refreshing your browser at the correct endpoint, e.g. http://192.168.99.100:9050/. The application should switch between responding with a 1.0 page and a 1.1 page.\n\n{{ note title=\"Note!\"}}\nThis works best with the \"Incognito\" or \"Anonymous\" mode of your browser because of the caching of static assets.\n{{ /note }}\n\nStep 3: Use conditions to target specific groups\n\nUsing percentages to divide traffic between versions is already quite powerful, but also very simplistic.\nWhat if, for instance, you want to specifically target a group of users? Or a specific channel of requests\nfrom an internal service? Vamp allows you to do this right from the blueprint DSL.\n\nLet's start simple: We will allow only Chrome users to access v1.1.0 of our application by inserting this routing scheme:\n\n---\nroutes:\n  sava:1.1.0:\n    weight: 0%\n    filter_strength: 100%\n    filters:\n    condition: User-Agent = Chrome\n\nNotice three things:\n\nWe inserted a list of conditions (with only one condition for now).\nWe set the filter strength to 100% (it would be also by default set to 100%). This is important because we want all Chrome users to access the new service - we could also say filter_strength: 50% to give access just to half of them (the other 50% would be redirected to weight rules and routed accordingly).\nWe set the weight to 0% because we don't want any other users to access sava:1.1.0\n\nThe first service where the filter matches the request will be used to handle the request. \nMore information about using filters, weights, sticky sessions etc..  \nOur full blueprint now looks as follows:\n\n---\nname: sava:1.0\ngateways:\n  9050: sava/webport\nclusters:\n  sava:\n    gateways:\n      routes:\n        sava:1.0.0:\n          weight: 100%\n        sava:1.1.0:\n          weight: 0%\n          condition_strength: 100%\n          condition: User-Agent = Chrome\n    services:  services is now a list of breeds\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\n      breed:\n          name: sava:1.1.0 # a new version of our service\n          deployable: magneticio/sava:1.1.0\n          ports:\n            webport: 8080/http\n        scale:\n          cpu: 0.2\n          memory: 64MB\n          instances: 1\n\nUsing the UI, you can either use the Edit deployment button again and completely paste in this blueprint or just\nfind the right place in the blueprint and edit it by hand. The result should be the same as using our UI to insert a filter condition:\n\nAs we are not actually deploying anything but just reconfiguring routes, the update should be almost instantaneous. You can fire up a Chrome browser and a Safari browser and check the results. A hard refresh might be necessary because of your browser's caching routine.\n\nStep 4: Learn a bit more about conditions\n\nOur browser example is easily testable on a laptop, but of course a bit contrived. Luckily you can\ncreate much more powerful filters quite easily. Checking Headers, Cookies, Hosts etc. is all possible.\nUnder the hood, Vamp uses Haproxy's ACL's (cbonte.github.io/haproxy-dconv/configuration-1.5 - ACL basics) and you can use the exact ACL definition right in the blueprint in the condition field of a filter.\n\n Vamp short codes\n\nACLs can be somewhat opaque and cryptic. That's why Vamp has a set of convenient \"short codes\"\nto address common use cases. Currently, we support the following, but we will be expanding on this in the future:\n\nUser-Agent = string\nHost = string\nCookie cookie name Contains string\nHas Cookie cookie name\nMisses Cookie cookie name\nHeader header name Contains string\nHas Header header name\nMisses Header header name\n\nVamp is also quite flexible when it comes to the exact syntax. This means the following are all equivalent:\n\nhdr_sub(user-agent) Android   # straight ACL\nuser-agent=Android            # lower case, no white space\nUser-Agent=Android            # upper case, no white space\nuser-agent = Android          # lower case, white space\n\nMultiple conditions in a filter\nHaving multiple conditions in a filter is perfectly possible, all filters will be implicitly\n\"OR\"-ed together, as in \"if the first filter doesn't match, proceed to the next\".   \n\nIn the example below, the filter would first check whether the string \"Chrome\" exists in the User-Agent header of a\nrequest. If that doesn't result in a match, it would check whether the request has the header\n\"X-VAMP-TUTORIAL\". So any request matching either condition would go to this service.\n\n---\nroutes:\n  sava:1.1.0:\n    filter_strength: 100%\n    filters:\n    condition: User-Agent = Chrome\n    condition: Has Header X-VAMP-TUTORIAL\n\nUsing a tool like httpie (github.com/jakubroztocil - httpie) makes testing this a breeze.\n\n    http GET http://10.26.184.254:9050/ X-VAMP-TUTORIAL:stuff\n\n{{ note title=\"What next?\" }}\nCool stuff. But we are dealing here with single, monolithic applications. Where are the microservices?  We will chop up this monolith into services and deploy them with Vamp in the third part of our tutorial →\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/tutorials/split-a-monolith",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Split a monolith\n---\nOverview\n\nIn the previous tutorial we did some basic canary releasing on two versions of a monolithic application. Very nice, but Vamp isn't\ncalled the Very Awesome Microservices Platform for nothing. The next step is to split our monolithic Sava application into separate services.\n\n In this tutorial we will:\n\ndefine a new service topology\nlearn about environment variables and service discovery\n\nIn depth\n\n Step 1: Define a new service topology\n\nTo prove our point, we are going to slightly \"over-engineer\" our services solution. This will also help\nus demonstrate how we can later remove parts of our solution using Vamp. For now, we'll split the\nmonolith into a topology of one frontend and two separate backend services. After our engineers\nare done with coding, we can catch this new topology in the following blueprint. Please notice a couple\nof things:\n\nWe now have three clusters: sava, backend1 and backend2. Each cluster could have multiple\nservices on which we could do separate canary releases and set separate filters.\nThe sava cluster has explicit dependencies on the two backends. Vamp will make sure these dependencies\nare checked and rolled out in the right order.\nUsing environment_variables we connect the dynamically assigned ports and hostnames of the backend\nservices to the \"customer facing\" sava service.\nWe've change the gateway port to 9060 so it doesn't collide with the  monolithic deployment.\n\n---\nname: sava:1.2\ngateways:\n  9060: sava/webport\nclusters:\n  sava:\n    services:\n      breed:\n        name: sava-frontend:1.2.0\n        deployable: magneticio/sava-frontend:1.2.0\n        ports:\n          webport: 8080/http                \n        environment_variables:\n          BACKEND_1: http://$backend1.host:$backend1.ports.webport/api/message\n          BACKEND_2: http://$backend2.host:$backend2.ports.webport/api/message\n        dependencies:\n          backend1: sava-backend1:1.2.0\n          backend2: sava-backend2:1.2.0\n      scale:\n        cpu: 0.2      \n        memory: 64MB\n        instances: 1               \n  backend1:\n    services:\n      breed:\n        name: sava-backend1:1.2.0\n        deployable: magneticio/sava-backend1:1.2.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1              \n  backend2:\n    services:\n      breed:\n        name: sava-backend2:1.2.0\n        deployable: magneticio/sava-backend2:1.2.0\n        ports:\n          webport: 8080/http\n      scale:\n        cpu: 0.2       \n        memory: 64MB\n        instances: 1\n\nDeploy this blueprint using either the UI or a REST call and when deployed check out the new topology in your browser (on port 9060 this time). When deployed it should yield something similar to:\n\nStep 2: Learn about environment variables and service discovery\n\nIf you were to check out the Docker containers using docker inspect, you would see the environment variables that we set in the blueprint.\n\n docker inspect 66e64bc1c8ca\n...\n\"Env\": [\n    \"BACKEND_1=http://172.17.42.1:33021/api/message\",\n    \"BACKEND_2=http://172.17.42.1:33022/api/message\",\n    \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n],\n...\n\nHost names and ports are configured at runtime and injected in the right parts of your running deployment. Your service/app should pick up these variables to configure itself. Luckily, this is quite easy and common in almost all languages and frameworks.\n\n{{ tip }}\nRemember, there is no \"point-to-point\" wiring. The exposed host and port are actually service\nendpoints. The location, amount and version of containers running behind that service endpoint can vary.\nLearn more about how Vamp does service discovery →\n{{ /tip }}\n\n{{ note title=\"What next?\" }}\nGreat! We just demonstrated that Vamp can handle dependencies between services and configure these services with host and port information at runtime. Now let's do a more complex migration to a new service based topology →.\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/artifacts",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Artifacts\n---\n\nVamp has a few basic entities or artifacts you can work with, these can be classed as static resource descriptions and dynamic runtime entities. Note that API actions on static resource descriptions are mostly synchronous, while API actions on dynamic runtime entities are largely asychronous.\n\nStatic resource descriptions\n\nBreeds describe single services and their dependencies.  Read more...\nBlueprints are, well, blueprints! They describe how breeds work in runtime and what properties they should have.  Read more...  \n\n Dynamic runtime entities\n\nDeployments are running blueprints. You can have many deployments from one blueprint and perform actions on each at runtime. Plus, you can turn any running deployment into a blueprint.  Read more...  \nGateways are the \"stable\" routing endpoint - defined by a port (incoming) and routes (outgoing).  Read more... \nWorkflows are apps (services) deployed on cluster, used for dynamically changing the runtime configuration (e.g. SLA, scaling, condition weight update).  Read more...\n\nWorking across multiple teams\n\nIn larger companies with multiple teams working together on a large project, all required information is often not available at the same time. To facilitate this style of working, Vamp allows you to set placeholders. Placeholders let you communicate with other teams using simple references and gradually build up a complicated deployment. Vamp will only check references at deployment time, this means:\n\nBreeds can be referenced in blueprints before they exist \nYou do not need to know the contents of an SLA when you reference it.\nYou can reference a variable that someone else should fill in.\n\nRead more about referencing artifacts and environment variables.\n\n{{ note title=\"What next?\" }}\nRead about Vamp breeds\nCheck the API documentation\nTry Vamp\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/blueprints",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Blueprints\n---\n\nBlueprints are execution plans - they describe how your services should be hooked up and what their topology should look like at runtime. This means you reference your breeds (or define them inline) and add runtime configuration to them.\n\nBlueprints allow you to add the following extra properties:\n\nGateways: a stable port where the service can be reached.\nClusters and services: a cluster is a grouping of services with one purpose, i.e. two versions (a/b) of one service.\nEnvironment variables: a list of variables (interpolated or not) to be made available at runtime.\nDialects: a dialect is a set of native commands for the underlying container platform, i.e. Docker or Mesosphere Marathon.\nScale: the CPU and memory and the amount of instance allocate to a service.\nConditions: how traffic should be directed based on HTTP and/or TCP properties.\nSLA and escalations: SLA definition that controls autoscaling.\n\nExample - key concepts of blueprints\n\n---\nname: my_blueprint                         Custom blueprint name\ngateways:\n  8080/http: my_frontend/port\nclusters:\n  my_frontend:                            # Custom cluster name.\n  \n    gateways:                             # Gateway for this cluster services.\n      routes:                             # Makes sense only with\n        somecoolbreed:                  # multiple services per cluster.\n          weight: 95%\n          condition: User-Agent = Chrome\n        someotherbreed:                 # Second service.\n          weight: 5%\n          \n    services:                             # List of services\n      breed:\n          ref: somecoolbreed\n        scale:                            # Scale for this service.\n          cpu: 2                          # Number of CPUs per instance.\n          memory: 2048MB                  # Memory per instance (MB/GB units).\n          instances: 2                    # Number of instances\n      breed: \n          ref: someotherbreed           # Another service in the same cluster.  \n        scale: large                      # Notice we used a reference to a \"scale\". \n                                          # More on this later.\n\nGateways\n\nA gateway is a \"stable\" endpoint (or port in simplified sense) that almost never changes. When creating the mapping, it uses the definition (my_frontend/port in this case) from the \"first\" service in the cluster definition you reference. This service can of course be changed, but the gateway port normally doesn't.\n\nPlease take care of setting the /tcp or /http (default) type for the port. Using /http allows Vamp to record more relevant metrics like response times and metrics.\n\nRead more about gateways.\n\n{{ note title=\"Note!\" }}\ngateways are optional. You can just deploy services and have a home grown method to connect them to some stable, exposable endpoint.\n{{ /note }}\n\n Clusters and services\n\nIn essence, blueprints define a collection of clusters.\nA cluster is a group of different services, which will appear as a single service and serve a single purpose.\n\nCommon use cases would be service A and B in an A/B testing scenario - usually just different\nversions of the same service (e.g. canary release or blue/green deployment).\n\nClusters are configured by defining an array of services. A cluster can be given an arbitrary name. Services are just lists or arrays of breeds.\n\n---\nmycoolcluster\n  services\n   breed: \n      ref: mycoolservice_A      # reference to an existing breed\n   breed:                       # shortened inline breed\n       name: mycoolservice_B\n       deployable: some_container\n       ...\n\nClusters and services are just organisational items. Vamp uses them to order, reference and control the actual containers and gateways and traffic.\n\n This all seems redundant, right? We have a reference chain of blueprints - gateways - clusters - services - breeds - deployable. However, you need this level of control and granularity in any serious environment where DRY principles are taken seriously and where \"one size fits all\" doesn't fly.\n\nDialects\n\nVamp allows you to use container driver specific tags inside blueprints. We call this a “dialect”.  Dialects effectively enable you to make full use of, for instance, the underlying features like mounting disks, settings commands and providing access to private Docker registries.\n\nWe currently support the following dialects:\n\ndocker:\nmarathon:\n\n Docker dialect\n\nThe following example show how you can mount a volume to a Docker container using the Docker dialect.\n\nExample blueprint - using the Docker dialect\n\n---\nname: busybox\nclusters:\n  busyboxes:\n    services:\n      breed:\n        name: busybox-breed\n        deployable: busybox:latest\n      docker:\n        Volumes:\n          \"/tmp\": ~\n\nVamp will translate this into the proper API call. Inspecting the container after it's deployed should show something similar to this:\n\n...\n\"Volumes\": {\n      \"/tmp\": \"/mnt/sda1/var/lib/docker/volumes/1a3923fa6108cc3e19a7fe0eeaa2a6c0454688ca6165d1919bf647f5f370d4d5/_data\"\n  },\n...    \n\n Marathon dialect\n\nThis is an example with Marathon that pulls an image from private repo, mounts some volumes, sets some labels and gets run with an ad hoc command: all taken care of by Marathon.\n  \nWe can provide the marathon: tag either on the service level, or the cluster level. Any marathon: tag set on the service level will override the cluster level as it is more specific. However, in 9 out of 10 cases the cluster level makes the most sense. Later, you can also mix dialects so you can prep your blueprint for multiple environments and run times within one description.\n\nexample blueprint - using the Marathon dialect\n\nNotice the following:\n\nUnder the marathon: tag, we provide the command to run in the container by setting the cmd: tag.\nWe provide a url to some credentials file in the uri array. As described in the Marathon docs (mesosphere.github.io/marathon - using a private Docker repository) this enables Mesos\nto pull from a private registry, in this case registry.example.com where these credentials are set up.\nWe set some labels with some arbitrary metadata.\nWe mount the /tmp to in Read/Write mode.\n\n---\nname: busy-top:1.0\nclusters:\n  busyboxes:\n    services:\n      breed:\n        name: busybox\n        deployable: registry.example.com/busybox:latest\n      marathon:\n       cmd: \"top\"\n       uris:\n         \"https://somehost/somepath/somefilewithdockercredentials\"\n       labels:\n         environment: \"staging\"\n         owner: \"buffy the vamp slayer\"\n       container:\n         volumes:\n           containerPath: \"/tmp/\"\n             hostPath: \"/tmp/\"\n             mode: \"RW\"\n\n Scale\n\nScale is the \"size\" of a deployed service. Usually that means the number of instances (servers) and allocated CPU and memory.\n\nScales can be defined inline in a blueprint or they can defined separately and given a unique name. The following example is a scale named \"small\". POST-ing this scale to the /scales REST API endpoint will store it under that name so it can be referenced from other blueprints.\n\nExample scale\n\n---\nname: small    Custom name.\n\ncpu: 2        # Number of CPUs per instance.\nmemory: 2gb   # Memory per instance, MB/GB units.\ninstances: 2  # Number of instances.\n\n{{ note title=\"What next?\" }}\nRead about Vamp deployments\nCheck the API documentation\nTry Vamp\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/breeds",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Breeds \n---\nBreeds are static descriptions of applications and services available for deployment. Each breed is described by the DSL in YAML notation or JSON, whatever you like. This description includes name, version, available parameters, dependencies etc.\nTo a certain degree, you could compare a breed to a Maven artifact or a Ruby Gem description.\n\nBreeds allow you to set the following properties:\n\nDeployable: the name of actual container or command that should be run.\nPorts: a map of ports your container exposes.\nEnvironment variables: a list of variables (interpolated or not) to be made available at runtime.\nDependencies: a list of other breeds this breed depends on.\n\nDeployable\n\nDeployables are pointers to the actual artifacts that get deployed. Vamp supports Docker containers or can support any other artifacts supported by your container manager. \n\n Example breed - deploy a Docker container\n\n---\nname: my_breed:0.1\ndeployable: company/myfrontendservice:0.1\n\nports:\n  web: 8080/http   \n\nThis breed, with a unique name, describes a deployable and the port it works on. \n\nDocker deployables\n\nBy default, the deployable is a Docker container. \nWe could also make this explicit by setting type to docker. The following statements are equivalent:\n\n---\ndeployable: company/myfrontendservice:0.1\n\n---\ndeployable: \n  type: docker\n  definition: company/myfrontendservice:0.1\n\nThis shows the full (expanded) deployable with type and definition.\n\nDocker images are pulled by your container manager from any of the repositories configured. By default that would be the public Docker hub, but it could also be a private repo.\n\n Other deployables\n\nRunning \"other\" artifacts such as zips or jars heavily depends on the underlying container manager.\nWhen Vamp is set up to run with Marathon (mesosphere.github.io - Marathon), command (or cmd) deployable types can be used.\nIn that case cmd (Marathon REST API - post v2/apps) parameter will have value of deployable.\n\nExample breed - run a custom jar after it has been downloaded \nCombining this definition and the Vamp Marathon dialect  uris parameter allows the requested jar to be downloaded from a remote location (Marathon REST API - uris Array of Strings). \n\n---\nname: location\nclusters:\n  api:\n    services:\n      breed:\n        name: location\n        deployable: \n          type: cmd\n          definition: java -jar location.jar\n      marathon:\n        uris: [\"https://myrepolocation_jar\"]\n\n JavaScript deployables \n\nBreeds can have type application/javascript and definition should be a JavaScript script:\n\n---\nname: hello-world\ndeployable:\n  type: application/javascript\n  definition: |\n    console.log('Hello World Vamp!');\n\nIt is possible to create or update breeds with the API request POST|PUT /api/v1/breeds/{name}, Javascript script as body and header Content-Type: application/javascript.\n\nPorts\n\nThe ports property is an array of named ports together with their protocol. It describes on what ports the deployables is offering services to the outside world. Let's look at the following breed:\n\n---\nname: my_breed:0.1\ndeployable: company/myfrontendservice:0.1\n\nports:\n  web: 8080/http\n  admin: 8081/http\n  redis: 9023/tcp   \n\nPorts come in two flavors:\n\n/http HTTP ports are the default type if none is specified. They are always recommended when dealing with HTTP-based services. Vamp can record a lot of interesting metrics like response times, errors etc. Of course, using /tcp will work but you miss out on cool data.\n/tcp Use TCP ports for things like Redis, MySQL etc.\n\n{{ note title=\"Note!\" }}\n/http notation for ports is required for use of filters.\n{{ /note }}\n\nNotice we can give the ports sensible names. This specific deployable has web port for customer traffic, an admin port for admin access and a redis port for some caching probably. These names come in handy when we later compose different breeds in blueprints.\n\n{{ note title=\"What next?\" }}\nRead about Vamp blueprints\nCheck the API documentation\nTry Vamp\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/conditions",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Conditions\n---\n\nCreating conditions is quite easy. Checking Headers, Cookies, Hosts etc. is all possible.\nUnder the hood, Vamp uses Haproxy's ACL's (cbonte.github.io/haproxy-dconv - 7.1 ACL basics) and you can use the exact ACL definition right in the blueprint in the condition field of a condition.\n\nHowever, ACL's can be somewhat opaque and cryptic. That's why Vamp has a set of convenient \"short codes\"\nto address common use cases. Currently, we support the following:\n\n| description           | syntax                       | example                  |\n| ----------------------|:----------------------------:|:------------------------:|\n| match user agent      | user-agent == value          | user-agent == Firefox    |\n| mismatch user agent   | user-agent != value          | user-agent != Firefox    |\n| match host            | host == value                | host == localhost        |\n| mismatch host         | host != value                | host != localhost       |\n| has cookie            | has cookie value             | has cookie vamp          |\n| misses cookie         | misses cookie value          | misses cookie vamp       |\n| has header            | has header value             | has header ETag          |\n| misses header         | misses header value          | misses header ETag       |\n| match cookie value    | cookie name has value    | cookie vamp has 12345    |\n| mismatch cookie value | cookie name misses value | cookie vamp misses 12345 |\n| header has value      | header name has value   | header vamp has 12345    |\n| header misses value   | header name misses value | header vamp misses 12345 |\n\nAdditional syntax examples: github.com/magneticio/vamp - ConditionDefinitionParserSpec.scala.\n\nVamp is also quite flexible when it comes to the exact syntax. This means the following are all equivalent:\n\nIn order to specify plain HAProxy ACL, ACL needs to be between { }:\n\ncondition: \" hdr_sub(user-agent) Chrome \"\n\nHaving multiple conditions in a condition is perfectly possible. For example, the following condition would first check whether the string \"Chrome\" exists in the User-Agent header of a\nrequest and then it would check whether the request has the header\n\"X-VAMP-MY-COOL-HEADER\". So any request matching both conditions would go to this service.\n\n---\ngateways:\n  weight: 100%\n  condition: \"User-Agent = Chrome AND Has Header X-VAMP-MY-COOL-HEADER\"\n\nUsing a tool like httpie (github.com/jkbrzt/httpie) makes testing this a breeze.\n\n    http GET http://10.26.184.254:9050/ X-VAMP-MY-COOL-HEADER:stuff\n\nBoolean expression in conditions\n\nVamp supports AND, OR, negation NOT and grouping ( ):\n\n---\ngateways:\n  weight: 100%\n  condition: (User-Agent = Chrome OR User-Agent = Firefox) AND has cookie vamp\n\nAdditional boolean expression examples: github.com/magneticio/vamp - BooleanParserSpec.scala.\n\n URL path rewrite\n\nVamp also supports URL path rewrite which can be powerful solution in defining service APIs (e.g. RESTful) outside of application service.\n\nroutes:\n  web/port1:\n    rewrites:\n    path: a if b\n  web/port2:\n    weight: 100%\n\nPath rewrite is defined in format: path: NEW_PATH if CONDITION:\n\nNEW_PATH new path to be used; HAProxy variables are supported, e.g. %[path]\nCONDITION condition using HAProxy directives, e.g. matching path, method, headers etc.\n\nVamp managed and external routes\n\nVamp managed routes are in the format:\n\ngateway - pointing to another gateway, e.g. it is possible to chain gateways\ndeployment/cluster - pointing to deployment cluster, i.e. services are not 'visible'\ndeployment/cluster/service - pointing to specific service within deployment cluster\n\nAll examples above cover only Vamp managed routes.\nIt is also possible to route traffic to specific IP or hostname and port.\nIn that case IP or hostname and port need to be specified between brackets, e.g. [hostname:port] (and double quotes due to Yaml syntax).\n\nname: mesos\nport: 8080/http\nsticky: route\n\nroutes:\n  \"[192.168.99.100:5050]\":\n    weight: 50%\n  \"[localhost:5050]\":\n    weight: 50%\n\n{{ note title=\"What next?\" }}\nRead about Vamp events\nCheck the API documentation\nTry Vamp\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/deployments",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Deployments\n---\n\nA deployment is a \"running\" blueprint. Over time, new blueprints can be merged with existing deployments or parts of the running blueprint can be removed from it. Each deployment can be exported as a blueprint and \ncopy / pasted to another environment, or even to the same environment to function as a clone.\n\nCreate a deployment\n\nYou can create a deployment in the following ways:\n\nSend a POST request to the /deployments endpoint.\nUse the UI to deploy a blueprint using the \"deploy\" button on the \"blueprints\" tab.\nUse the CLI vamp deploy command  \n $ vamp deploy my_blueprint.\n\nThe name of the deployment will be automatically assigned as a UUID (e.g. 123e4567-e89b-12d3-a456-426655440000).\n\n Vamp deployment process\n\nOnce we have issued the deployment, Vamp will do the following:\n\nUpdate Vamps internal model.\nIssue and monitor deployment commands to the container platform.\nUpdate the ZooKeeper entry.\nStart collecting metrics.\nMonitor the container platform for changes.\n\nVamp will add runtime information to the deployment model, like start times, resolved ports etc.\n\nDeployment scenarios\n\nA common Vamp deployment scenario is to introduce a new version of the service to an existing cluster, this is what we call a merge. After testing/migration is done, the old or new version can be removed from the cluster, simply called a removal. Let's look at each in turn.\n\n Merge\n\nMerging of new services is performed as a deployment update. You can merge in many ways:\n\nSend a PUT request to the /deployments/{deployment_name} endpoint.\nUse the UI to update a deployment using the \"Edit deployment\" button. \nUse the CLI with a combination of the vamp merge and vamp deploy commands.\n\nIf a service already exists then only the gateways and scale will be updated. Otherwise a new service will be added. If a new cluster doesn't exist in the deployment, it will be added.\n\nLet's deploy a simple service:\n\n---\nname: monarch_1.0\n\nclusters:\n  monarch:\n    # Specifying only a reference to the breed.\n    breed: monarch_1.0   \n\nAfter this point we may have another version ready for deployment and now instead of only one service, we have added another one:\n\n---\nname: monarch_1.1\n\nenvironment_variables:\n  # Some variable needed for our new recommendation engine,\n  # just as an example.\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    # Just a reference and this breed has one dependency:\n    # recommendation_1.0\n    breed: monarch_1.1\n\n  recommendation:\n    breed: recommendation_1.0\n \n\nNow our deployment (in simplified blueprint format) looks like this:\n\n---\nname: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 100%\n      monarch_1.1:\n        weight: 0%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    \n    gateways:\n      recommendation_1.0:\n        weight: 100%\n\nNote that the route weight for monarch_1.1 is 0, i.e. no traffic is sent to it.\nLet's redirect some traffic to our new monarch_1.1 (e.g. 10%):\n\n---\nclusters:\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 90%\n      monarch_1.1:\n        weight: 10%\n\nNote that we can omit other fields like name, parameters and even other clusters (e.g. recommendation) if the change is not relevant to them. In this example we just wanted to update the weights.\n\nIn the last few examples we have shown the following:\n\nA fresh new deployment.\nA canary release with a cluster update and change of the topology (a new cluster was added).\nAn update of the gateways for a cluster - similar to a cluster scale update (instances, cpu, memory).\n\nRemoval\n\nRemoval is done using the REST API DELETE request together with the new blueprint as request body.\nIf a service exists it will be removed, otherwise the request is ignored. If a cluster has no more services left the cluster will be removed completely. Lastly, if a deployment has no more clusters it will be completely removed (destroyed).\n\nLet's use the example from the previous section. Notice the weight is evenly distributed (50/50). \n\n---\nname: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.0\n      breed: monarch_1.1\n          \n    gateways:\n      monarch_1.0:\n        weight: 50%\n      monarch_1.1:\n        weight: 50%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    gateways:\n      recommendation_1.0:\n        weight: 100\n\nIf we are happy with the new monarch version 1.1, we can proceed with the removal of the old version.\nThis change is applied on the running deployment. We send the following YAML as the body of the DELETE request\nto the /deployments/deployment_UUID endpoint.\n\n---\nname: monarch_1.0\n\nclusters:\n  monarch:\n    breed: monarch_1.0\n\nNote that this is the same original blueprint we started with. What we are doing here is basically \"subtracting\" one blueprint from the other, although \"the other\" is a running deployment.\nAfter this operation our deployment is:\n\n---\nname: monarch_1.0\n\nenvironment_variables:\n  recommendation.route: \"/api/v1\"\n\nclusters:\n\n  monarch:\n    services:\n      breed: monarch_1.1\n    gateways:\n      monarch_1.1:\n        weight: 100%\n\n  recommendation:\n    services:\n      breed: recommendation_1.0\n    gateways:\n      recommendation_1.0:\n        weight: 100%\n\nIn a nutshell: If we say that the first version was A and the second B, then we just did the migration from A to B without downtime:\nA** - A + B - A + B - A - **B\n\nWe could also remove the newer version (monarch_1.1 with/without recommendation cluster) in case that it didn't perform as we expected.\n\n{{ note title=\"What next?\" }}\nRead about Vamp environment variables\nCheck the API documentation\nTry Vamp\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/environment-variables",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Environment variables \n---\n\nBreeds and blueprints can include lists of environment variables that will be injected into the container at runtime. You set environment variables with the environment_variables keyword or its shorter version env, e.g. both examples below are equivalent.\n\n---\nenvironment_variables:\n  PORT: 8080\n\n---\nenv:\n  PORT: 8080\n\nBreeds can also have dependencies on other breeds. These dependencies should be stated explicitly, similar to how you would do in a Maven pom.xml, a Ruby Gemfile or similar package dependency systems, i.e:\n\n---\ndependencies:\n  cache: redis:1.1\n\nIn a lot of cases, dependencies coexist with interpolated environment variables or constants because exact values are not known untill deploy time.\n\n'Hard' setting a variable\n\nYou want to \"hard set\" an environment variable, just like doing an export MYVAR=somevalue in a shell. This  variable could be some external dependency you have no direct control over: the endpoint of some service you use that is out of your control. It can also be some setting you want to tweak, like JVMHEAPSIZE or AWS_REGION.\n\n---\nname: javaawsapp:1.2.1\ndeployable: acmecorp/tomcat:1.2.1\n\nenvironment_variables:\n  JVMHEAPSIZE: 1200\n  AWS_REGION: 'eu-west-1'     \n\nIt is also possible to use wildcard * at the end of the name:\n\n---\ndependencies:\n  cache: redis:1.*\n\nThis will match any breed name that starts with redis:1.\n\n Using place holders\n\nUse the ~ character to define a place holder for a variable that should be filled in at runtime (i.e. when this breed actually gets deployed), but for which you do not yet know the actual value. \n\n{{ tip title=\"Typical use case\" }}\nWhen different roles in a company work on the same project. Developers can create place holders for variables that operations should fill in: it helps with separating responsibilities.\n{{ /tip }}\n\nExample - ORACLE_PASSWORD designated as a place holder\n\n---\nname: javaawsapp:1.2.1\ndeployable: acmecorp/tomcat:1.2.1\nenvironment_variables:\n  JVMHEAPSIZE: 1200\n  AWS_REGION: 'eu-west-1' \n  ORACLE_PASSWORD: ~    \n\n Resolving variables\n\nUse the $ character to reference other statements in a breed/blueprint. This allows you to dynamically resolve ports and hosts names that we don't know until a deployment is done. You can also resolve to hard coded and published constants from some other part of the blueprint or breed, typically a dependency.\n\n{{ note title=\"Note!\" }}\nThe $ value is escaped by $$. A more strict notation is ${some_reference}\n{{ /note }}\n\nVamp host variable\n\nVamp provides just one magic* variable: the host. This resolves to the host or ip address of the referenced service. Strictly speaking, the host reference resolves to the gateway agent endpoint, but users do not need to concern themselves with this. Users can think of one-on-one connections where Vamp actually does server-side service discovery to decouple services.\n\n Example - resolving variables from a dependency\n\n---\nname: blueprint1\nclusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      deployable: acmecorp/tomcat:1.2.1      \n    environment_variables:\n      MYSQL_HOST: $backend.host        # resolves to a host at runtime\n      MYSQL_PORT: $backend.ports.port  # resolves to a port at runtime\n    dependencies:\n      backend: mysql:1.0\n  backend:\n    breed: mysql:1.0\n\nExample - resolving variables from a dependency's environment variables\nWhat if the backend is configured through some environment variable, but the frontend also needs that information? For example, the encoding type for our database. We can just reference that environment variable using the exact same syntax.\n\n---\nname: blueprint1\nclusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      deployable: acmecorp/tomcat:1.2.1      \n    environment_variables:\n      MYSQL_HOST: $backend.host       \n      MYSQL_PORT: $backend.ports.port \n      BACKENDENCODING: $backend.environmentvariables.ENCODING_TYPE\n    dependencies:\n      backend: mysql:1.0\n  backend:\n    breed: mysql:1.0\n    environment_variables:\n      ENCODING_TYPE: 'UTF8'    injected into the backend MySQL container\n\nYou can do everything with environment_variables but constants (see below) allow you to just be a little bit cleaner with regard to what you want to expose and what not.\n\nEnvironment variable scope\n\nEnvironment variables can live on different scopes and can be overridden by scopes higher in the scope hierarchy.\nA scope is an area of your breed or blueprint definition that limits the visibility of variables and references inside that scope.\n\nBreed scope: The scope used in all the above examples is the default scope. If you never define any environment_variables in any other place, this will be used.\n\nCluster scope: Will override the breed scope and is part of the blueprint artifact. Use this to override environment variables for all services that belong to a cluster.\n\nService scope: Will override breed scope and cluster scope, and is part of the blueprint artifact. Use this to override all environment variables for a specific service within a cluster.\n\n{{ note title=\"Note!\" }} \nEffective use of scope is completely dependent on your use case. The various scopes help to separate concerns when multiple people and/or teams work on Vamp artifacts and deployments and need to decouple their effort.\n{{ /note }}\n\n Examples of scope use\n\nRun two of the same services with different configurations\nOverride the JVMHEAPSIZE in production\nUse a place holder\nJust for fun - combine all scopes and references\n\nExample 1\nRun two of the same services with different configurations\n\nUse case: As a devOps-er you want to test one service configured in two different ways at the same time. Your service is configurable using environment variables. In this case we are testing a connection pool setting. \n\nImplementation: In the below blueprint we just use the breed level environment variables. The traffic is split into a 50/50 divide between both services.\n\n---\nname: production_deployment:1.0\ngateways:\n  9050: frontend/port\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0-a\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            CONNECTION_POOL: 30                \n      breed:\n          name: frontend_app:1.0-b               different breed name, same deployable\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            CONNECTION_POOL: 60                 # different pool size\n\n  gateways:\n    frontend_app:1.0-a:\n      weight: 50% \n    frontend_app:1.0-b:\n      weight: 50% \n\nExample 2\nOverride the JVM_HEAP_SIZE in production\n\nUse case: As a developer, you created your service with a default heap size you use on your development laptop and maybe on a test environment. Once your service goes \"live\", an ops guy/gal should be able to override this setting.\n\nImplementation: In the below blueprint we override the variable JVMHEAPSIZE for the whole frontend cluster by specifically marking it with .dot-notation cluster.variable \n\n---\nname: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                   cluster level variable \n  frontend.JVMHEAPSIZE: 2800          # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0\n          deployable: acmecorp/tomcat:1.2.1\n          ports:\n            port: 8080/http\n          environment_variables:           \n            JVMHEAPSIZE: 1200         # will be overridden by deployment level: 2800\n\nExample 3\nUse a place holder\n\nUse case: As a developer, you might not know some value your service needs at runtime, say the Google Anaytics ID your company uses. However, your Node.js frontend needs it! \n\nImplementation: In the below blueprint the ~ place holder is used to explicitly demand that a variable is set by a higher scope. When this variable is NOT provided, Vamp will report an error at deploy time.\n\n---\nname: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                             cluster level variable \n  frontend.GOOGLEANALYTICSKEY: 'UA-53758816-1'  # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          name: frontend_app:1.0\n          deployable: acmecorp/node_express:1.0\n          ports:\n            port: 8080/http\n          environment_variables:           \n            GOOGLEANALYTICSKEY: ~               # If not provided at higher scope, \n                                                  # Vamp reports error.\n\nExample 4\nCombine all scopes and references\n\nAs a final example, let's combine some of the examples above and include referenced breeds. In this case, we have two breed artifacts already stored in Vamp and include them by using the ref keyword.\n\nIn the below blueprint:  \n\nwe override all breed scope JVMHEAPSIZE variables with cluster scope environment_variables\nto further tweak the JVMHEAPSIZE for the service frontendapp:1.0-b, we also add service scope environmentvariables for that service.\n\n---\nname: production_deployment:1.0\ngateways:\n  9050: frontend/port\nenvironment_variables:                             cluster level variable \n  frontend.JVMHEAPSIZE: 2400                    # overrides the breed level\nclusters:\n  frontend:\n    services:\n      breed:\n          ref: frontend_app:1.0-a\n      breed:\n          ref: frontend_app:1.0-b        \n        environment_variables:           \n          JVMHEAPSIZE: 1800               # overrides the breed level AND cluster level\n          \n  gateways:\n    frontend_app:1.0-a:\n      weight: 50% \n    frontend_app:1.0-b:\n      weight: 50% \n\nConstants\n\nSometimes you just want configuration information to be available in a breed or blueprint. You don't need that information to be directly exposed as an environment variable. As a convenience, Vamp allows you to set constants.\nThese are values that cannot be changed during deploy time.\n\nYou can do everything with environment_variables but constants allow you to just be a little bit cleaner with regard to what you want to expose and what not.\n\n Exammple - using constants\n\n---\nclusters:\n  frontend:\n    breed:\n      name: frontend_app:1.0\n      environment_variables:\n        MYSQL_HOST: $backend.host       \n        MYSQL_PORT: $backend.ports.port \n        BACKENDENCODING: $backend.environmentvariables.ENCODING_TYPE\n        SCHEMA: $backend.constants.SCHEMA_NAME\n      dependencies:\n        backend: mysql:1.0\n  backend:\n    breed:\n      name: mysql:1.0\n      environment_variables:\n        ENCODING_TYPE: 'UTF8'\n      constants:\n        SCHEMA_NAME: 'customers'    # NOT injected into the backend MySQL container\n\n{{ note title=\"What next?\" }}\nRead about Vamp gateways\nCheck the API documentation\nTry Vamp\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/escalations",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Escalations\n\n---\n\nAn escalation is a workflow triggered by an escalation event. Vamp checks for these escalation events using a continuous background process with a configurable interval time. If the events match the escalation handlers defined in the DSL, the action is executed.\n\nAgain: escalation events can be generated by third party systems and they will be handled in the same manner as events created by Vamp SLA workflows. \n\nEscalation handlers\n\nAny escalation that is triggered should be handled by an escalation handler\n\nVamp ships with the following set of escalation handlers - scaleinstances, scalecpu and scale_memory. These handlers can be composed into intricate escalation systems.\n\n scale_instances   \nScales up the number of running instances. It is applied only to the first service in the cluster (old or \"A\" version). You can set upper limits to how far you want to scale out or in, effectively guaranteeing a minimum set of running instances. This is very much like AWS auto-scaling.  \nExample - scale_instances\n---\ntype: scale_instances\ntarget: monarch   Target cluster for the scale up/down.\n                 # If it's not specified, by default it's the \n                 # current cluster where SLA escalations are \n                 # specified.\nminimum: 1       # Minimum number of instances.\nmaximum: 3       # Maximum number of instances.\nscale_by: 1      # Increment/decrement to use on current \n                 # number of running instances.\nscale_cpu \nScales up the number of CPUs per instances. It is applied only on the first service in the cluster (old or \"A\" version).  \n Example - scale_cpu\n---\ntype: scale_cpu\ntarget: monarch  \nminimum: 1\nmaximum: 3 \nscale_by: 0.5\nscale_memory   \nScales up the memory per instance. It is applied only on the first service in the cluster (old or \"A\" version).  \n Example - scale_memory  \n---\ntype: scale_memory\ntarget: monarch  \nminimum: 512     # In MB.\nmaximum: 4096    # In MB.\nscale_by: 512    # In MB.\n\nComposing escalation handlers\n\nVamp has a set of predefined escalation handler types that deal with escalations. You can compose these handlers in the DSL to get the desired outcome of an escalation event. The escalation handlers toall and toone are supported:\n\n to_all  \nThis is a \"group\" escalation handler that contains a list of escalations. On each escalation event it will propagate the escalation to all escalation handlers from its list. No order or hierarchy.    \n\nExample - to_all  \n---\nto_all:\n  escalations:\n     Scale up/down.\n    scale_instances\n    # And notify for each event.\n    notify\nto_one  \nThis is a group escalation handler that contains a list of escalations. On each escalation event it will propagate the escalation to each escalation handler (from its list) until one can handle it. On an Escalate event, it will start at the head of the list. During a DeEscalate event is will start at the rear.  \nWhen is this useful? Well, Vamp could try to scale up the service and if that;s not possible anymore (e.g. reached the upper limit of allowed scale) then an email can be sent.  \n\n Example - simple use of to_one\n---\nto_one:\n  escalations:\n    # First try to escalate.\n    scale_instances\n    # If it's not possible, proceed with notifying.\n    notify\nExample - complex use of to_one  \n---\nname: monarch\n\ngateways:\n  80: monarch1/port\n\nenvironment_variables:\n  monarch2.password: secret\n\nclusters:\n\n  monarch1:\n    breed:\n      name: monarch1\n      deployable: vamp/monarch1\n      ports:\n        port: 80/http\n      environment_variables:\n        STORAGE_HOST: $storage.host\n        DB_PORT: $storage.ports.port\n        STORAGEPASS: $storage.environmentvariables.password\n      \n      dependencies:\n        storage: monarch2\n\n    scale:\n      cpu: 1\n      memory: 1024MB\n      instances: 1\n    \n    sla:\n      type: responsetimesliding_window\n      threshold:\n        upper: 1000\n        lower: 100\n      window:\n        interval: 600\n        cooldown: 600\n      escalations:\n        to_one:\n            escalations:\n              type: scale_instances\n                 First try to scale up storage service.\n                target: monarch2\n                minimum: 1\n                maximum: 3\n                scale_by: 1\n              type: scale_instances\n                # If we cannot scale up storage anymore, scale up this.\n                target: monarch1\n                minimum: 1\n                maximum: 3\n                scale_by: 1\n              \n  monarch2:\n    breed:\n      name: monarch2\n      deployable: vamp/monarch2\n      ports:\n        port: 3306\n      environment_variables:\n        STORAGE_PASS: password\n    scale:\n      cpu: 1\n      memory: 1024MB\n      instances: 1\n\n{{ note title=\"What next?\" }}\nRead about Referencing artifacts in Vamp\nCheck the API documentation\nTry Vamp\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/events",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Events\n---\n\nVamp collects events on all running services. Interaction with the API also creates events, like updating blueprints or deleting a deployment. Furthermore, Vamp allows third party applications to create events and trigger Vamp actions.\n\nAll events are stored and retrieved using the Event API that is part of Vamp.\n\nExample - JSON \"deployment create\" event\n\n{\n  \"tags\": [\n    \"deployments\",\n    \"deployments:6ce79a33-5dca-4eb2-b072-d5f65e4eef8a\",\n    \"archiving\",\n    \"archiving:create\"\n  ],\n  \"value\": \"name: sava\",\n  \"timestamp\": \"2015-04-21T09:15:42Z\"\n}\n\n Basic event rules\n\nAll events stick to some basic rules:\n\nAll data in Vamp are events. \nValues can be any JSON object or it can be empty.\nTimestamps are in ISO8601/RFC3339.\nTimestamps are optional. If not provided, Vamp will insert the current time.\nTimestamps are inclusive for querying.\nEvents can be tagged with metadata. A simple tag is just single string.\nQuerying data by tag assumes \"AND\" behaviour when multiple tags are supplied, i.e. [\"one\", \"two\"] would only fetch records that are tagged with both.\nSupported event aggregations are: average, min, max and count.\n\nHow tags are organised\n\nIn all of Vamp's components we follow a REST (resource oriented) schema, for instance:\n/deployments/{deployment_name} \n/deployments/{deploymentname}/clusters/{clustername}/services/{service_name}\nTagging is done using a very similar schema: \"{resourcegroup}\", \"{resourcegroup}:{name}\". Some examples:\n\n\"deployments\", \"deployments:{deployment_name}\"\n\"deployments\", \"deployments:{deploymentname}\", \"clusters\", \"clusters:{clustername}\", \"services\", \"services\",services:{service_name} \"\n\nThis schema allows querying per group and per specific name. Getting all events related to all deployments is done by using tag \"deployments\". Getting events for specific deployment \"deployments:{deployment_name}\".\n\n Query events using tags\n\nUsing the tags schema and timestamps, you can do some powerful queries. Either use an exact timestamp or use special range query operators, described on the elastic.co site (elastic.co - Range query).\n\n{{ note title=\"Note!\" }}\nthe default page size for a set of returned events is 30.\n{{ /note }}\n\nExample queries\n\nGet all events\nResponse time for a cluster\nCurrent sessions for a service\nll known events for a service\n \n Example 1\nGet all events\n\nThe below query gets ALL metrics events up till now, taking into regard the pagination.\n\n{{ note title=\"Note!\" }}\nGET request with body - similar to approach used by Elasticsearch.\n{{ /note }}\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"metrics\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\nExample 2 \nResponse time for a cluster\n\nThe below query gets the most recent response time events for the \"frontend\" cluster in the \"d9b42796-d8f6-431b-9230-9d316defaf6d\" deployment.\n\nNotice the \"gateways:UUID\", \"metrics:responseTime\" and \"gateways\" tags. This means \"give me the response time of this specific gateway at the gateway level\". The response will echo back the events in the time range with the original set of tags associated with the events. \n\nGET /api/v1/events\n\n{\n  \"tags\": [\"routes:d9b42796-d8f6-431b-9230-9d316defaf6dfrontend8080\",\"metrics:rtime\",\"route\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\n[\n    {\n        \"tags\": [\n            \"gateways\",\n            \"gateways:d9b42796-d8f6-431b-9230-9d316defaf6dfrontend8080\",\n            \"metrics:rate\",\n            \"metrics\",\n            \"gateway\"\n        ],\n        \"value\": 0,\n        \"timestamp\": \"2015-06-08T10:28:35.001Z\",\n        \"type\": \"gateway-metric\"\n    },\n    {\n        \"tags\": [\n            \"gateways\",\n            \"gateways:d9b42796-d8f6-431b-9230-9d316defaf6dfrontend8080\",\n            \"metrics:rate\",\n            \"metrics\",\n            \"gateway\"\n        ],\n        \"value\": 0,\n        \"timestamp\": \"2015-06-08T10:28:32.001Z\",\n        \"type\": \"gateway-metric\"\n    }\n]    \n\n Example 3\nCurrent sessions for a service\n\nAnother example is getting the current sessions for a specific service, in this case the monarch_front:0.2 service that is part of the 214615ec-d5e4-473e-a98e-8aa4998b16f4 deployment and lives in the frontend cluster.\n\nNotice we made the search more specific by specifying the \"services\" and then \"service:SERVICE NAME\" tag.\nAlso, we are using relative timestamps: anything later or equal (lte) than \"now\".\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"routes:214615ec-d5e4-473e-a98e-8aa4998b16f4frontend8080\",\"metrics:scur\",\"services:monarch_front:0.2\",\"service\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\nExample 4\nAll known events for a service\n\nThis below query gives you all the events we have for a specific service, in this case the same service as in example 2. In this way you can get a quick \"health snapshot\" of service, server, cluster or deployment.\n\nNotice we made the search less specific by just providing the \"metrics\" tag and not telling the API which specific one we want.\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"routes:214615ec-d5e4-473e-a98e-8aa4998b16f4frontend8080\",\"metrics\",\"services:monarch_front:0.2\",\"service\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\n Server-sent events (SSE)\n\nEvents can be streamed back directly from Vamp.\n\nGET /api/v1/events/stream\n\nIn order to narrow down (filter) events, list of tags could be provided in the request body.\n\n{\n  \"tags\": [\"routes:214615ec-d5e4-473e-a98e-8aa4998b16f4frontend8080\",\"metrics\"]\n}\n\nGET method can be also used with tag parameter (may be more convenient):\n\nGET /api/v1/events/stream?tag=archiving&tag=breeds\n\nArchiving\n\nAll changes in artifacts (creation, update or deletion) triggered by REST API calls are archived. We store the type of event and the original representation of the artifact. It's a bit like a Git log. \n\nHere is an example event:\n\n{\n  \"tags\": [\n    \"deployments\",\n    \"deployments:6ce79a33-5dca-4eb2-b072-d5f65e4eef8a\",\n    \"archiving\",\n    \"archiving:delete\"\n  ],\n  \"value\": \"\",\n  \"timestamp\": \"2015-04-21T09:17:31Z\",\n  \"type\": \"\"\n}\n\nSearching through the archive is 100% the same as searching for events. The same tagging scheme applies.\nThe following query gives back the last set of delete actions executed in the Vamp API, regardless of the artifact type.\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"archiving\",\"archiving:delete\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\n{{ note title=\"What next?\" }}\nRead about Vamp SLA (Service Level Agreement)\nCheck the API documentation\nTry Vamp\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/gateways",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Gateways\n---\n\nGateways are dynmic runtime entities in the Vamp eco-system. They represent load balancer rules to deployment, cluster and service instances.\n\nThere are two types of gateways:\n\nInternal gateways: created automatically for each deployment cluster, updated using the gateway/deployment API\nExternal gateways: explicitly declared either in a deployment blueprint or using the gateway API\n\nThis is an example of automatically created gateway for deployment vamp, cluster sava and port port.\nCluster contains 2 services sava:1.0.0 and sava:1.1.0 with 2 running instances each. \n---\nname: vamp/sava/port           # name\nport: 40000/http               # port, either http or tcp, assigned by Vamp\nactive: true                   # is it running - not in case of non (yet) existing routes\nsticky: none\nroutes:                        # routes\n  vamp/sava/sava:1.0.0/port:\n    weight: 50%\n    instances:\n    name: vamp_6fd83b1fd01f7dd9eb7f.cda3c376-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31463\n    name: vamp_6fd83b1fd01f7dd9eb7f.cda2d915-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31292\n  vamp/sava/sava:1.1.0/port:\n    weight: 50%\n    instances:\n    name: vamp_2e2fc6ab8a1cdbe79dc3.caa3c9e4-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31634\n    name: vamp_2e2fc6ab8a1cdbe79dc3.caa37bc3-ae26-11e5-91fb-0242f7e42bf3\n      host: default\n      port: 31826\n\nGateway API allows programmable routing and having external gateways gives:\n\nentry point to clusters, e.g. specified (but not necessarily) in deployment blueprints\ncanary release and A/B testing on cross deployment level.\n\nExample of A/B testing of 2 deployments\n\nDeployment 1: PUT /api/v1/deployments/sava:1.0\n\n---\nname: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nDeployment 2: PUT /api/v1/deployments/sava:1.1\n\n---\nname: sava:1.1\ngateways:\n  9060/http: sava/port\nclusters:\n  sava:\n    services:\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nGateway (90% / 10%): POST /api/v1/gateways\n\n---\nname: sava\nport: 9070/http\nroutes:\n  sava:1.0/sava/port:\n    weight: 90%          # condition can be used as well\n  sava:1.1/sava/port:\n    weight: 10%\n\nThis is similar to putting both sava:1.0.0 and sava:1.1.0 in the same cluster but that is just because this is a basic example.\nIt is easy to imagine having an older legacy application and the new one and doing full canary release (or A/B testing) in seamless way by using gateways like this.\n\nGateway Usage\n\nGateways define a set of rules for routing traffic between different services within the same cluster.\nVamp allows you to determine this in following ways:\n\nby setting a weight in the percentage of traffic.\nby setting a condition condition to target specific traffic.\nby setting a condition strength in the percentage of traffic matching the condition.\n\nYou can define gateways inline in a blueprint or store them separately under a unique name and just use that name to reference them from a blueprint.\n\nLet's have a look at a simple inline gateway. This would be used directly inside a blueprint.\n\n---\ncondition_strength: 10%   Amount of traffic for this service in percents.\ncondition: User-Agent = IOS\n\n Notice: we added a condition named reallycoolcondition here. This condition is actually a reference to a separately stored condition definition we stored under a unique name on the /conditions endpoint.\n\nDefining weights, condition strength and basic weight rules\n\nFor each route, weight can be set regardless of any condition.\nThe basic rule is the following:\n\nfind the first condition that matches request\nif route exists, send the request to it depending on condition strength\nif based on condition strength route should not follow that route then send request to one from all routes based on their weights.\n\nWhen defining weights, please make sure the total weight of routes always adds up to 100%.\nThis means that when doing a straight three-way split you give one service 34% as 33%+33%+33%=99%. Vamp has to account for all traffic and 1% can be a lot in high volume environments.\n\n Example - Route all Firefox and only Firefox users to route service_B:\n\nservice_A:\n  weight: 100%\nservice_B:\n  weight: 0%\n  condition_strength: 100%\n  condition: user-agent == Firefox\n\nExample - Route half of Firefox users to serviceB, other half to serviceA (80%) or service_B (20%):\nNon Firefox requests will be just sent to serviceA (80%) or serviceB (20%).\nservice_A:\n  weight: 80%\nservice_B:\n  weight: 20%\n  condition_strength: 50%\n  condition: user-agent == Firefox\n\n{{ note title=\"What next?\" }}\nRead about Vamp conditions\nCheck the API documentation\nTry Vamp\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/references",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Referencing artifacts\n---\n\nWith any artifact, Vamp allows you to either use an inline notation or reference the artifact by name. For references, you use the reference keyword or its shorter version ref. Think of it like either using actual values or pointers to a value. This has a big impact on how complex or simple you can make any blueprint, breed or deployment. It also impacts how much knowledge you need to have of all the different artifacts that are used in a typical deployment or blueprint.\n\nVamp assumes that referenced artifcats (the breed called my_breed in the example below) is available to load from its datastore at deploy time. This goes for all basic artifacts in Vamp: SLA's, gateways, conditions, escalations, etc.\n\nExample - reference notation\n\ninline notation\n\n---\nname: my_blueprint\n  clusters:\n    my_cluster:\n      services:\n        breed:\n          name: my_breed\n          deployable: registry.example.com/app:1.0\n        scale:\n          cpu: 2\n          memory: 1024MB\n          instances: 4\nreference notation\n\n---\nname: my_blueprint\n  clusters:\n    my_cluster:\n      services:\n        breed:\n          reference: my_breed\n        scale:\n          reference: medium  \n\n Working with references\n\nWhen you begin to work with Vamp, you will probably start with inline artifacts. You have everything in one place and can directly see what properties each artifact has. Later, you can start specialising and basically build a library of often used architectural components. \n\nExample use of references\n\nCreate a library of containers\nFix scales per environment\nReuse a complex condition\n\n Example 1\nCreate a library of containers\n\nUse case: You have a Redis container you have tweaked and setup exactly the way you want it. You want to use that exact container in all your environments (dev, test, prod etc.). \n\nImplementation: Put all that info inside a breed and use either the Vamp UI or API to save it (below). Now you can just use the ref: redis:1.0 notation anywhere in a blueprint.\n\nPOST /api/v1/breeds\n\n---\nname: redis:1.0\ndeployable: redis\nports: 6379/tcp\n\nExample 2\nFix scales per environment\n\nUse case: You want to have a predetermined set of scales you can use per deployment per environment. For instance, a \"mediumproduction\" should be something else than a \"mediumtest\".\n\nImplementation: Put all that info inside a scale and use either the Vamp API to save it (below). Now you can use the ref: mediumtest or ref: mediumprod notation anywhere a scale type is required.\n\nPOST /api/v1/scales\n\n---\nname: medium_prod\ncpu: 2\nmemory: 4096MB\ninstances: 3\n\n---\nname: medium_test\ncpu: 0.5\nmemory: 1024MB\ninstances: 1\n\n Example 3\nReuse a complex condition\n\nUse case: You have created a complex condition to target a specific part of your traffic. In this case users with a cookie that have a specific session variable set in that cookie. You want to use that condition now and then to do some testing. \n\nImplementation: Put all that info inside a condition and use either the Vamp API to save it (below). Now you can use the  ref: conditionemptyshopping_cart anywhere that condition is required.\n\n---\nname: conditionemptyshopping_cart\ncondition: Cookie SHOPSESSION Contains shoppingbasketitems=0 \n\n{{ note title=\"What next?\" }}\nRead about Vamp workflows\nCheck the API documentation\nTry Vamp\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/sla",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: SLA (Service Level Agreement)\n\n---\n\nSLA stands for \"Service Level Agreement\". Vamp uses it to define a pre-described set of boundaries to a service and the actions that should take place once the service crosses those boundaries. In essence, an SLA and its associated escalation is a workflow that is checked and controlled by Vamp based on the runtime behaviour of a service. SLAs and escalations are defined with the VAMP DSL.\n\nThe SLA event system\n\nYou can define an SLA for each cluster in a blueprint. A common example would be to check if the average response time of the cluster (averaged across all services) is higher or lower than a certain threshold. Under the hood, an SLA workflow creates two distinct events. These are are sent from Vamp and stored to Elasticsearch.\n\nEscalate for a specific deployment and cluster  \ne.g. if the response time is higher than the upper threshold.\nDeEscalate for a specific deployment and cluster  \ne.g. if the response time is lower than the lower threshold.\n\nSLA monitoring is a continuous background process with a configurable interval time. On each run an SLA workflow is executed for each deployment & cluster that has an SLA defined. Within the same SLA definition it's possible to define a list of escalations. Escalations are triggered by escalation events (Escalate/DeEscalate).\n\nThis means escalation events can be generated by the third party systems by sending them to Elasticsearch. This would allow scaling up or down to be triggered by basically any system that can POST a piece of JSON.\n\nSLA's are in essence pieces of code inside Vamp that stick to this event model and can use, if they want, the metrics and event data streaming out of Elasticsearch to make decisions on how things are and should be running.\n\n SLA types\n\nVamp currently ships with the following SLA types:\n\nresponsetimesliding_window\n\nResponse time with sliding window \n\nThe responsetimesliding_window SLA triggers events based on response times. \n\n Example - SLA defined inline in a blueprint.\n\nNotice the SLA is defined at the cluster level and acts on the first service in the cluster.\n\nNotice how the SLA is defined separately from the escalations. This is key to how Vamp approaches SLA's and how modular and extendable the system is.\n\n---\nname: monarch\n\ngateways:\n  80: monarch/port\n\nclusters:\n\n  monarch:\n    breed:\n      name: monarch\n      deployable: vamp/monarch\n      ports:\n        port: 80\n\n    scale:\n      cpu: 1\n      memory: 1024MB\n      instances: 2\n\n    sla:\n      # Type of SLA.\n      type: responsetimesliding_window\n      threshold:\n        upper: 1000   # Upper threshold in milliseconds.\n        lower: 100    # Lower threshold in milliseconds.\n      window:\n        interval: 600 # Time period in seconds used for\n                      # average response time aggregation.\n        cooldown: 600 # Time period in seconds. During this \n                      # period no new escalation events will \n                      # be generated. New event may be expected \n                      # not before cooldown + interval time has \n                      # been reached after the last event. \n     \n      # List of escalations.\n      escalations:\n        type: scale_instances\n          minimum: 1\n          maximum: 3\n          scale_by: 1\n\n{{ note title=\"What next?\" }}\nRead about Vamp escalations\nCheck the API documentation\nTry Vamp\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/sticky-sessions",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Sticky Sessions\n\n---\n\nVamp supports route and instance level sticky sessions.\n\nRoute Level\n\nA common use case is when the end users have to have the same experience in A/B testing setup thus they should get the same service always (either A or B).\n\n---\nname: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    gateways:\n      sticky: route                            setting the route level\n      routes:\n        sava:1.0.0:\n          weight: 50%\n        sava:1.1.0:\n          weight: 50%\n          \n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true           # to show debug information such as instance id\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nInstance Level \n\nA common use case is when the end users need to be served by the same instance (e.g. stateful application).\n\n---\nname: sava:1.0\ngateways:\n  9050/http: sava/port\nclusters:\n  sava:\n    gateways:\n      sticky: instance                          setting the instance level\n      routes:\n        sava:1.0.0:\n          weight: 50%\n        sava:1.1.0:\n          weight: 50%\n          \n    services:\n      breed:\n          name: sava:1.0.0\n          deployable: magneticio/sava:1.0.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true           # to show debug information such as instance id\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n      breed:\n          name: sava:1.1.0\n          deployable: magneticio/sava:1.1.0\n          ports:\n            port: 8080/http\n          environment_variables:\n            debug[SAVA_DEBUG]: true\n            \n        scale:\n          cpu: 0.2\n          memory: 256MB\n          instances: 2\n\nOther Notes\n\nResetting the sticky value can be done by: sticky: none or sticky: ~ (setting it to null).\n\nSticky sessions can be also used for gateways:\n\n---\nname: sava:1.0\ngateways:\n  9050/http:\n    sticky: service\n    routes:            let's say we have 2 clusters: sava1 (90%) and sava2 (10%)\n      sava1/port:   \n        weight: 90%\n      sava2/port:\n        weight: 10%\nclusters:\n  sava1: \n    ...\n  sava2: \n    ...\n\n{{ note title=\"What next?\" }}\nRead about using Vamp with virtual hosts\nCheck the API documentation\nTry Vamp\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/virtual-hosts",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Virtual Hosts\n---\n\nVamp can be configured to support virtual host via HAProxy:\n\nvamp.operation.gateway {\n    virtual-hosts = true\n    virtual-hosts-domain = \"vamp\"\n}\n\nExample - Virtual hosts\n \n PUT ${VAMP_URL}/api/v1/deployments/runner with body:\n\n---\nname: runner\n\ngateways:\n  9070: runner1/port\n  9080: runner2/port\n\nclusters:\n  runner1:\n    services:\n      breed:\n        name: http:1.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 1.0.0\n  runner2:\n    services:\n      breed:\n        name: http:2.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 2.0.0\n\nNow you can request:\n\n$ curl --resolve 9070.runner.vamp:80:${VAMPGATEWAYAGENT_IP} http://9070.runner.vamp\n{\"id\":\"1.0.0\",\"runtime\":\"CF2136D9CA81282E\",\"port\":8081,\"path\":\"\"}\n\n$ curl --resolve 9080.runner.vamp:80:${VAMPGATEWAYAGENT_IP} http://9080.runner.vamp\n{\"id\":\"2.0.0\",\"runtime\":\"1E188B006FF44AA6\",\"port\":8081,\"path\":\"\"}\n{{ note title=\"Note!\" }}\nIf you are running Vamp in one of the quick setups, ${VAMPGATEWAYAGENTIP} should have value of ${DOCKERHOST_IP} - See the hello world quick setup instructions.\n{{ /note }}\n\nVamp creates a virtual host for each gateway - name of the gateway (/ replaced with .) appended to value from vamp.gateway-driver.virtual-hosts-domain.\nIn case of above example:\n\n9050.runner.vamp\n9060.runner.vamp\nport.runner1.runner.vamp\nport.runner2.runner.vamp\n\nUsing Gateway API it is possible to get virtual hosts for each gateway, e.g.\nGET ${VAMP_URL}/api/v1/gateways\n\n Custom virtual hosts\n\nAs you could see each gateways has virtual_hosts field.\nUsing that field it is also possible to set list of custom virtual hosts.\nLet's see that in the following example:\n\n---\nname: runner\n\ngateways:\n  9080:\n    virtual_hosts: [\n      \"run.vamp.run\"\n    ]\n    routes:\n      runner1/port:\n        weight: 50%\n      runner2/port:\n        weight: 50%\n\nclusters:\n  runner1:\n    services:\n      breed:\n        name: http:1.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 1.0.0\n  runner2:\n    services:\n      breed:\n        name: http:2.0.0\n        deployable: magneticio/sava:runner_1.0\n        ports:\n          port: 8081/http\n        environment_variables:\n          SAVARUNNERID: 2.0.0\n\nIf you deploy this blueprint as runner and check gateway 9080/runner:\n\nGET ${VAMP_URL}/api/v1/gateways/runner/9080\n{\n  \"name\": \"runner/9080\",\n  \"virtual_hosts\": [\n    \"9080.runner.vamp\",\n    \"run.vamp.run\"\n  ]\n  ...\n\n$ curl --resolve run.vamp.run:80:${VAMPGATEWAYAGENT_IP} http://run.vamp.run\n{\"id\":\"1.0.0\",\"runtime\":\"C0013E858F213AE0\",\"port\":8081,\"path\":\"\"}\n\n9080.runner.vamp is added if configuration parameter vamp.operation.gateway.virtual-hosts is set, otherwise just custom virtual hosts if any.\n\n{{ note title=\"What next?\" }}\nCheck the API documentation\nTry Vamp\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/documentation/using vamp/workflows",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Workflows \n---\n\nA \"workflow\" is an automated change of the running system and its deployments and gateways. \nChanging the number of running instances based on metrics (e.g. SLA) is an example of a workflow. \nA workflow can be seen as a recipe or solution, however it has a more generic meaning not just related to \"problematic\" situations.\n\nAnother example is a workflow that will decide automatically if a new version, when doing a canary release, should be accepted or not. \nFor instance, push the route up to 50% of traffic to the new version, compare metrics over some time (e.g. frequency of 5xx errors, response time), change to 100% and remove the old version. \nThis workflow could define the rate of the transitions (e.g. 5% - 10% - 25%, ...) as well.\n\nRationale\n\nWorkflows allow closing the feedback loop: deploy, measure, react.\nVamp workflows are based on running separate services (breeds) and in its simplest form scripting can be used - e.g. application/javascript breeds. \nScripting allows experimentation with different features and if the feature is common and generic enough, it could be supported later in Vamp DSL.\nSince workflows are running breeds in similar way as in deployments (blueprints), all other breed features are supported - ports, environment variables etc.\n\n Workflow API\n\nEach workflow is represented as an artifact and they follow basic CRUD operation patterns as any other artifact:\n  /api/v1/workflows\n\nEach workflow has to have:\n\n name\n breed - either reference or inline definition, similar to blueprints\n schedule \n scale - optional\n environment_variables (or env)- overrides breed environment variables\n arguments- Docker arguments, overrides default configuration arguments and breed arguments\n\nExample:\n\n---\nname: metrics\nbreed: metrics   # breed reference, inline definition can be also used\nschedule: daemon\nscale:           # inline scale, reference definition can be also used, e.g. scale: small\n  cpu: 1\n  memory: 128MB\n  instances: 2\nenvironment_variables:\n  interval: 5s\n\nSchedule\n\nFollowing schedule types are supported:\n\ndaemon\nevent with tags (set)\ntime - period, start (optional, by default starts now) and repeat (optional, by default runs forever) \n\nExamples:\n\n---\nschedule: daemon\n  \n time schedule\n\nschedule:\n  time:\n    period: P1Y2M3DT4H5M6S\n    start: now # or e.g. start: 2016-12-03T08:15:30Z\n    repeat: 10\n\nevent schedule\n\nschedule:\n  event:  event with following tags will trigger the workflow\n  deployments:sava\n  cluster:runner\n  \nor shorten notation in case of single event (still array can be used as above)\n\nschedule:\n  event: archive:bluprints\n\n      \nTime schedule period is in ISO8601 repeating interval notation.\n\nExample:\n\n---\nname    : metrics\nschedule: daemon\nbreed   :\n  name: metrics\n  deployable:\n    type: application/javascript\n    definition: |\n      'use strict';\n      \n      var _ = require('lodash');\n      var vamp = require('vamp-node-client');\n      \n      var api = new vamp.Api();\n      var metrics = new vamp.Metrics(api);\n      \n      var period = 5;  // seconds\n      var window = 30; // seconds\n      \n      var process = function() {\n        api.gateways(function (gateways) {\n          _.forEach(gateways, function(gateway) {\n            metrics.average({ \n              ft: gateway.lookup_name \n            }, 'Tt', window, function(total, rate, responseTime) {\n              api.event(['gateways:' + gateway.name, 'metrics:rate'], rate);\n              api.event(['gateways:' + gateway.name, 'metrics:responseTime'], responseTime);\n            });\n          });\n        });ß\n      };\n      \n      setInterval(process, period * 1000);\n\n{{ note title=\"Note!\" }}\nProbably it would be better to keep breed as a reference and create breed as shown below:\n{{ /note }}\n\nPUT Content-Type: application/javascript /api/v1/breeds/metrics\n'use strict';\n\nvar _ = require('lodash');\nvar vamp = require('vamp-node-client');\n\nvar api = new vamp.Api();\nvar metrics = new vamp.Metrics(api);\n\nvar period = 5;  // seconds\nvar window = 30; // seconds\n\nvar process = function() {\n  api.gateways(function (gateways) {\n      _.forEach(gateways, function(gateway) {\n          metrics.average({ ft: gateway.lookup_name }, 'Tt', window, function(total, rate, responseTime) {\n              api.event(['gateways:' + gateway.name, 'metrics:rate'], rate);\n              api.event(['gateways:' + gateway.name, 'metrics:responseTime'], responseTime);\n          });\n      });\n  });\n};\n\nsetInterval(process, period * 1000);\n\nJavaScript breeds will be executed by Vamp Workflow Agent (github.com/magneticio - Vamp workflow agent).  \n\nFor additional JavaScript API check out Vamp Node Client (github.com/magneticio - Vamp node client) project.\n\n{{ note title=\"What next?\" }}\nRead about Sticky sessions\nCheck the API documentation\nTry Vamp\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/draft/api/0.8.5/example1",
        "content": "---\ndate: 2016-03-09T19:56:50+01:00\ntitle: Example 1\n\nmenu:\n  main:\n    parent: API\n    identifier: 0.8.5/Example 1\n    weight: 10\n---\n\nSubtitle\n\nTaxidermy hella fingerstache, ugh yuccie tote bag shoreditch. Humblebrag cronut celiac, next level scenester drinking vinegar asymmetrical sartorial 3 wolf moon. Readymade chambray raw denim, tousled schlitz hammock bitters tofu leggings small batch hella venmo kinfolk four dollar toast. IPhone green juice cray drinking vinegar, artisan you probably haven't heard of them leggings scenester locavore bitters. Chartreuse 90's health goth tacos heirloom. Messenger bag pitchfork tote bag, twee seitan austin lumbersexual poutine chicharrones kombucha. Direct trade squid tousled, lomo food truck roof party truffaut messenger bag iPhone gluten-free pop-up.\n\nTumblr portland tattooed deep v everyday carry. Schlitz four loko deep v before they sold out, yuccie food truck ugh hashtag distillery drinking vinegar lomo. Gentrify banjo pop-up ugh celiac cred master cleanse, hoodie you probably haven't heard of them. Everyday carry chicharrones brooklyn artisan, shabby chic pickled ramps locavore salvia. Normcore chambray occupy artisan. Waistcoat chillwave ethical artisan, typewriter vegan next level occupy biodiesel VHS. Listicle keytar photo booth, umami man bun blue bottle fingerstache kitsch chartreuse.\n",
        "tags": []
    },
    {
        "uri": "/draft/api/0.8.5/index",
        "content": "---\ndate: 2016-08-01T12:53:48+02:00\ntitle: API\n\nmenu:\n  main:\n    parent: API\n    identifier: 0.8.5/index\n    weight: 10\n---\n\nExample 1",
        "tags": []
    },
    {
        "uri": "/draft/api/0.9.0/example2",
        "content": "---\ndate: 2016-08-01T11:16:39+02:00\ntitle: Example 2\n\nmenu:\n  main:\n    parent: API\n    identifier: 0.9.0/Example 2\n    weight: 20\n---\n\nChurch-key gentrify tousled microdosing, bitters yr food truck post-ironic sustainable kitsch 90's twee kombucha. Typewriter 3 wolf moon artisan try-hard. Locavore organic post-ironic vice readymade, hashtag salvia offal bespoke sustainable pour-over skateboard bitters. Pop-up letterpress trust fund quinoa, jean shorts authentic artisan. Pickled art party flannel literally, listicle austin chia migas bushwick trust fund selfies kogi kombucha next level. Ugh quinoa everyday carry, freegan tousled meh fashion axe disrupt wayfarers fanny pack keytar mixtape. Kogi beard food truck fashion axe.\n\nViral cornhole 3 wolf moon plaid, ennui swag pork belly PBR&B locavore letterpress you probably haven't heard of them. Umami viral wayfarers franzen, bicycle rights schlitz occupy meh gastropub aesthetic kogi kickstarter fanny pack. Four dollar toast jean shorts cold-pressed PBR&B. Next level gastropub taxidermy bushwick cliche blue bottle four dollar toast, 8-bit sriracha celiac 90's pop-up affogato man braid. Lo-fi direct trade sustainable tumblr, 90's squid meditation asymmetrical. Marfa artisan humblebrag four loko pork belly. Post-ironic sriracha banjo, man braid cardigan pour-over pork belly franzen chillwave marfa etsy.",
        "tags": []
    },
    {
        "uri": "/draft/api/0.9.0/index",
        "content": "---\ndate: 2016-08-01T12:53:48+02:00\ntitle: API\n\nmenu:\n  main:\n    parent: API\n    identifier: 0.9.0/index\n    weight: 10\n---\n\nExample 2",
        "tags": []
    },
    {
        "uri": "/draft/api/master/example1",
        "content": "---\ndate: 2016-03-09T19:56:50+01:00\ntitle: Example 1\n\nmenu:\n  main:\n    parent: API\n    identifier: master/Example 1\n    weight: 10\n---\n\nSubtitle\n\nTaxidermy hella fingerstache, ugh yuccie tote bag shoreditch. Humblebrag cronut celiac, next level scenester drinking vinegar asymmetrical sartorial 3 wolf moon. Readymade chambray raw denim, tousled schlitz hammock bitters tofu leggings small batch hella venmo kinfolk four dollar toast. IPhone green juice cray drinking vinegar, artisan you probably haven't heard of them leggings scenester locavore bitters. Chartreuse 90's health goth tacos heirloom. Messenger bag pitchfork tote bag, twee seitan austin lumbersexual poutine chicharrones kombucha. Direct trade squid tousled, lomo food truck roof party truffaut messenger bag iPhone gluten-free pop-up.\n\nTumblr portland tattooed deep v everyday carry. Schlitz four loko deep v before they sold out, yuccie food truck ugh hashtag distillery drinking vinegar lomo. Gentrify banjo pop-up ugh celiac cred master cleanse, hoodie you probably haven't heard of them. Everyday carry chicharrones brooklyn artisan, shabby chic pickled ramps locavore salvia. Normcore chambray occupy artisan. Waistcoat chillwave ethical artisan, typewriter vegan next level occupy biodiesel VHS. Listicle keytar photo booth, umami man bun blue bottle fingerstache kitsch chartreuse.\n",
        "tags": []
    },
    {
        "uri": "/draft/api/master/example2",
        "content": "---\ndate: 2016-08-01T11:16:39+02:00\ntitle: Example 2\n\nmenu:\n  main:\n    parent: API\n    identifier: master/Example 2\n    weight: 20\n---\n\nChurch-key gentrify tousled microdosing, bitters yr food truck post-ironic sustainable kitsch 90's twee kombucha. Typewriter 3 wolf moon artisan try-hard. Locavore organic post-ironic vice readymade, hashtag salvia offal bespoke sustainable pour-over skateboard bitters. Pop-up letterpress trust fund quinoa, jean shorts authentic artisan. Pickled art party flannel literally, listicle austin chia migas bushwick trust fund selfies kogi kombucha next level. Ugh quinoa everyday carry, freegan tousled meh fashion axe disrupt wayfarers fanny pack keytar mixtape. Kogi beard food truck fashion axe.\n\nViral cornhole 3 wolf moon plaid, ennui swag pork belly PBR&B locavore letterpress you probably haven't heard of them. Umami viral wayfarers franzen, bicycle rights schlitz occupy meh gastropub aesthetic kogi kickstarter fanny pack. Four dollar toast jean shorts cold-pressed PBR&B. Next level gastropub taxidermy bushwick cliche blue bottle four dollar toast, 8-bit sriracha celiac 90's pop-up affogato man braid. Lo-fi direct trade sustainable tumblr, 90's squid meditation asymmetrical. Marfa artisan humblebrag four loko pork belly. Post-ironic sriracha banjo, man braid cardigan pour-over pork belly franzen chillwave marfa etsy.",
        "tags": []
    },
    {
        "uri": "/draft/api/master/index",
        "content": "---\ndate: 2016-08-01T12:53:48+02:00\ntitle: API\n\nmenu:\n  main:\n    parent: API\n    identifier: master/index\n    weight: 10\n---\n\nExample 1\nExample 2",
        "tags": []
    },
    {
        "uri": "/draft/blueprint",
        "content": "---\ndate: 2016-03-09T19:56:50+01:00\ntitle: blueprint (reference)\n---\nback to resources page  \n\nreference - work in progress\n\nBlueprints\n\nBlueprints allow you to add the following properties:\n\ngateways: a stable port where the service can be reached.\nclusters & services: a cluster is a grouping of services with one purpose, i.e. two versions (a/b) of one service.\nenvironment variables: a list of variables (interpolated or not) to be made available at runtime.\ndialects: a dialect is a set of native commands for the underlying container platform, i.e. Docker or Mesosphere Marathon.\nscale: the CPU and memory and the amount of instance allocate to a service.\nrouting: how much and which traffic the service should receive.\nfilters: how traffic should be directed based on HTTP and/or TCP properties.\nsla & escalations: SLA definition that controls autoscaling.\n\n Breeds\n\nBreeds allow you to set the following properties:\n\ndeployable: the name of actual container or command that should be run. Vamp supports Docker containers or can support any other artefacts supported by your container manager. \nports: a map of ports your container exposes.\nenvironment variables: a list of variables (interpolated or not) to be made available at runtime.\ndependencies: a list of other breeds this breed depends on.\n\nDeployable\n\nLet’s start with an example breed that deploys a Docker container:\n\n---\nname: my_breed:0.1\ndeployable: company/myfrontendservice:0.1\n\nports:\n  web: 8080/http   \n  \nThis breed, with a unique name, describes a deployable and the port it works on. By default, the deployable is a Docker container. We could make this also explicit by adding docker://. The following statements are equivalent.\n\ndeployable: company/myfrontendservice:0.1\ndeployable: docker://company/myfrontendservice:0.1\n\nDocker images are pulled by your container manager from any of the repositories configured. This can be private repos but by default are the public Docker hub.\n\nRunning “other” artefacts such as zips or jars heavily depend on the underlying container manager. When Vamp is setup to run with Marathon, command:// (or cmd://) deployable type can be used. In that case cmd parameter will have value of deployable.\n\nFor instance running a custom jar after it has been downloaded (uris parameter):\n\nname: location\nclusters:\n  api:\n    services:\n      breed:\n        name: location\n        deployable: cmd://java -jar location.jar\n      marathon:\n        uris: [\"https://myrepolocation_jar\"]\n\nFor instance it can be specified cmd://java -jar some.jar and using Vamp Marathon dialect uris parameter can be used for some.jar can be downloaded from the remote location.\n\n Ports\n\nThe ports property is an array of named ports together with their protocol. It describes on what ports the deployables is offering services to the outside world. \n\nPorts come in two flavors:\n\n/tcp this is the default type if none is specified. Use it for things like Redis, MySQL etc.\n/http HTTP ports are always recommended when dealing with HTTP-based services. Vamp can record a lot of interesting metrics like response times, errors etc. Of course, using /tcp will work but you miss out on cool data.\n\n{{ note title=\"TIP!\" }}\nUse the /http notation for ports whenever possible!\n{{ /note }}\n\nExample - breed with multiple ports\n\n---\nname: my_breed:0.1\ndeployable: company/myfrontendservice:0.1\n\nports:\n  web: 8080/http\n  admin: 8081/http\n  redis: 9023/tcp   \n    \nNotice we have given the ports sensible names.   \nThis specific deployable has a web port for customer traffic, an admin port for admin access and a redis port for some caching probably. These names will come in handy when we start to compose different breeds in blueprints.\n\nback to resources page\n",
        "tags": []
    },
    {
        "uri": "/draft/downloads",
        "content": "---\ndate: 2016-03-09T19:56:50+01:00\ntitle: downloads\n---\n\n{{ warning title=\"DRAFT\" }}\n{{ /warning }}\n\nlist of downloads available and links\n\n",
        "tags": []
    },
    {
        "uri": "/index.json",
        "content": "---\ndate: 2016-08-02T01:13:07+02:00\ntype: json\nurl: index.json\n---",
        "tags": []
    },
    {
        "uri": "/index",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Canary releasing and autoscaling for microservice systems\ntype: index\n---\ntry Vamp\nlearn more\n\n-----------\n\nPercentage and condition based routing   \nPrevent performance issues and service disruption with gradual rollouts and upgrades.\nIntegrated metrics- and event-driven workflows   \nAutomatically scale and optimise deployments or running systems.\nContainer-scheduler agnostic API   \nAvoid vendor lock-in with YAML based deployments.\n\n----------\n\nVamp works with:  \n\nMesosphere  \nKubernetes  \nDC/OS  \nRancher  \nDocker\nAzure\n\n Cloud packets:  \n\nAWS\nGoogle\n\nCompanies using Vamp:  \n\nBBVA  \nExact  \nMijn domein  \nWehkamp  \nRabobank  \nZiggo\n\n-------------\n\n Vamp feature list\n\ngraphical UI and dashboard\nintegrated javascript-based workflow system\nmetric-driven autoscaling, canary releasing and other optimisation and automation patterns\nautomatic loadbalancing for autoscaled services\nAPI gateway routing features like conditional rewrites\nCLI for integration with common CI/CD pipelines\nopen source (Apache 2.0)\nevent API and server-side events (SSE) stream\nmulti-level metric aggregation\nport-based, virtual host names or external service (consul etc) based service discovery support\nlightweight design to run in high-available mission-critical architectures\nintegrates with ELK stack (Elastic Search, Logstash, Kibana) for custom Kibana dashboards\nVamp Runner provides automated integration and workflows testing  \n\n-------------\n\nStay up to date\nsign up for mailing list\n",
        "tags": []
    },
    {
        "uri": "/instructions/index",
        "content": "---\ndate: 2016-08-01T12:53:48+02:00\ntitle: Instructions for creating content\nweight: 10\n---\n\nAdding content to an existing section (menu)\n\nLet's create our first content file for your documentation. \nOpen a terminal and add the following command for each new file you want to add. \nReplace section-name with a general term that describes your document in detail.\n\nhugo new section-name/filename.md\n\nVisitors of your website will find the final document under www.example.com/section-name/filename/.\n\nTo be properly added to the menu, file should contains menu section, e.g.:\n\nmenu:\n  main:\n    parent: Examples\n    identifier: Example 1\n    weight: 10\n\nCheck out content of Examples.\n\n Creating section (menu)\n\nOpen config.toml file and append new section (menu) definition.\nLet's add Getting started submenu. Add the following after the Examples definition (end of the file):\n\n[[menu.main]]\n\tname   = \"Getting started\"\n\turl    = \"getting-started/\"\n\tweight = 20\n\nFields:\nname - displayed name\nurl - section URL, should be the same as directory name\nweight - order of the submenu\n\nNow let's add the index page:\n\nhugo new getting-started/index.md\n\nAdding a new API version\n\nAPI section (menu) is specific because of custom rendering.\nIn order to ad a new API version docs, go to /content/api and copy (branch) one of the existing versions.\nFor instance let's copy 0.9.0 to 0.9.1 directory.\n\n$ tree content/api\n\ncontent/api\n├── 0.8.5\n│   ├── example1.md\n│   └── index.md\n├── 0.9.0\n│   ├── example2.md\n│   └── index.md\n├── 0.9.1\n│   ├── example2.md\n│   └── index.md\n└── master\n    ├── example1.md\n    ├── example2.md\n    └── index.md\n\nNow change all 0.9.0 occurrences in 0.9.1 files to 0.9.1. \nThis will also prevent Hugo to complain about duplicate page identifiers.\nIf you check API menu, 0.9.1 version should also appear (you may need to rerun hugo serve --watch).\n\n Table of contents\n\nYou maybe noticed that the menu on the left contains a small table of contents of the current page. All h2 tags (## Headline in Markdown) will be added automatically.\n\nAdmonitions\n\nAdmonition is a handy feature that adds block-styled side content to your documentation, for example hints, notes or warnings. It can be enabled by using the corresponding shortcodes inside your content:\n\n{{/* note title=\"Note\" */}}\nNothing to see here, move along.\n{{/* /note */}}\n\nThis will print the following block:\n\n{{ note title=\"Note\" }}\nNothing to see here, move along.\n{{ /note }}\n\nThe shortcode adds a neutral color for the note class and a red color for the warning class. You can also add a custom title:\n\n{{/* warning title=\"Don't try this at home\" */}}\nNothing to see here, move along.\n{{/* /warning */}}\n\nThis will print the following block:\n\n{{ warning title=\"Don't try this at home\" }}\nNothing to see here, move along.\n{{ /warning }}\n\nUsing the tip keyword makes a tip box\n\n{{ tip title=\"Don't try this at home\" }}\nNothing to see here, move along.\n{{ /tip }}\n\n",
        "tags": []
    },
    {
        "uri": "/resources/community",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Join the Vamp community\n---\nVamp is an open source project, actively developed by Magnetic.io. We encourage anyone to pitch in with pull requests, bug reports etc. \n\nContribute to Vamp \nVamp is split into separate repos and projects. Check the source on Github for an overview of all key repos (github.com - magneticio).   \nFeel free to contribute with Github pull requests.\n\n Submit change or feature requests \nLet us know your change or feature requests.  \nSubmit an issue on github tagged \"feature proposal\". \n\nReport a bug \nif you find  bug, please report it!  \nSubmit an issue on github, including details of the environment you are running Vamp in.\n",
        "tags": []
    },
    {
        "uri": "/resources/downloads/index",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Downloads\n---\n\nBinaries\nVamp\nVamp Gateway Agent (VGA)\nVamp CLI\n\n Homebrew\nVamp CLI for MacOS X\n\nDocker images\nVamp Gateway Agent (VGA) and HAProxy\nVamp workflow agent\n\n Build from source\nBuild Vamp\nBuild Vamp Gateway Agent (VGA)\n  \n-----------\n\nBinaries\n\n Vamp\nDownload: bintray.com/magnetic-io - Vamp  \nRequirements: OpenJDK or Oracle Java version 1.8.0_40 or higher\n\nExample\nLet's assume that the Vamp binary is vamp.jar.\njava -Dlogback.configurationFile=logback.xml -Dconfig.file=application.conf -jar vamp.jar\n\nlogback.xml is the log configuration file (example logback.xml file)  \nVamp uses the Logback library. Additional information about using Logback and the log file configuration format can be found on the Logback project page (logback.qos.ch). \napplication.conf is the main Vamp configuration file (example application.conf file)  \nDefault values (github.com/magneticio - reference.conf) are loaded on start and application.conf may override any of them.\nProcessing configuration is based on the typesafe library. Additional information about syntax and usage can be found on the project page (github.com/typesafehub - config). \n\n Vamp Gateway Agent (VGA)\n\nDownload: bintray.com/magnetic-io - Vamp Gateway Agent\n\nDocumentation can be found on the project page (github.com/magneticio - Vamp Gateway Agent). \nVamp CLI\n\nDownload: bintray.com/magnetic-io - Vamp CLI  \nRequirements: OpenJDK or Oracle Java version 1.8.0_40 or higher\n\n Manual install - Windows and Linux\nInside the extracted Vamp CLI binary package (bintray.com/magnetic-io - Vamp CLI) is a bin directory. Add it to your PATH statement, open a Console/CMD window and type vamp.  \nAfter installation, set Vamp’s host location:\n\nVamp’s host location specified as a command line option ( --host )\n\nvamp list breeds --host=http://192.168.59.103:8080\n\nVamp’s host location specified via the environment variable VAMP_HOST\n\nexport VAMP_HOST=http://192.168.59.103:8080\n\nHomebrew\n Vamp CLI for MacOS X\nDownload: We have Homebrew support to install the Vamp CLI on MacOS X  \nRequirements: OpenJDK or Oracle Java version 1.8.0_40 or higher  \n\nHomebrew install - MacOS X\n\nbrew tap magneticio/vamp\nbrew install vamp\n\nAfter installation, check if everything is properly installed with vamp version, then export the location of the Vamp host and check that the CLI can talk to Vamp:\nexport VAMP_HOST=http://localhost:8080\nvamp info\n\n Docker images\n\nVamp Gateway Agent (VGA) and HAProxy\nVamp Gateway Agent (VGA) Docker images with HAProxy can be pulled from the Docker hub (hub.docker.com - magneticio Vamp Gateway Agent).\n\n Vamp Workflow Agent\nA container for running small JavaScript-based workflows can be pulled from the Docker hub (hub.docker.com - magneticio Vamp workfow agent). \n Usually this will be pulled automatically.\n\nBuild from source\n\n Build Vamp\nRequirements: OpenJDK or Oracle Java version of 1.8.0_40 or higher, git (git-scm.com), sbt (scala-sbt.org), npm (npmjs.com) and Gulp (gulpjs.com)\n\nCheckout the source from the official repo (github.com/magneticio - Vamp):   \n  {{ note title=\"Note!\" }} \n  master branch contains the latest released version (e.g. 0.9.0). Versions are tagged.\n  vamp-ui is a separate project added as a git submodule to Vamp (ui subdirectory) it is, therefore, necessary to also checkout the submodule  \n  {{ /note }}\n  git clone --recursive git@github.com:magneticio/vamp.git  \n  or specific branch: git clone --recursive --branch 0.9.0 git@github.com:magneticio/vamp.git\n\nRun ./build-ui.sh && sbt test assembly\nAfter the build ./bootstrap/target/scala-2.11 directory will contain the binary with name matching vamp-assembly-*.jar\n\nCheck this example: github.com/magneticio - Vamp docker quick start make.sh.\n\nBuild Vamp Gateway Agent (VGA)\n\nRequirements: Go (golang.org) and git (git-scm.com)\n\nCheckout the source from the official repo (github.com/magneticio - Vamp gateway agent). Current master branch is backward compatible with the latest 0.9.0 Vamp build.\nSet Go variables depending on target environment\nRun:\n\ngo get github.com/tools/godep\ngodep restore\ngo install\nCGO_ENABLED=0 go build -v -a -installsuffix cgo\n\nCheck this example: github.com/magneticio - clique-base make.sh. More details can found on the project page: github.com/magneticio - Vamp gateway agent.",
        "tags": []
    },
    {
        "uri": "/resources/news/8-docker-productivity-hacks",
        "title": "8 Docker productivity hacks for better living",
        "content": "\n\nWe've been building smaller and bigger tools on/for/with Docker for roughly the last 1,5 years.\nDuring that time I've found I always come back to using little productivity hacks and one-liners that\nI found on the internet or came up with myself. Here they are in one place, as much as for our own reference\nas for anybody else to enjoy. !--more-- We use this stuff every day. Here we go:\n\n1. Remove images that are no longer used by a container\nLet's free up some disk space!\n\n    $ docker rmi docker images -q -f dangling=true\n\n 2. Remove all exited containers\nLet's free up some more disk space!\n\n    $ docker rm docker ps -q -a -f status=exited\n\n3. Log into an already running container\nSomehow it took me ages to figure this one out. Hope I'm not the only dummy. Of course, the container should have some\nshell installed, like bash.\n\n    $ docker exec -i -t IDOFRUNNING_CONTAINER /bin/bash\n\n 4. Run \"top\" inside a container\n\nThis one is somewhat controversial. Resource usage in Docker is counterintuitive and complicated. Having said that, just running top\ninside a container gives me a relatively good insight how things are doing in that container`s context:\n\n    $ docker exec -t IDOFRUNNNIG_CONTAINER /bin/sh -c \"export TERM=xterm-256color && top -bn 2\"\n\nP.S: Ignore the first output cycle. Why? Because reasons.    \n\n5. Don't expect anything magic from the \":latest\" tag     \n\nOne of the huge misconceptions among fresh Docker users. The :latest tag has no special magic. It is not the HEAD\nin a Git log. It is not an automatic pointer to the top of of your stack of images. It is just a convention. Getting\nthe :latest version of an image implies you trust the creator of the image to really tag the last one the build with the\n:latest tag. More info see this post by our friends at Container Solutions\n\n 6. Cut container download time with export/load in Vagrant\n\nNot a real one-liner, but hey! When building stuff with (CoreOS) Vagrant boxes, I destroy those boxes quite often just to get a clean slate. However, my box runs Docker and stores the images inside the box. The box now has to download some of the containers I run in that box every time I refresh my environment. Let's fix that.\n\nJust save the Docker image to disk as a tar file.\n\ndocker save magneticio/myimage:1.0  /home/share/magneticiomyimage10.tar\n\nNow mount your disk to Vagrant and import it using this snippet in your Vagrantfile:\n\nVagrantfile\nMYCONTAINER=\"magneticiomyimage_10.tar\"\nvm.syncedfolder \".\", \"/home/share\", id: \"core\", :nfs = true, :mountoptions = ['nolock,vers=3,udp']\nconfig.vm.provision :shell, :inline = \"export TMPDISK=/\", :privileged = false\nif File.exist?(MY_CONTAINER)\n    config.vm.provision :shell, :inline = \"docker load -i /home/share/{MY_CONTAINER}\", :privileged = false\nend\n\n7. Use \"boot2docker shellinit\" to setup your Docker ENV variables\n\nDestroying and restarting a lot of boot2docker instances on OSX? Creating too many terminal sessions? Use this your Docker client setup correctly for boot2docker.\n\n    $ eval $(boot2docker shellinit)\n\n 8. Read the \"Dockerfile best practices\" info\n\nReally useful and compulsory reading: https://docs.docker.com/articles/dockerfile_best-practices/    \n\n",
        "tags": [
            "articles",
            "Docker",
            "tips",
            "tricks",
            "linux",
            "one-liners"
        ]
    },
    {
        "uri": "/resources/news/canaries-and-containers",
        "title": "Canaries and Containers - DC/OS and VAMP",
        "content": "\nWhat do canaries and containers have in common? \n\nWith the advent of container-technology (developed by Google in their now famous Borg project and made wildly popular by companies like Docker and Mesosphere) innovative ways of deploying, running and (auto)scaling software like microservices become possible for everybody. But “with great power also comes great responsibility” and thus these new possibilities also need a new breed of tools to really leverage the benefits of container technology in real world production environments and scenarios.\n\n[DC/OS] (http://dcos.io) and [VAMP] (http://vamp.io) are such tools that help you make it easy to get the best out of using containers when you move away from “hello world” test-setups and into real world environments.\n\n!--more--\n\nTo run containers in production environments, you will need a cluster manager. A single machine to run your containers on simply doesn’t give you the resilience you are aiming for. If a machine breaks down you want your containers to instantly, automatically and transparently be moved to other machines without your users experiencing any downtime. If you need more processing-power because of increased demand you want to be able to easily and quickly add machines to increase the pool of computing-resources that your containers can transparently make use of. This is where a container cluster-manager comes in.\n\nOne of the most used and battle-hardened container cluster-managers out there is Mesos. Mesos is an Apache project that spun out of technology developed and used by AirBnB and Twitter. Mesosphere is the company that is one of the core contributors to Mesos and also packages Mesos into DC/OS (the Datacenter Operation System). DC/OS is a robust and commercially supported product that is built on top of Mesos and adds powerful features like command-line and web-interfaces, simple packaging and installation, and a growing ecosystem of frameworks that can run on top of it. Mesos and DC/OS are powering famous platforms like Apple Siri, Bloomberg, Paypal, but can also be very useful on (much) smaller clusters.\n\nAn important feature of DC/OS is that it’s open-source. Opensource software these days is an important aspect of running trusted and hardened production systems and has become an important requirement for engineers and IT departments alike. We believe DC/OS is a great step forward in easing the transition to and adoption of containers and container-cluster managers in real world production environments.\n\nBecause of the heritage of Mesos, DC/OS delivers a very compelling way to run containers in production. It’s opensource, easy to install, well documented, fully featured and solves real world problems with a growing ecosystem of integrated solutions and frameworks that are easy to install and experiment with.\n\n Canary testing & releasing containers\nOne of the most popular and essential frameworks for Mesos and DC/OS is Marathon. Marathon is a container-orchestration framework and is designed to manage long-running jobs. Long-running jobs in containers are typically web-oriented API’s, applications and microservices.\n\nWhile DC/OS and Marathon make it easy to deploy, run and orchestrate these containers, you need an additional “experiment framework” to enable a process of continuously measuring, improving and scaling your container-packaged software without negatively affecting your visitors or having downtime. We call this “Continuous Improvement” and this is where VAMP comes in. \n\nCompanies like Facebook, Spotify and Netflix are very successfully using a pattern called Canary testing & releasing. It’s an advanced variant of blue-green releasing, where you seamlessly switch from the current to a new version of software without perceived downtime for your visitors. VAMP is an opensource framework that integrates with DC/OS and Marathon and delivers canary-testing & releasing and autoscaling features in an easy-to-use and powerful way.\n\nWhen deploying one or more new versions of your software in containers to production, VAMP enables you to expose these versions to only a small percentage of your visitors with specific criteria (a ‘bucket’ or ‘cohort’).\n\nYou can now test and validate the technical and/or business performance of your new software versions in this limited setting but in production. This means you don’t need to over-optimise prematurely, and can detect and fix potential  issues before you move gradually to a full scale deployment.\n\nA typical real world use-case for canary-testing & releasing with VAMP and DC/OS is:\n\nValidating the performance of a new (e.g. responsive) website front-end by developing and optimising it only for Chrome-browsers first, exposing it to 5% of your visitors with Chrome-browsers (you could even do this for specific resolutions and devices), validating your hypothesis (e.g. better conversion and/or faster response times), fixing technical issues, and then scaling up to a higher percentage of visitors and/or gradually adding new browser-types until you reach a full deployment. \n\nWith VAMP on DC/OS this can done by simply sending these rules to VAMP (either using our Graphical UI, command-line interface or directly to the REST API):\n\n---\n frontend_A: \n   weight: 100%\n frontend_B:\n   weight: 0%\n   filter_strength: 5%\n   filters:\n     user-agent == Chrome\n\nOther use-cases are the validation and testing of different technological and architectural solutions (f.e. couchDB vs MongoDB as an embedded microservice datastore) in production and without impacting your visitors, or experimental finding of the optimal balance between allocated computing-resources (i.e. running costs) and the performance of your services.\n\nVAMP supports grouping of filters, Boolean expressions (AND/OR/NOT), and provides a built-in set of commonly used “gateway short codes” like cookies, browser-types, headers and host-names in addition to supporting HAProxy ACL rules and configuration templates. VAMP also supports sticky sessions, URL path rewriting (very useful for API versioning, aggregation and gateway) and makes sure that services are correctly “drained” when taken out of the gateway.\n\nAutoscaling\nSo now we know that VAMP makes it very easy to implement all kinds of useful canary-testing & releasing patterns. But when you are increasing the percentage of visitors or are expanding your filter-criteria to allow more visitors, you also need to scale up the number of running instances or allocated computing resources. Of course with Marathon it’s easy to use the UI or API to set the scale of the running containers. But wouldn’t it be much more practical if you could simply change the scaling-settings at the same time when changing the gateway and load-balancing rules? Or even cooler: change the scaling automatically, based on performance-criteria? We thought so too, so we made it easy to do with VAMP:\n\nIn VAMP you can set scaling parameters manually using our API, UI or CLI:\n\n---\nname: myVAMPblueprint\n  clusters:\n    my_cluster:\n      services:\n        breed:\n          name: my_breed\n          deployable: registry.example.com/app:1.0\n        scale:\n          cpu: 2\n          memory: 1024MB\n          instances: 4\n\nYou can also use scale-sets and references to these, which is useful when working with several teams or environments:\n\n---\nname: medium_prod\ncpu: 2\nmemory: 4096MB\ninstances: 3\n---\nname: medium_test\ncpu: 0.5\nmemory: 1024MB\ninstances: 1\n\nand refer to them by using:\n\n---\nscale:\n  reference: medium_test \n\nEven cooler and very handy is that you can use VAMP to define automated up and down scaling. It’s very easy. In the deployment definition you simply define a Service Level Agreement (SLA) and an escalation-type. VAMP provides common built-in patterns for this, and our upcoming workflow-engine enables you to easily create your own workflows with a few lines of javascript.  \n\nTo setup a basic auto-scaling workflow based on the aggregated response-time of a cluster of containers we simply post the following to a VAMP cluster:\n\n---\n sla:\n       Type of SLA.\n      type: responsetimesliding_window\n      threshold:\n        upper: 1000   # Upper threshold in milliseconds.\n        lower: 100    # Lower threshold in milliseconds.\n      window:\n        interval: 300 # Time period in seconds used for\n                      # average response time aggregation.\n        cooldown: 600 # Time period in seconds. During this \n                      # period no new escalation events will \n                      # be generated. New event may be expected \n                      # not before cooldown + interval time has \n                      # been reached after the last event. \n     \n      # List of escalations.\n      escalations:\n        type: scale_instances\n          minimum: 1\n          maximum: 3\n          scale_by: 1\n\nThis will constantly measure the aggregated backend response time of the running cluster of containers, and when the response-time exceeds 1000 milliseconds for over 5 minutes the number of running instances will increase with one (1) until the maximum of three instances is reached. When the response-time becomes lower than 100 milliseconds for over 10 minutes VAMP will make sure the number of instances is scaled down one by one, until the defined minimum of one instance is reached again. VAMP will make sure that new instances are correctly load-balanced, and that removed instances will be correctly drained (of course taking into account sticky sessions and TTL settings).  \n\nVAMP does not only support horizontal scaling as described above, but also vertical scaling (changing memory or CPU scales) and the grouping of multiple escalations, both in sequence or in parallel.\n\nGetting started with VAMP and DC/OS\nWhat do I need to do if I want to start experimenting with all these possibilities that VAMP and DC/OS deliver as described in this blog post?\n\nFirst setup a cluster and download and install DC/OS as instructed: [https://dcos.io/docs/1.7/administration/installing/custom/gui/] (https://dcos.io/docs/1.7/administration/installing/custom/gui/)\n\nNow install VAMP as a DC/OS package: [http://vamp.io/documentation/installation/dcos/] (http://vamp.io/documentation/installation/dcos/)\n\nNow you can start with our Getting started with VAMP tutorials: [http://vamp.io/documentation/guides/] (http://vamp.io/documentation/guides/)\n\n TL/DR & Summing it up\nTo make the best use of containers in production environments you need a container cluster manager to deliver resilience and performance. Mesos has an amazing heritage, and all this experience is now packaged into the DC/OS. This gives you an opensource container-cluster solution that is easy to install, battle-hardened, well documented, and solves real world problems by providing an extensive and growing set of solutions and frameworks that run on top of it. \n\nOne of these solutions is VAMP (www.vamp.io) an opensource framework that makes canary-testing/releasing and autoscaling of containers and microservices easy and powerful. Companies like Spotify, Facebook and Booking.com have moved from linear continuous deployment pipelines to continuous improvement feedback-loops using canary-testing&releasing and autoscaling patterns. Canary-testing & releasing is an advanced version of blue-green deployments to avoid downtime when releasing new software. \n\nWhen deploying new versions of their software to production they expose these to a small percentage of visitors with specific criteria (a ‘bucket’ or ‘cohort’) to test and validate technical and business performance. When successfully validated they increase the percentage of visitors that land on the new version and at the same time scale-up the number of instances and/or assigned resources to handle the increasing number of visitors with the desired performance-requirements like f.e. response time. \n\nThese canary-test/release and autoscaling patterns require a complex and highly technical coordination and choreography between deploying, load-balancing, metric-aggregation and scaling. Until recently this was only possible to the few companies that could dedicated large amounts of research and development capabilities to this problem.  \n\nVAMP and DC/OS now make it easy to setup and leverage a container-cluster for production-grade environments and start working with containers, microservices and canary-test/release and autoscaling patterns without having to custom-build or understand the underlying technologies.\n\nOlaf Molenveld\nolaf@magnetic.io\n",
        "tags": [
            "blog",
            "vamp",
            "news"
        ]
    },
    {
        "uri": "/resources/news/epic-first-post",
        "title": "Epic first post: open sourcing Vamp!",
        "content": "\nFrom today onwards, we are open sourcing the development of Vamp, the Very Awesome Microservice Platform. Yay!\nFor me and the team this is quite a milestone and a very exciting next step in how we build Vamp and who contributes\nto it. We are still in alpha, so things might still be somewhat rough around the edges, but what Vamp can do \nalready surpasses what I had imagined over a year ago! Big up for the team!\n\n      \nSo, what can you expect from Vamp and its development?\n\nOpen development on all fronts: open design, open code, easy extending.\nFocus on usability: good documentation, helpful error messages and clear API design.\nStandard where possible: No reinvention if not necessary!\n\nOver the coming weeks and months we will be releasing a lot of write-ups, reference docs and cookbook-type\ntutorials on how to use Vamp and what tricks it can do.\n\nTim Nolet, CTO\n \n \n",
        "tags": [
            "first",
            "launch"
        ]
    },
    {
        "uri": "/resources/news/getting-started",
        "title": "New getting started tutorial & UI sneak preview!",
        "content": "\nI'm very happy to report we just added an initial set of tutorials to our getting started section. These tutorials should get you up and running quickly and help you really kick the tires of Vamp.\n\nIn these tutorials, you'll learn how to do simple deployments, do canary releases, use filters and do merges and \ndeletes on deployments using blueprints. There's a lot more coming, like tutorials on SLA's, but the key principles are all there. We've also updated the installation steps, outlining some of the restrictions more clearly and adding some extra helpers. \n\nOf course, we've also finished another sprint and pushed 0.7.0-RC3 which has some important bugfixes. More exciting\nis that we started designing and coding the UI for Vamp. Its initial release as part of Vamp Core is planned for\nrelease 0.8.0 at the end of next month, but here's a little sneak peek. \n\nNote: This is very early stage! Lots of things might change!\n\nmanage deployments\n\nedit and deploy blueprints\n\nWe hope these updates will help people to get started with Vamp while in alpha but also show where Vamp\nis heading in the near future.\n\nCheers!\n\nTim Nolet, CTO\n\n",
        "tags": [
            "tutorials",
            "ui"
        ]
    },
    {
        "uri": "/resources/news/going-native",
        "content": "---\ntitle: \"Going native - building native packages for Vamp\"\ndate: \"2015-08-05\"\ntags: [\"articles\", \"sbt-native-packager\",\"vamp\",\"debian\",\"homebrew\"] \ncategory: [\"articles\"] \nauthor: \"Matthijs Dekker\" \ntype: blog \ndescription: \"Improving you experience using Vamp starts with how you install Vamp. We decided to create native packages, which will make your life easier, without complicating ours (to much). In this technical write up we tell you how we did it.\"\n---\n\nTo give you a better experience installing Vamp, we decided to create native installers for the most common used platforms.\n\nOkay, we already did the hard part: We designed and created the software, performed the tests, made a zip file, put the zip file on the website and wrote some documentation.\n\nOnly thing you had to do, is download the file, unzip it, put it contents in the correct directory, change the configuration files and write some startup scripts. And repeat these steps for every Vamp module. Easy as eating pie, right?\n\nWe didn't think so either. \n\nTime to go native.\n!--more-- \n\nTargeting platforms\n\nThe Vamp modules will be typically installed on a server, with the exception of Vamp CLI, which is likely to be installed on desktop/laptop environments also. For servers, we'll create native installers for the most widely used Linux flavors; for the desktop we'll also include OSX, since thats what we are using ourself. And even for the platforms we don't target directly, we can make things a bit easier, by creating a universal install package.\n\n SBT Native Packager\n\nWhere a Java developer uses Maven, a Scala developer uses SBT as its build system. The SBT ecosystem is quite vibrant and there a some real nice plugins available. One of them is the SBT native packager plugin, which promisses to package your software to run anywhere.\n\nWe created a separate project for the distribution aspect of Vamp, which contains a directory for each module we want to publish. Out of the box, sbt-native-packager has the ability to create a so called 'Universal' package. This is a zip file in which you'll find a bin directory and a lib directory. To use a universal package, all you have to do is unzip the file and add the bin directory to your PATH statement. That is already a huge improvement.\n\nUniversal customization\n\nHaving a closer look at the Universal package, there are somethings we'd like to change.\nFirst of, the contents of the lib directory. It contains all the jar files our application needs, which are quite a few file. Lets change that, by wrapping the files into one single big jar file. With the help of the sbt-assembly plugin this can be done quite easy, as the excerpt from the CLI   build.sbt file shows:\n\n//Make a fat jar\nassemblyJarName in assembly := \"vamp-cli.jar\"\n\n// removes all jar mappings in universal and appends the fat jar\nmappings in Universal := {\n val universalMappings = (mappings in Universal).value\n val fatJar = (assembly in Compile).value\n // removing means filtering\n val filtered = universalMappings filter {\n   case (file, fileName) =  ! fileName.endsWith(\".jar\")\n }\n // add the fat jar\n filtered :+ (fatJar - (\"lib/\" + fatJar.getName))\n}\n\n// the bash scripts classpath only needs the fat jar\nscriptClasspath := Seq( (assemblyJarName in assembly).value )\n\nThe second thing we want to change in the CLI package, is the contents of the bin directory. The scripts in the bin directory are automatically generated when the package is being build and there are methods of adding or overriding functionality. For Vamp CLI, we want the name of the executable script to be vamp instead of vamp-cli and we'd like to include scripting to verify the installed Java version. For this, we had to add some lines to the build.sbt.\n\nexecutableScriptName := \"vamp\"\n\n// Add check for Java 8 (not for windows)\nbashScriptExtraDefines ++= IO.readLines(baseDirectory.value / \"scripts\" / \"java_check.sh\")\n\n Standard Linux\n\nFor Linux, we'd like to support Ubtuntu / Debian & Red Hat / CentOS. This means, we need to create .deb and .rpm packages. To do this, we specify some values in the build.sbt, to be included in the package mata information. For .deb packages, we've added:\n\ndescription := \"This is the command line interface for VAMP\"\npackageDescription := \"CLI for the Very Awesome Microservices Platform\"\npackageSummary := \"The Vamp CLI\"\nmaintainer :=  \"Matthijs Dekker matthijs@magnetic.io\"\nFor the .rpm package, we've added:\n\nrpmVendor := \"Magnetic.io\"\nrpmUrl := Some(\"http://vamp.io\")\nrpmLicense := Some(\"Apache 2\")\nAnd that's it. Now we can create Linux packages just by executing the sbt tasks debian:packageBin and rpm:packageBin\n\nWell not quite ...\n\nThe init problem\n\nThe .deb package for Vamp CLI was easy to setup, but for our other components, things got a little more complex. The Linux 'civil war', which was all about which system to use for startup and shutdown routines, left a fragmented Linux world. Ubuntu 14.04 LTS uses Upstart, Debian 7 uses SystemV and Debian 8 and Ubuntu 15.04 Systemd. Since we want to support all 4 releases, this means we have to create 3 different .deb packages, one for each init system.\n\nLuckily, sbt-native-packager has support for all three init systems and by specifying the serverLoading system, you can tell it which one to use.\nFor example, to use SystemV you would use:\n\nserverLoading in Debian := com.typesafe.sbt.packager.archetypes.ServerLoader.SystemV\n\nWith some SBT trickery, defining a new task 'packageDebianAll', we can create packages for all three init systems.\nFor full details, see the build.sbt file for Vamp Core.\n\n Configuration & data\n\nOur server applications require configuration files and need to write data to disk. By adding mappings, we can include files into the package.\n\nmappings in Universal += (packageBin in Compile, sourceDirectory ) map { (_, src) =\n  val conf = src / \"main\" / \"resources\" / \"reference.conf\"\n  conf - \"conf/application.conf\"\n}\n\nThis will create a conf directoy in our package, with a file called application.conf.\n\nTo have the configuration file picked up by the startup script, we add it to the bashScriptExtraDefines.\n\nbashScriptExtraDefines += \"\"\"addJava \"-Dconfig.file=${app_home}/../conf/application.conf\"\"\"\"\n\nWe have configured our application to write data in a data directory, which we need to create when installing the software:\n\n// Add an empty folder to mappings\nlinuxPackageMappings += packageTemplateMapping(s\"/usr/share/${name.value}/data\")() withUser(name.value) withGroup(name.value)\n\nAnd that's it for Vamp.\n\nLets GO all the way\n\nOne Vamp module got left out, Vamp Router. While the other Vamp modules are all written in Scala, Vamp Router is a Go application and has it's own build process. This build process creates .zip files, based on the hardware platform it can run on. Linux .deb & .rpm packages are however not created by this build, and we came up with a way to use sbt-native-packager for this purpose.\n\nFirst, we'll pull the vamp-router zip file from our Bintray download site in a resourceGenerator:\n\nval vampRouterVersion = \"0.7.9\"\n\nval platform = \"amd64\"\n\nresourceGenerators in Compile += Def.task {\n  val location = url(s\"https://bintray.com/artifact/download/magnetic-io/downloads/vamp-router/vamp-router${vampRouterVersion}linux_$platform.zip\")\n  IO.unzipURL(location, target.value / platform).toSeq\n}.taskValue\n\nNext we create mappings, to include the files we've extracted from the zip file.\n\n// copy vamp-router from the extracted bintray zip\nlinuxPackageMappings += packageMapping( (target.value / platform / \"vamp-router\",  \"/usr/share/vamp-router/vamp-router\") ) withPerms \"755\"\n\n// Add the config files\nmappings in Universal += (packageBin in Compile, target ) map { (_, target) =\n  val conf = target / platform / \"configuration\" / \"error_pages\"  / \"500rate.http\"\n  conf - \"configuration/error_pages/500rate.http\"\n}\n\nmappings in Universal += (packageBin in Compile, target ) map { (_, target) =\n  val conf = target / platform / \"configuration\" / \"templates\" / \"haproxy_config.template\"\n  conf - \"configuration/templates/haproxy_config.template\"\n}\n\nmappings in Universal += (packageBin in Compile, target ) map { (_, target) =\n  val conf = target / platform / \"examples\" / \"example1.json\"\n  conf - \"examples/example1.json\"\n}\n\nWe add our own start up script, since the generated one would not work for us:\n\n// Add the script file to which starts vamp-router\nmappings in Universal += (packageBin in Compile, sourceDirectory ) map { (_, src) =\n  val bin = src / \"templates\" / \"bash-template\"\n  bin - \"bin/vamp-router\"\n}\n\nAnd we remove the generated jar file, since there isn't actually anything in there.\n\n// removes all jar mappings in universal\nmappings in Universal := {\n  val universalMappings = (mappings in Universal).value\n  val filtered = universalMappings filter {\n    case (file, fileName) =  ! fileName.endsWith(\".jar\")\n  }\n  filtered\n}\n\nSince the packages we are creating have dependencies and are not architecture independent, we need to add some additional statements to reflect this in the meta data of the packages\n\nval rpmArchitecture=\"x86_64\"\nval debianArchitecture = \"amd64\"\n\ndebianPackageDependencies in Debian ++= Seq(\"haproxy\", \"bash (= 2.05a-11)\")\npackageArchitecture in Debian := debianArchitecture\ndebianSection in Debian := \"net\"\n\npackageArchitecture in Rpm := rpmArchitecture\n\nWith this done, we can create packages by running sbt rpm:packageBin and sbt packageDebianAll. This will create the debian amd64 and the rpm x86_64 packages. For the Debian i386 packages, we needed to repeat the trick, which we've done by simple duplicating the whole router-amd64 directory.\n\nAnd that wraps up all Linux packaging.\n\n Brewing our first OSX package\n\nA popular method of installing software on OSX is using homebrew. It allows you to install software from the command line, without the hassle of the Apple App Store. Creating a brew package is not that hard. We setup an additional github repository, homebrew-vamp, here we could store our formula.\n\nOur forumula uses the Vamp CLI Universal package as a basis. For the brew package, we add an additional script to the Universal package\n\nThe complete formula is just a couple of lines of Ruby\n\nrequire \"formula\"\n\nclass Vamp < Formula\n  homepage \"http://vamp.io\"\n  version \"0.7.9\"\n  url \"https://bintray.com/artifact/download/magnetic-io/downloads/vamp-cli/vamp-cli-#{version}.zip\"\n  # generate the sha256 hash on your mac with the command: shasum -a 256 filename\n  sha256 \"c6385ceff1200c1f990bf133f5189270eca4a174ceedacb4a8e9915eda3b02ca\"\n\n  def install\n      inreplace \"brew/vamp\", \"##PREFIX##\", \"#{prefix}\"\n      prefix.install \"lib/vamp-cli.jar\"\n      bin.install \"brew/vamp\"\n  end\nend\nWhen we release a new version of Vamp, we need to update our formula, with the latest version number and an updated sha256 hash.\n\nFire and forget\n\nWith everything setup to create packages, the next step was automating it, so we can publish all packages with the push of a button. For every release, we update the version number the build.sbt files and update the library dependencies. Once, we push these changes to Github, Travis CI will start building the packages and with the help of some custom bash scripts, push it all to Bintray.\n\nThe final step is to update the brew formula on Github and we've done another release!\n",
        "tags": []
    },
    {
        "uri": "/resources/news/index",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: News\n---\n\nVamp 0.9.0 Beta release \n9th September 2016\n\nThe Vamp 0.9.0 release is a very important milestone in the lifecycle of Vamp, as we're removing the Alpha label and are moving to Beta! This means that we will do our utmost best to avoid breaking changes in our API's and DSL, focus even more on stabilising and optimising the current feature-set, while of course continuously introducing powerful new features.\n  \nThe Vamp 0.9.0 release is the culmination of three months of hard work by our amazing team! This release incorporates nothing less than 115 issues and I'm very proud of what we've achieved.\n\nSome of the most notable new features are:\n \na brand new opensource UI with much better realtime graphs, sparklines, info, events panel and access to all relevant API objects like breeds, deployments but also new options like gateways and workflows.  \npowerful integrated workflows for automation and optimisation like autoscaling, automated canary-releasing etc. using efficient Javascript-based scripting.\nKubernetes and Rancher support.\nsupport for custom virtual host names in gateways.\nsupport for custom HAProxy templates.\na brand new Vamp Runner helper application for automated integration testing, mocking scenarios and educational purposes.  \n\nAnd, of course, there's a massive amount of improvements, bug fixes and other optimisations.\n\ncomplete list of all the closed issues in this release",
        "tags": []
    },
    {
        "uri": "/resources/news/Magnetic_io-VAMP-haalt-500k-op-in-investeringsronde-geleid-door-VoltaVentures",
        "title": "Persbericht  - Magnetic.io VAMP haalt €500k op in investeringsronde geleid door Volta Ventures",
        "content": "\nMagnetic.io VAMP haalt €500k op in investeringsronde geleid door Volta Ventures om opensource canary test en release platform voor containers en microservices op te schalen\n\nDEVOPS EN BUSINESS TEAMS GEBRUIKEN VAMP OM STORINGEN TE VOORKOMEN, RELEASES TE VERSNELLEN, EN PERFORMANCE VAN ONLINE SOFTWARE TE OPTIMALISEREN.\n\nAMSTERDAM, 23 Mei, 2016 — Magnetic.io, een in Amsterdam gevestigde hightech startup, heeft aangekondigd dat het een investering van €500.000 heeft opgehaald in een ronde geleid door Volta Ventures. Magnetic.io ontwikkelt VAMP, een opensource oplossing om canary-test en -release, en auto-scaling functionaliteiten, toe te voegen aan systemen die gebruik maken van populaire (Docker) container en microservice technologieën. De functionaliteiten die VAMP toevoegt maken het voor DevOps teams makkelijker om in minder tijd een hogere kwaliteit van online software in productie te brengen. De investering zal worden gebruikt voor de ontwikkeling van enterprise functionaliteiten voor het VAMP framework, het stimuleren en ondersteunen van de opensource community en het partner netwerk, en om de internationale groei te versnellen. Huidige investeerder Startupbootcamp (SBC) en succesvolle ondernemer Henri de Jong (Mijndomein) nemen ook deel in de investeringsronde. Dhr. Maene, managing partner van Volta Ventures, zal toetreden tot de raad van bestuur.\n\nOm uitdagingen op het gebied van online en digitale groei, performance en time-to-market op te lossen, passen steeds meer online organisaties technieken als continuous-delivery, containers en microservices toe. Om het volledige potentieel van deze technologieën te benutten, voor zowel IT als business, is een overkoepelend experimenteer-systeem zoals VAMP essentieel. Door alle onderliggende onderdelen slim te coördineren wordt het mogelijk om snel, eenvoudig en vooral risico-vrij continue nieuwe versies van online software naar productie uit te rollen, te testen, optimaliseren en op te schalen. \n\n!--more--\n\nSuccesvolle online bedrijven zoals Netflix, AirBnB en Facebook passen deze “canary test en release” methode al jaren zeer succesvol toe. Zij lanceren vrijwel continue nieuwe versies en varianten van hun software naar kleine percentages bezoekers, meten en optimaliseren vervolgens de performance, en verhogen dan stapsgewijs het percentage bezoekers om zo op een risicovrije en gecontroleerde manier tot een stabiele en geoptimaliseerde upgrade te komen. \n\nTot voor kort was het voor de meeste organisaties complex en duur om dit soort “canary methodieken” toe te passen. VAMP maakt het eenvoudig en kosteneffectief om deze slimme en veilige manier van online software in productie testen en upgraden toe te voegen aan bestaande en nieuwe architecturen. VAMP kan zowel stand-alone werken, als ook in combinatie met veelgebruikte container en microservices oplossingen zoals bijvoorbeeld Docker, Mesosphere DC/OS, Azure Container Service, Rancher en Kubernetes.\n\nFrank Maene, managing partner Volta Ventures:\n“Wij zijn overtuigd van de visie van Olaf en zijn team op de toegevoegde waarde van VAMP in een sterk groeiende container en microservices markt. Wij geloven dat VAMP een belangrijke rol zal spelen bij het mogelijk maken van het slimmer uitrollen, optimaliseren en schalen van online software en containers. Wij zijn enthousiast om hier deel van uit te maken en het VAMP team hierbij te ondersteunen.”\n\nOlaf Molenveld, Magnetic.io CEO en co-founder: \n“We zijn zeer blij om Volta Ventures, Henri de Jong en SBC aan boord te hebben. Samen kunnen we onze missie om organisaties slimmer software te laten uitrollen, optimaliseren en schalen nog beter vorm geven. De ervaring waar we nu uit kunnen putten is zeer waardevol. Dit gaat ons absoluut helpen om ons product, het team en de opensource community verder te laten groeien.” \n\nOm binnen 5 minuten met VAMP’s canary en auto-scaling functionaliteiten aan de slag te gaan is een eenvoudig te installeren QuickStart versie gratis te downloaden vanaf http://vamp.io. Voor informatie over de enterprise functionaliteiten en andere vragen stuur een email naar info@magnetic.io     \n\nOver Magnetic.io  \nMagnetic.io VAMP is het opensource platform dat slimme functionaliteiten voor (A/B) testen, uitrollen en automatisch schalen toevoegt aan (micro)service en container gebaseerde architecturen. Het bedrijf is gevestigd in Amsterdam en is mede opgericht door Olaf Molenveld\nwww.vamp.io\n\nContact: Olaf Molenveld, CEO,  \nolaf@magnetic.io, tel +31 653 36 27 83\n\nOver Volta Ventures  \nVolta Ventures Arkiv investeert in jonge en ambitieuze internetbedrijven in de Benelux. Het fonds heeft € 55 miljoen onder beheer en wordt ondersteund door het EIF en ARKimedes-Fonds II.\nwww.volta.ventures\n\nContact: Frank Maene, managing partner,   \nfrank@voltaventures.eu, tel. +32 477 20 17 82\n\nOver Startupbootcamp  \nStartupbootcamp is opgericht in 2010 en is inmiddels uitgegroeid tot een internationaal accelerator programma voor startups met 13 programma's in Amsterdam, Barcelona, ​​Berlijn, Eindhoven, Istanbul, Londen, Miami, New York en Singapore. Het mentor en alumni-netwerk is verspreid over meer dan 50 landen. Meer informatie is te vinden op: www.startupbootcamp.org\n",
        "tags": [
            "vamp",
            "news",
            "press"
        ]
    },
    {
        "uri": "/resources/news/Magnetic_io-VAMP-raises-500k-in-investmentround-led-by-VoltaVentures",
        "title": "Magnetic.io VAMP raises €500K in round led by Volta Ventures to deliver opensource canary testing and releasing solution for container and microservice systems",
        "content": "\nDevOps and business teams use VAMP to reduce downtime, improve time-to-market and increase performance of online software.\n\nAMSTERDAM, May 23, 2016 — Magnetic.io, an Amsterdam-based hi-tech startup that develops opensource solution VAMP (http://vamp.io), today announced that it has raised € 500,000 in a round led by Volta Ventures. \nVAMP adds powerful canary testing and releasing, and autoscaling features to (Docker) container and microservice based systems. These features enable DevOps teams to deliver higher quality software in a shorter time. The investment will fuel the development of VAMP’s enterprise-grade features, grow the opensource community and partner network, and boost international expansion. Existing investor Startupbootcamp (SBC) and successful entrepreneur Henri de Jong (Mijndomein) will also participate in the round. Mr. Maene, Volta Ventures’ managing partner, will join the board of directors. \n\nTo solve scaling, performance and time-to-market challenges, digital-savvy organisations are quickly adopting strategies like continuous-delivery, containers and microservices. To unlock the full potential of these technologies to both IT and business, VAMP adds an \"experiment system\" with canary and autoscaling features that make it safe, fast and easy to release, test and scale new software versions in production. \n\n!--more--\n\nSuccessful online companies like Netflix, AirBnB and Facebook have already demonstrated the power of canary testing and releasing. New versions of software are continually released in production to small percentages of visitors, performance is measured and improved, and the number of allowed visitors is gradually increased to achieve a risk-free and stable full release. \n\nUntil recently, it was complex and expensive to apply these canary test and release methods. With VAMP it becomes straightforward and cost effective to add these features to both new or existing architectures. VAMP can work standalone or integrate with container systems like Docker, Mesosphere DC/OS, Azure Container Service, Rancher and Kubernetes.\n\nFrank Maene, managing partner Volta Ventures: “We like the vision Olaf and his team have on the value add of VAMP in the burgeoning containers and microservices market. We believe VAMP will play an important role in enabling smarter ways of testing, delivering and scaling online software and containers, and we are excited to become a partner of the VAMP team and help to make this happen.”\n\nOlaf Molenveld, Magnetic.io CEO and co-founder: “We are very excited to have Volta Ventures, Henri de Jong and SBC on board in our mission to help organisations test, deliver and scale software in a smarter and more effective way. The experience and track-record available in this extended team is huge. It will help us to accelerate the product, the company and the opensource community adoption and we are very much looking forward to work together!”\n\nTo start working with VAMP’s Canary and autoscaling features, download the free opensource version of VAMP from http://vamp.io. An easy-to-install QuickStart package is available to start working with VAMP within 5 minutes. For information about enterprise features please email info@magnetic.io\n\nAbout Magnetic.io  \nMagnetic.io’s VAMP is an open source platform that provides features for (A/B) testing, deploying and autoscaling (micro)service oriented architectures that rely on container technology. \nThe company is headquartered in Amsterdam and was created by Olaf Molenveld.\nwww.vamp.io\n\nContact: Olaf Molenveld, CEO,  \nolaf@magnetic.io, tel +31 653 36 27 83\n\nAbout Volta Ventures  \nVolta Ventures Arkiv invests in young and ambitious internet and software companies in the Benelux. The fund has € 55 million under management and is supported by EIF and ARKimedes-Fund II.\nwww.volta.ventures\n\nContact: Frank Maene, managing partner,  \nfrank@voltaventures.eu, tel. +32 477 20 17 82\n\nAbout Startupbootcamp  \nStartupbootcamp started in 2010 and has grown into an international accelerator program for startups with 13 programs in Amsterdam, Barcelona, Berlin, Eindhoven, Istanbul, London, Miami, New York and Singapore. The mentor and alumni network is spread across more than 50 countries. More information can be found at: www.startupbootcamp.org\n",
        "tags": [
            "vamp",
            "news",
            "press"
        ]
    },
    {
        "uri": "/resources/news/release-0710",
        "title": "Vamp 0.7.10: weight editing and easy A/B-testing with environment variables",
        "content": "\n\nVamp release 0.7.10 introduces a nice addition to the UI where you can easily shift traffic between multiple services based on percentages. You can combine this with our revised way of handling environment variables. \n\nJust launch two exactly the same containers and tweak them using standard ENV variables. A/B-testing Docker containers has never been easier.\n\n!--more-- \n\nEasy A/B-testing\n\nDuring our last sprint we dedicated a lot of time to how we can make simple A/B-testing scenario's easier. Taking in the feedback we get through Github we changed how we handle the scope of environment variables. \n\nYou can now just launch two (or more) of exactly the same containers and just give them different environment variables to tweak and configure the application inside the container. To this, we added a nice little extra in the UI to shift around the traffic based on percentages. \n\nAn extra benefit of changing how we deal with scopes is the option to override variables on different levels if you want to. This allows for a separation of concerns, i.e. have operations override settings provided by developers. Check the updated docs for more scenario's →\n\n Clearer references\n\nIn addition to changing how we deal with scopes we made using references to artifacts more explicit. You now use the reference or ref keyword to include artifacts by name in other artifacts. Previously, this was done using the name keyword, which was confusing. Check the reference docs for more info →\n\n Note: this is a breaking change. Please update blueprints and other artifacts accordingly.\n\nDocker dialect\n\nWhen using Vamp with Docker, you can now add Docker API specific commands to your blueprint. This means you can hook up volumes or use a private registry, or basically do anything the Docker API allows. The example below mount \"/tmp\"\nin every busybox container:\n\n---\nname: busybox\nclusters:\n  busyboxes:\n    services:\n      breed:\n        name: busybox-breed\n        deployable: busybox:latest\n      docker:\n        Volumes:\n          \"/tmp\": ~\nCheck the dialects docs for more info →\n\n Simple deployment validation, or a NOOP run\n\nThe last thing I'd like to point out is that we've added the ?validate_only=true option to de /deployments endpoint. This means you can have Vamp validate whether the blueprint you are about to deploy is actually valid!\nVamp will check if all references to other artifacts are in place and if the blueprint is semantically valid.\nThis works like a \"noop\" (no operation) run and helps with sanity checking complex blueprints. Check the deployment API docs for more info →\n\nRelease notes:  \nhttps://github.com/magneticio/vamp/releases/tag/0.7.10\n\nClosed issues:  \nhttps://github.com/magneticio/vamp/issues?q=is%3Aissue+milestone%3A0.7.10+is%3Aclosed\n\nTim Nolet, CTO",
        "tags": [
            "releases",
            "news"
        ]
    },
    {
        "uri": "/resources/news/release-075",
        "title": "Vamp 0.7.5: Archiving and redesigned environment variable DSL",
        "content": "\nWe just tagged release 0.7.5 and I'm excited about what we achieved this sprint and managed to cram into Vamp 0.7.5. Some of the highlights are below, but you can check the full release note on Github. There are some breaking changes in this release,\nso please update where necessary. We've updated our getting started tutorial to reflect the latest changes.\n\nArchiving of everything\n\nFrom 0.7.5 on, all CRUD actions on Vamp artifacts are fully archived to Vamp Pulse. Put in simple terms: You can track all edits, updates and deletes to your microservices architecture similar to a Git log. By tracking all actions performed on any part of your container driven architecture, we give you the ability to start using that data to track changes, solve problems and spot trends. \n\nIn the screencast below you can see us create, update and delete a breed on Vamp core. After that we get the archive of all those actions from Vamp pulse, together with the orginal YAML version of the breed:\n\n Environment variables DSL in blueprints and breeds\nIn this release we focussed on redesigning our initial way of handling environment variables. Some\nof our concerns were:\n\nhow are variables used and declared in blueprints in breeds?\nhow simple can we make it, without making it too simple?\nhow can we allow inline variable interpolation?\n\nFrom 0.7.5 onwards, we allow straightforward naming of variables and constants. We looked at different\nuse cases, like:\n\n'Hard' set a variable\n\nYou want to \"hard set\" an environment variable, just like doing an export MYVAR=somevalue in a shell. This  variable could be some external dependency you have no direct control over: the endpoint of some service you use that is out of your control. \n\nYou may also want to define a placeholder for a variable of which you do not know the actual value yet, but should be filled in when this breed is used in a blueprint: the following deployment will not run without it and you want Vamp to check that dependency. This placeholder is designated with a ~ character.\n\npre class=\"prettyprint lang-yaml\"\nname: my_breed\ndeployable: repo/container:version\n\nenvironment_variables:\n    MYENVVAR1: somestringvalue   hard set\n    MYENVVAR1: ~                  # placeholder\n/pre\n\nResolve a reference at deploy time\n\nYou might want to resolve a reference and set it as the value for an environment variable. This reference can either be dynamically resolved at deploy-time, like ports and hosts names we don't know till a deployment is done, or be a reference to a hardcoded and published constant from some other part of the blueprint or breed, typically a dependency.\n\nYou would use this to hook up services at runtime based on host/port combinations or to use a hard dependency that never changes but should be provided by another breed. \n\nNotice: you use the $ sign to reference other statements in a blueprint and you use the constants keyword\nto create a list of constant values.\n\nHave a look at this example blueprint. We are describing a frontend service that has a dependency on a backend service. We pick up the actual address of the backend service using references to variables in the blueprint that are filled in at runtime. However, we also want to pick up a value that is set by \"us humans\": the api path, in this case \"/v2/api/customers\".\n\npre class=\"prettyprint lang-yaml\"\nname: my_blueprint\nclusters:\n\n  frontend:\n    breed:\n      name: frontend_service\n    environment_variables:\n         resolves to a host and port at runtime\n        BACKEND_URL: http://$backend.host:$backend.ports.port\n        # resolves to the \"published\" constant value\n        BACKENDURIPATH: $backend.constants.uri_path        \n      dependencies:\n        backend: myotherbreed\n  backend:\n    breed:\n      name: myotherbreed\n    constants:\n      uri_path: /v2/api/customers   \n/pre\n\nFind all downloadable packages at:  \nhttps://bintray.com/magnetic-io/downloads\n\nClosed issues:  \nhttps://github.com/magneticio/vamp/issues?q=is%3Aissue+milestone%3A0.7.5+is%3Aclosed\n\nTim Nolet, CTO",
        "tags": [
            "releases",
            "news"
        ]
    },
    {
        "uri": "/resources/news/release-076",
        "title": "Vamp 0.7.6: introducing the Marathon dialect",
        "content": "\nRelease 0.7.6 is tagged and you can check the full release note on Github. Notice that there are breaking changes in this release so please update where necessary. We've updated our getting started tutorial to reflect the latest changes.\nSome of the highlights are below.\n\nMesosphere Marathon dialect\n\nVamp now allows you to use Marathon specific tags inside Vamp blueprints by using the marathon: tag. We call this a \"dialect\". We will be enabling these dialects for all platforms Vamp supports (i.e. a 'straight' Docker driver and Kubernetes are on the roadmap).  \n\nThis effectively allows you to make full use of the underlying Marathon features like mounting disks, settings commands and providing access to private Docker registries.\n\nLet's look at an example blueprint that pulls an image from private repo, mounts some volumes, sets some labels and gets run with an ad hoc command: all taken care of by Marathon.\n\nname: busy-top:1.0\n\nclusters:\n\n  busyboxes:\n    services:\n      breed:\n        name: busybox\n        deployable: registry.example.com/busybox:latest\n      marathon:\n       cmd: \"top\"      \n       uris:\n         \"https://somehost/somepath/somefilewithdockercredentials\"\n       labels:\n         environment: \"staging\"\n         owner: \"buffy the vamp slayer\"\n       container:  \n         volumes:\n           containerPath: \"/tmp/\"\n             hostPath: \"/tmp/\"\n             mode: \"RW\"\n      scale:\n        cpu: 0.1       \n        memory: 256  \n        instances: 1\n\nNotice the following:\n\nUnder the marathon: tag, we provide the command to run in the container by setting the cmd: tag.\nWe provide a url to some credentials file in the uri array. As described in the Marathon docs this enables Mesos\nto pull from a private registry, in this case registry.example.com where these credentials are set up.\nWe set some labels with some arbitrary metadata.\nWe mount the /tmp to in Read/Write mode.\n\nWe can provide the marathon: tag either on the service level, or the cluster level. Any marathon: tag set on the service level will override the cluster level as it is more specific. However, in 9 out of 10 cases the cluster level makes the most sense. Later, you can also mix dialects so you can prep your blueprint for multiple environments and run times within one description.\n\nWe are convinced this is a very convenient addition and helps users to leverage their container platform while using Vamp's powerful, higher level features.\n\nFind all downloadable packages at:  \nhttps://bintray.com/magnetic-io/downloads\n\nClosed issues:  \nhttps://github.com/magneticio/vamp/issues?q=is%3Aissue+milestone%3A0.7.6+is%3Aclosed\n\nTim Nolet, CTO",
        "tags": [
            "releases",
            "news"
        ]
    },
    {
        "uri": "/resources/news/release-079",
        "title": "Vamp 0.7.9: introducing the UI and CLI.",
        "content": "\n\nWe just pushed release 0.7.9 which is easily the biggest Vamp release to date! With this release we focused on making Vamp easy to install and work with.\n\n!--more-- \n\nFirst of all, we package the first version of our graphical user interface. With this ReactJS based UI, you can create, update and delete Vamp artifacts quickly and dive into the Vamp metrics feed. Quick start →\n\nSecondly, we released the first version of the Vamp command line interface. The CLI allows admins and Devops engineers to do fully automated canary releases and blue/green deployments on Docker and Mesophere's/DCOS Marathon from CI tools like Jenkins or just from bash scripts.  \n\nYou can install the CLI on your Mac using Homebrew, Learn more →\n\na href=\"https://asciinema.org/a/371lzojapwenuoxd7ihta3857?autoplay=1\" target=\"_blank\"img src=\"https://asciinema.org/a/371lzojapwenuoxd7ihta3857.png\" width=\"700\" //a\n\nThirdly, we've updated our website, the configuration and installation documentation and quick start to make using and installing Vamp easier than ever. We support native package managers Centos, RHEL, Debian, Ubuntu. Check the docs →\n\nClosed issues:  \nhttps://github.com/magneticio/vamp/issues?q=is%3Aissue+milestone%3A0.7.9+is%3Aclosed\n\nTim Nolet, CTO",
        "tags": [
            "releases",
            "news"
        ]
    },
    {
        "uri": "/resources/news/release-090",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Vamp 0.9.0 Beta release \n---\n\n9th September 2016\n\nThe Vamp 0.9.0 release is a very important milestone in the lifecycle of Vamp, as we're removing the Alpha label and are moving to Beta! This means that we will do our utmost best to avoid breaking changes in our API's and DSL, focus even more on stabilising and optimising the current feature-set, while of course continuously introducing powerful new features.\n  \nThe Vamp 0.9.0 release is the culmination of three months of hard work by our amazing team! This release incorporates nothing less than 115 issues and I'm very proud of what we've achieved.\n\nSome of the most notable new features are:\n \na brand new opensource UI with much better realtime graphs, sparklines, info, events panel and access to all relevant API objects like breeds, deployments but also new options like gateways and workflows.  \npowerful integrated workflows for automation and optimisation like autoscaling, automated canary-releasing etc. using efficient Javascript-based scripting.\nKubernetes and Rancher support.\nsupport for custom virtual host names in gateways.\nsupport for custom HAProxy templates.\na brand new Vamp Runner helper application for automated integration testing, mocking scenarios and educational purposes.  \n\nAnd, of course, there's a massive amount of improvements, bug fixes and other optimisations.\n\ncomplete list of all the closed issues in this release",
        "tags": []
    },
    {
        "uri": "/resources/news/updates-docs",
        "title": "Updates to docs and tutorial resources",
        "content": "\nHi,  \nJust a short update from HQ. Over the last two weeks we spend some time updating, refreshing and sharpening our documentation \nand some of the resources that go with the docs and tutorials. Here's a list!\n\nUpdated our vamp-docker repo with fresh Dockerfiles\nfor spinning up a Mesosphere cluster and a private Docker registry. All images are also pushed to our Docker hub\n\nAdded and updated documentation on routing & filters, SLA's and the underlying event system. Check it out here and here.\n\nAdded a section on the REST API for Vamp Core and Vamp Router.\n\nWe are slowly but surely moving to Beta! Yay!\n\nTim Nolet, CTO",
        "tags": [
            "news"
        ]
    },
    {
        "uri": "/support",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: support   \n---\nVamp is an open source project, actively developed by Magnetic.io. Vamp is Apache 2.0 licensed.\n\nCommunity support\nIf you have a question about Vamp, please check the Vamp documentation first  - we're always adding new resources, tutorials and examples.\n\n Bug reports\nIf you found a bug, please report it! Create an issue on GitHub and provide as much info as you can, specifically the version of Vamp you are running and the container driver you are using.\n\nGitter\nYou can post questions directly to us on our public Gitter channel  \n\n Twitter\nYou can also follow us on Twitter: @vamp_io\n\nProfessional support\nFor extended support, pricing information on the Vamp Enterprise Edition (EE), professional services or consultancy you can contact us at info@magnetic.io or call +31(0)88 555 33 99\n\n{{ note title=\"What next?\" }}\nTry Vamp\nLearn how Vamp works\nGet your teeth into the Vamp documentation\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/why use vamp/enterprise-edition",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Vamp Enterprise Edition\n---\n\nFor features, pricing and availability of our commercial Vamp Enterprise Edition (EE), please contact info@magnetic.io or call +31(0)88 555 33 99\n",
        "tags": []
    },
    {
        "uri": "/why use vamp/faq",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: frequently asked questions\n---\n\nVamp and your company\nWhy use Vamp?   \nhow do you safeguard business goals after deployment?   \n\nVamp challenges you to rethink how you develop and release microservice based applications.   \n\n     \nWho is already using Vamp and how?\n\nWhat is canary testing/releasing?\n\nWill Vamp make that much difference to my company?  \nVamp brings clarity and flexibility to microservice upgrades and deployment monitoring. \n\n---------\n Common concerns\nWill we need to make big changes to start using Vamp?  \n\nAre containers secure?\nWhat about databases?\nDo you provide support?\n\n--------\nPractical info\nCan I use Vamp together with… [insert package name]   \nVamp is platform agnostic so can run on top of all common PaaS solutions. \n\ncredibility style bar - what can we work with. lots of logos\n\nWhat does Vamp cost? (how do you make money)\nWhere can I try Vamp?  \nFollow the demo tutorial this will walk you through the main features of Vamp on your laptop. It should be enough to gain an initial introduction and get you started thinking about how Vamp can help your company.  \nIf you want to know more, or have specific questions, contact us to arrange a demo.\n\n---------------------------\n About us\nWho are you?\nWhat will the future bring? What is your vision?",
        "tags": []
    },
    {
        "uri": "/why use vamp/feature-list",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Feature list\n---\n\nVamp 0.9.0 (Beta release) includes: \n\nContainer-scheduler agnostic API\nPercentage and condition based programmable routing\nYAML based configuration blueprints with support for dependencies, clusters and environment variables\nGraphical UI and dashboard\nIntegrated javascript-based workflow system \nMetric-driven autoscaling, canary releasing and other optimisation and automation patterns\nAutomatic loadbalancing for autoscaled services\nAPI gateway routing features like conditional rewrites\nCLI for integration with common CI/CD pipelines\nOpen source (Apache 2.0)\nEvent API and server-side events (SSE) stream\nMulti-level metric aggregation\nPort-based, virtual host names or external service (consul etc) based service discovery support\nLightweight design to run in high-available mission-critical architectures\nIntegrates with ELK stack (Elastic Search, Logstash, Kibana) for custom Kibana dashboards\nVamp Runner provides automated integration and workflows testing \n\nDid you know?\n\nVamp is platform agnostic\nVamp is functionality agnostic, but functions well in an API centric, event driven and stateless environment. \nVamp is not a strict container platform, but uses the power of container platforms under the hood.\nVamp is written in Scala, Go and ReactJS \nVamp includes clear SLA management and service level enforcement out of the box\nVamp is an open source project, actively developed by Magnetic.io\n\n{{ note title=\"What next?\" }}\nTry Vamp\nUse cases: Vamp solutions to practical problems\nWhat Vamp offers compared to other tools and services\nHow Vamp works\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/why use vamp/get-started",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Get started\n---\n\nTry Vamp\nTo make the most of your 'my first Vamp' experience, we suggest you start by installing our single, all-in-one Vamp Docker Hello World package. This will set up everything you need to play around with Vamp on a local or remote dev machine - the container package includes Mesos/Marathon and Elastic Stack (ELK), as well as all the necessary Vamp components. We have some nice tutorials to get a feel for the supernatural powers of Vamp.\n\n Install a production-grade Vamp setup\nOf course our Hello World package is no production-grade setup. We suggest your next step should be to understand the Vamp architecture and then find the Vamp version for your favorite container scheduler. We support most common container schedulers, so you should be able to find one to your liking in our installation docs. If you're still not sure which container scheduler to work with, our 'what to choose' guide can help you make an informed decision.\n\nFine tune and integrate\nAfter you've successfully installed a production-grade Vamp on your preferred container-cluster manager/scheduler (if you need help here, find us in our public Gitter channel), it's time to either dive into the ways you can use Vamp or investigate how you can configure and fine-tune Vamp to match your specific requirements. It might also be interesting to integrate Vamp into your CI pipeline to create a CD pipeline with Vamp's canary-releasing features. You can check out our CLI and REST API documentation for integrations.\n\n Get your teeth into the fun stuff!\nAt this point you've become a real Vamp guru. The next step could be to start playing around with our Vamp Runner tool to investigate typical recipes, such as automated canary-releasing, auto-scaling and more. You can use the JavaScript-based workflows in the recipes as a reference to create your own recipes and workflows. Once you've created some cool workflows and recipes we would of course like to hear from you!\n\n{{ note title=\"What next?\" }}\nTry the Vamp Docker Hello World package and tutorials\nRead about the Vamp architecture\nCheck the installation docs\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/why use vamp/index",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Why use Vamp?\n---\n\nWe recognise the pain and risk involved with delivering microservice applications.  We've been there too - facing downtime and unexpected issues while transitioning from one release to the next. \nIn microservice architectures, these concerns can quickly multiply. It's all too easy to get stuck dealing with the added complexities and miss out on the potential benefits. \n\nWhat is Vamp?\n\nVamp is an open source, self-hosted platform for managing (micro)service oriented architectures that rely on container technology. Vamp provides a DSL to describe services, their dependencies and required runtime environments in blueprints and a runtime/execution engine to deploy these blueprints (similar to AWS Cloudformation). Planned deployments and running services can be managed from your choice of Vamp interface - graphical UI, command line interface or RESTful API. \n\nAfter deployment, Vamp workflows continue to monitor running applications and can act automatically based on defined SLAs.  You can use Vamp to orchestrate complex deployment patterns, such as architecture level A/B testing and canary releases. Vamp will take care of all the heavy lifting, such as route updates, metrics collection and service discovery.\n\n Vamp facts\n\nVamp is platform agnostic\nVamp is functionality agnostic, but functions well in an API centric, event driven and stateless environment. \nVamp is not a strict container platform, but uses the power of container platforms under the hood.\nVamp is written in Scala, Go and ReactJS \nVamp includes clear SLA management and service level enforcement out of the box\nVamp is an open source project, actively developed by Magnetic.io\n\n{{ note title=\"What next?\" }}\nTry Vamp\nUse cases: Vamp solutions to practical problems\nWhat Vamp offers compared to other tools and services\nHow Vamp works\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/why use vamp/see-vamp-in-action",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: See Vamp in action\n---\n\n",
        "tags": []
    },
    {
        "uri": "/why use vamp/use cases/create-responsive-website",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Canary test and release a responsive frontend\n---\n\n“We need to upgrade our website frontend to make it responsive”\n  \nDeveloping a responsive web frontend is often a major undertaking, requiring a large investment of hours and extensive testing. Until you go live, it's difficult to predict how the upgrade will be received by users - will it actually improve important metrics, will it work on all browser, devices and resolutions, etc.?   \n\nBut why develop this new responsive frontend in one go, having to go for the dreaded and risky big-bang release? Using Vamp you can apply a canary release to introduce the new frontend to a selected cohort of users, browsers and/or devices. This would require a minimal investment of development and delivering real usage data:\n\nStart small: Build the new frontend for only one specific browser/resolution first to measure effectiveness. Vamp can deploy the new responsive frontend and route a percentage of supported users with this specific browser and screen-resolution there. All other users will continue to see the old version of your website.\nOptimise: With the new responsive frontend in the hands of real users with this specific browser/resolution, you can measure actual data and optimise accordingly without negatively affecting the majority of your users.\nScale up: Once you are satisified with the performance of the new frontend, you can use Vamp to scale up the release, developing and canary releasing one browser/resolution at a time. Of course other cohort combinations are also possible, Vamp is open and supports all HAproxy ACL rules.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to resolve client-side incompatibilities after an upgrade\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/why use vamp/use cases/index",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: use cases\n---\n\nThe integrated deployment, routing and workflow features of Vamp support a broad range of scenarios and industry verticals. We specifically see powerful use cases in the areas of testing in production, migrating to microservices, and realtime system optimisation. In this section we describe specific scenarios and how Vamp can effectively solve these.\n\nTesting in production \nUse canary testing and releasing to introduce a responsive web frontend. Read more ...\nResolve client-side incompatibilities after an upgrade. Read more ...\nA/B test architectural changes in production. Read more ...\n\n Migrating to microservices\nMove from VM based monoliths to modern microservices. Read more ...\n\nRealtime system optimisation\n\nWhat would happen if...... Simulate and test auto scaling behaviour. Read more ...\nSelf-healing and self-optimising. Read more ...\n\nWe're always interested to hear specific use-cases. If you have one to share, send us an email at info@magnetic.io",
        "tags": []
    },
    {
        "uri": "/why use vamp/use cases/modernise-architecture",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Test and modernise architecture\n---\n\n\"We want to switch to a NoSQL database for our microservices, but don't know which solution will run best for our purposes\"\n\nWith multiple NoSQL database options available, it's hard to know which is the best fit for your specific circumstances. You can try things out in a test lab, but the real test comes when you go live with production load.\n\nWhy guess? Using Vamp you could A/B test different versions of your services with different NoSQL backends, in production, and then use real data to make an informed and data-driven decision.   \n\nDeploy two versions: Vamp can deploy multiple versions of your architecture, each with a different database solution (or other configuration settings), then distribute incoming traffic across each.\nStress test: Use the metrics reported by Vamp to measure which option performs best in production.\nKeep the best performing option: Once you have made your decision, Vamp can route all traffic to your chosen architecture. Services from the alternative options will be drained to ensure customer experience is not impacted by the test.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to simulate and test scaling behaviour\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/why use vamp/use cases/refactor-monolithic-to-microsystems",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Move from monoliths and VM's to microservices\n---\n\n“We want to move to microservices, but we can’t upgrade all components at once and want to do a gradual migration”\n  \nRefactoring a monolithic application to a microservice architecture is a major project. A big bang style re-write to upgrade all components at once is a risky approach and requires a large investment in development, testing and refactoring.\n\nWhy not work incrementally? Using Vamp's routing you could introduce new services for specific application tiers like the frontend or business logic layers, and move traffic with specific conditions to these new services. These services in turn can connect to your legacy systems again, using Vamp's proxying. A typical example would be introducing an angular based frontend or node.js based API microservice. You can send 2% of your incoming traffic to this new microservice frontend, which in turn connects with the legacy backend system. This way you can test your new microservices in a small and controlled way and avoid a big bang release. You can introduce new services one by one, test them in production and increase traffic until you migrated your entire application from a monolith to microservices.\n\nStart small: You can build e.g. one new frontend component. Vamp will deploy this to run alongside the legacy monolithic system.\nActivate smart routing: Vamp can route traffic behind the scenes, so a small percentage of visitors is sent to the new frontend service, while the new frontend is routed by Vamp to the legacy backend. You can continue transferring components from the legacy monolithic system to new microservices and Vamp can adapt the routing as you go.\nRemove legacy components: Once all services have been transferred from the legacy monolith, you can start removing components from the legacy system.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to test and modernise architecture\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/why use vamp/use cases/resolve-incompatibilities-after-upgrade",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Resolve client-side incompatibilities after an upgrade\n---\n\n“We upgraded our website self-management portal, but our biggest client is running an unsupported old browser version”\n  \nLeaving an important client unable to access your services after a major upgrade is a big and potentially costly problem. The traditional response would be to rollback the upgrade asap - if that's even possible.  \n\nWhy rollback? Using Vamp's smart conditional routing you could send specific clients or browser-versions to an older version of your portal while others can enjoy the benefits of your new upgraded portal. Because Vamp supports SLA based autoscaling for (Docker) containers, you can deploy the old version on the same infrastructure as the new version is running on. This also avoids having to provision costly over dimensioned DTAP environments for only a small user-base, leveraging your existing infrastructure efficiently.\n\nRe-deploy: Vamp can (re)deploy a (containerised) compatible version of your portal to run side-by-side with the upgraded version.\nActivate smart routing: Vamp can route all users with e.g. a specific IP, browser or location to a compatible version of the portal. Other clients will continue to see the new upgraded portal.\nResolve the incompatibility: Once the client upgrades to a compatible browser Vamp can automatically route them to the new portal version.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to move from monoliths and VMs to microservices\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/why use vamp/use cases/self-healing-and-self-optimising",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Self-healing and self-optimising\n---\n\n\"Our website traffic can be unpredictable, it's hard to plan and dimension the exact resources we're going to need to run within SLA's\"\n\nWhy overdimension your whole system? Using Vamp you can auto-scale individual services based on clearly defined SLAs (Service Level Agreements). It's also easy to create advanced workflows for up and down scaling, based on your application or business specific requirements. Vamp can also make sure that unhealthy and failing services are corrected based on clearly defined metrics and treshholds.\n\nSet SLAs: You can define SLA metrics, tresholds and escalation workflows. You can do this in Vamp YAML blueprints, modify our packaged workflows, or create your own workflow scripts for advanced use-cases.\nOptimise: Vamp workflows can automatically optimise your running system based on metrics that are relevant to your application or services.\nSleep easy: Vamp will track troughs and spikes in activity and automatically scale services up and down to match your SLAs. All scaling events will be logged. Unhealthy services can be healed by Vamp.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp for service discovery\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/why use vamp/use cases/service-discovery",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Service discovery\n---\n\nVamp uses a service discovery pattern called server-side service discovery. This pattern allows service discovery without the need to change your code or run any other service-discovery daemon or agent. In addition to service discovery, Vamp also functions as a service registry. We recognise the following benefits of this pattern:\n\nNo code injection needed\nNo extra libraries or agents needed\nPlatform/language agnostic: it’s just HTTP\nEasy integration using ENV variables\n\nVamp doesn't need point-to-point wiring. Vamp uses environment variables that resolve to service endpoints Vamp automatically sets up and exposes. Even though Vamp provides this type of service discovery, it does not put any constraint on other possible solutions. For instance services can use their own approach specific approach using service registry, self-registration etc. Vamp can also integrate with common service discovery solutions like Consul and read from these to setup the required routing automatically.\n\nSmartStack\nVamp can automatically deploy a VampGatewayAgent(VGA)+HAproxy on every node of your container cluster. This creates a so-called Layer 7 intra service network mesh which enables you to create a \"SmartStack\", an automated service discovery and registration framework originally coined and developed by AirBnB. More on the history and advantages of the SmartStack approach can be read here (nerds.airbnb.com - SmartStack: Service Discovery in the Cloud).\n\n{{ note title=\"What next?\" }}\nRead more about Vamp service discovery\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}\n",
        "tags": []
    },
    {
        "uri": "/why use vamp/use cases/simulate-and-test-scaling-behaviour",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Simulate and test scaling behaviour\n---\n\n\"How would our system react if... the number of users increased x10 ... the response time of a service increased with 20 seconds ... an entire tier of our application would be killed ...\"  \nYour company might dream of overnight success, but what if it actually happened? Stress tests rarely cater to extreme real world circumstances and usage patterns, and are often done on systems that are not identical to production environments. It's not uncommon the bottleneck sits in the system generating the load itself, so it's difficult to predict how your microservices would actually scale or to know if your planned responses will really help.\n\nWhy not find out for sure? Using Vamp you can test your services and applications against difficult to predict or simulate situations, mocking all kinds of metrics, and then validate and optimise the workflows that handle the responses, like for example auto up and down scaling. With the same workflows as would be running in production, on the same infrastructure, with the same settings.\n\nMock required load: Vamp can simulate (mock) high-stress situations for any kind of metric your system needs to respond to, without actually having to generate real traffic.\nOptimise: You can optimise your resource allocation and autoscaling configurations based on real validated behaviour under stress.\nIterate until you're certain: Vamp can repeat the tests until you're confident with the outcome. Then you can use the same scaling and optimising workflows in production.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp for self healing and self optimising\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}",
        "tags": []
    },
    {
        "uri": "/why use vamp/vamp-compared-to/frameworks-and-tools",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Frameworks and tools\n---\n\nVamp compared to CI/CD tools\nSpinnaker, Jenkins, Wercker, Travis, Bamboo    \nVamp closes the loop between the development and operations elements of a CI/CD pipeline by enabling the controlled introduction of a deployable into production (canary-test and canary-release) and feeding back runtime technical and business metrics to power automated optimisation workflows (such as autoscalers). Vamp integrates with CI/CD tools like Travis, Jenkins or Wercker to canary-release and scale the built deployables they provide. The initial deployment setup is defined in a YAML blueprint (e.g. deployable details, required resources, routing filters) and is typically provided by the CI tool as a template to the Vamp API. Vamp will then run, canary-release, monitor and scale the deployment based on the filters and conditions specified in the blueprint.\n\n Vamp compared to feature toggle frameworks\nLaunchDarkly, Togglz, Petri  \nFeature toggle frameworks use code level feature toggles to conditionally test new functionality in an application or service. Tools such as LaunchDarkly and Togglz work with these toggles to enable, for example, A/B testing and canary functionality. While there are many cases for using feature toggles, there are also times when feature toggling isn't the smartest choice.\nVamp allows controlled testing of new features without the need to adjust your code. To achieve this, Vamp controls traffic routing towards and between your applications and services based on blueprint descriptions of individual (micro)services and their dependencies. This makes sense on an application code level, offering increased security and reduced technical debt compared to maintaining toggles in your code, and provides a mature alternative for cases when feature toggles are not the appropriate choice.\n\nVamp compared to configuration management and provisioning tools\nPuppet, Ansible, Chef, Terraform    \nThe responsbilities of configuration management and infrastructure provisioning tools are often stretched to cover container deployment features. These tools were not intended for handling container deployments or for the dynamic management and routing traffic over these containers. Vamp has been designed and developed from the ground up specifically to fit these use cases.  \n\n Vamp compared to custom built solutions\nBuilding and maintaining a scalable and robust enterprise-grade system for canary-testing and releasing is not trivial. Vamp delivers programmable routing and automatic load balancing, deployment orchestration and workflows, as well as a powerful event system, REST API, graphical UI, integration testing tools and a CLI.  \n\nVamp compared to A/B and MVT testing tools\nOptimizely, VisualWebsiteOptimizer, Google Analytics, Planout  \nVamp enables canary testing versions of applications, effectively providing A/B and MVT testing of applications and services by deploying two or more versions of an application or service and dividing incoming traffic between the running versions. Vamp doesn't have a built-in analytics engine though, so the analysing of the relevant metrics needs to be done with a specific Vamp workflow or an external analytics engine. Results can be fed back to Vamp to automatically update routing rules and deployments to push a winning version to a full production release. Because of the flexible programmable routing and use of environment variables, Vamp can be used to canary test almost everything, from content and business logic to configuration settings and architectural changes.  \n\n Vamp compared to DevOps tools\nDeis, Flynn, Dokku  \nVamp has no ambition to provide a Heroku-like environment for containers. Vamp integrates programmable routing and load balancing, container deployments and orchestration to enable canary testing and canary releasing features. Vamp also adds metrics-driven workflows for auto-scaling and other optimisations. Vamp sees business as a first class citizen in DevOps teams, providing a graphical UI and tools for non-technical roles.   \n\n{{ note title=\"What next?\" }}\nTry Vamp\nUse cases -  some Vamp solutions to practical problems\nFind out how Vamp works\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/why use vamp/vamp-compared-to/paas-and-container-systems",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: PaaS and container systems\n---\n\nVamp compared to container schedulers and container clouds (CPaaS)\nDocker Swarm, DC/OS, Mesos/Marathon, Kubernetes, Nomad, Rancher, AWS ECS, Azure CS, Mantl, Apollo  \nContainer cluster managers and schedulers like Marathon, DC/OS, Kubernetes, Nomad or Docker Swarm provide great features to run containers in clustered setups. What they don't provide are features to manage the lifecycle of a microservices or container based system. How to do continuous delivery, how to gradually introduce and upgrade versions in a controlled and risk-free way, how to aggregate metrics and how to use these metrics to optimise and scale your running system. Vamp adds these features on top of well-used container schedulers by dynamically managing routing and load balancing, deployment automation and metric driven workflows. Vamp also adds handy features like dependencies, ordering of deployments and resource management.\n\n Vamp compared to PaaS systems\nCloud foundry, OpenStack, IBM Bluemix, Openshift  \nVamp adds an experimentation layer to PaaS infrastructures by providing canary-releasing features that integrate with common PaaS proxies like HAProxy. For continuous delivery and auto-scaling features, Vamp integrates with common container-schedulers included in PaaS systems, like Kubernetes in Openshift V3.   \n\n{{ note title=\"What next?\" }}\nRead about Vamp compared to common frameworks and tools\nTry Vamp\nFind out how Vamp works\n{{ /note }}\n\n",
        "tags": []
    },
    {
        "uri": "/why use vamp/vamp-compared-to/proxies-and-load-balancers",
        "content": "---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Proxies and load balancers\n---\n\nVamp compared to software based proxies and load balancers\nHAproxy, NGINX, linkerd, Traefik   \nVamp adds programmable routing (percentage and condition based) and load balancing to the battle-tested HAProxy proxy, as well as a REST API, graphical UI and CLI.  This means you can use Vamp together with all common container-schedulers to provide continuous delivery and auto-scaling features using automatic load balancing and clustering of scaled out container instances. By default Vamp is packaged with HAProxy, but you could also integrate the Vamp magic with other programmable proxies such as NGINX, linkerd or Traefik.\n\n{{ note title=\"What next?\" }}\nRead about Vamp compared to PaaS and container systems\nTry Vamp\nFind out how Vamp works\n{{ /note }}\n\n",
        "tags": []
    }
]