[{"path":"/about","date":"2016-09-13T09:00:00+00:00","title":"about us","content":"Vamp is developed by Magnetic.io in the heart of Amsterdam.  \nWe’re dedicated to helping companies of all sizes increase efficiency, save time and reduce costs. We provide powerful and easy-to-use solutions to optimise systems.\n\nVision\nTime is precious.  \nWe believe people should focus their efforts on the things they excel at and that matter to them.  \nLet computers do the repeatable and automatable tasks - they’re far more suited to that than us.\n\n Community\nWe encourage anyone to join the Vamp community and pitch in with pull requests, bug reports etc. to make Vamp even more awesome.\n","id":0},{"path":"/contact","date":"2016-09-13T09:00:00+00:00","title":"contact","content":"\nVamp is being developed by Magnetic.io in the heart of Amsterdam.\n\nOur Amsterdam offices:\n\nSint Antoniesbreestraat 16  \n1011 HB Amsterdam  \nThe Netherlands  \n+31(0)88 555 33 99  \ninfo@magnetic.io\n\nProfessional services and consultancy\nWe can provide professional services and consultancy around the implementation and use of Vamp. Send us an email on info@magnetic.io or call +31(0)88 555 33 99.\n\n Vamp Enterprise Edition (EE)\nWe also provide a commercial Enterprise Edition of Vamp with features specifically tuned to enterprise usage. Contact us to get more information.\n\nSupport\nCheck our support page for SLA's and other forms of support.\n\n Community\nJoin the Vamp community to make Vamp even better.\n","id":1},{"path":"/documentation/api/api-reference","date":"2016-09-13T09:00:00+00:00","title":"API Reference","content":"\nThis page gives full details of all available API calls. See using the Vamp API for details on pagination, json and yaml content types and effective use of the API.\n\nBlueprints\n\n List blueprints\n\nLists all blueprints without any pagination or filtering.\n\n    GET /api/v1/blueprints\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. 400 Bad Request in case some breeds are not yet fully defined.\n| only_references   | true or false     | false            | all breeds will be replaced with their references.\n\nGet a single blueprint\n\nLists all details for one specific blueprint.\n\n    GET /api/v1/blueprints/{blueprint_name}\n\n| parameter         | options           | default          | description       |\n| ----------------- |:-----------------:|:----------------:| -----------------:|\n| expand_references | true or false     | false            | all breed references will be replaced (recursively) with full breed definitions. 400 Bad Request in case some breeds are not yet fully defined.\n| only_references   | true or false     | false            | all breeds will be replaced with their references.\n\n Create blueprint\n\nCreates a new blueprint. Accepts JSON or YAML formatted blueprints. Set the Content-Type request header to application/json or application/x-yaml accordingly.\n\n    POST /api/v1/blueprint\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 201 Created if the blueprint is valid.This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON.     \n\nUpdate a blueprint\n\nUpdates the content of a specific blueprint.\n\n    PUT /api/v1/blueprints/{blueprint_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | validates the blueprint and returns a 200 OK if the blueprint is valid. This can be used together with the header Accept: application/x-yaml to return the result in YAML format instead of the default JSON. \n\n Delete a blueprint\n\nDeletes a blueprint.        \n\n    DELETE /api/v1/blueprints/{blueprint_name}\n\n| parameter     | options           | default          | description      |\n| ------------- |:-----------------:|:----------------:| ----------------:|\n| validate_only | true or false     | false            | returns a 204 No Content without actual delete of the blueprint.\n\n---------","id":2},{"path":"/documentation/api/using-the-api","date":"2016-09-13T09:00:00+00:00","title":"Using the Vamp API","content":"Vamp has one REST API. This page explains how to specify pagination, and json and yaml content types, and how to effectively use the Vamp REST API.\n\nSee also\nFull details of all available API calls\n\n Content types\n\nVamp requests can be in YAML format or JSON format. Set the Content-Type request header to application/x-yaml or application/json accordingly.\nVamp responses can be in YAML format or JSON format. Set the Accept request header to application/x-yaml or application/json accordingly.\n\nPagination\n\nVamp API endpoints support pagination with the following scheme:\n\nRequest parameters page (starting from 1, not 0) and per_page (by default 30) e.g:\n\nGET http://vamp:8080/api/v1/breeds?page=5&per_page=20\n\nResponse headers X-Total-Count giving the total amount of items (e.g. 349673) and a Link header for easy traversing, e.g.\nX-Total-Count: 5522\nLink: \n  http://vamp:8080/api/v1/events/get?page=1&per_page=5; rel=first, \n  http://vamp:8080/api/v1/events/get?page=1&per_page=5; rel=prev, \n  http://vamp:8080/api/v1/events/get?page=2&per_page=5; rel=next, \n  http://vamp:8080/api/v1/events/get?page=19&per_page=5; rel=last\n\nSee Github's implementation for more info.\n\n Return codes\n\nCreate & Delete operations are idempotent: sending the second request with the same content will not result to an error response (4xx).\nAn update will fail (4xx) if a resource does not exist.\nA successful create operation has status code 201 Created and the response body contains the created resource.\nA successful update operation has status code 200 OK or 202 Accepted and the response body contains the updated resource.\nA successful delete operation has status code 204 No Content or 202 Accepted with an empty response body.\n\nSending multiple artifacts (documents) - POST, PUT and DELETE\n \nIt is possible to send YAML document containing more than 1 artifact definition:\n\nGET /api/v1\n\nSupported methods are POST, PUT and DELETE. Example:\n\n","id":3},{"path":"/documentation/cli/cli-reference","date":"2016-09-13T09:00:00+00:00","title":"CLI reference","content":"\nThe VAMP CLI supports the following commands:  \ncreate, deploy, generate, help, info, inspect, list, merge, remove, undeploy, update, version  \nSee using the Vamp CLI for details on installation, configuration and effective use of the CLI\n\nFor details about a specific command, use vamp COMMAND --help\n\n----------","id":4},{"path":"/documentation/cli/using-the-cli","date":"2016-09-13T09:00:00+00:00","title":"Using the Vamp CLI","content":"\nVamp's command line interface (CLI) can be used to perform basic actions against the Vamp API. The CLI was\nprimarily developed to work in continuous delivery situations. In these setups, the CLI takes care of automating (canary) releasing new artifacts to Vamp deployments and clusters.\n\nSee also\nFull list of available CLI commands\n\n Installation\n\nCheck run vamp for details on how to install the Vamp CLI on your platform. \n\nConfiguration\n\nAfter installation, set Vamp's host location. This location can be specified as a command line option (--host)\n\nvamp list breeds --host=http://192.168.59.103:8080\n\n...or via the environment variable VAMP_HOST\nexport VAMP_HOST=http://192.168.59.103:8080\n\n Simple commands\n\nThe basic commands of the CLI, like list, allow you to do exactly what you would expect:\n\n vamp list breeds\nNAME                     DEPLOYABLE\ncatalog                  docker://zutherb/catalog-frontend\ncheckout                 docker://zutherb/monolithic-shop\nproduct                  docker://zutherb/product-service\nnavigation               docker://magneticio/navigation-service:latest\ncart                     docker://zutherb/cart-service\nredis                    docker://redis:latest\nmongodb                  docker://mongo:latest\nmonarch_front:0.1        docker://magneticio/monarch:0.1\nmonarch_front:0.2        docker://magneticio/monarch:0.2\nmonarch_backend:0.3      docker://magneticio/monarch:0.3\n\n vamp list deployments\nNAME                                    CLUSTERS\n1272c91b-ba29-4ad1-8d09-33cbaa8f6ac2    frontend, backend\n\nCI and chaining\n\nIn more complex continuous integration situations you can use the CLI with the --stdin flag to chain a bunch of commands together. You could for instance:\n\nget an \"old\" version of a breed with inspect\ngenerate a new breed based on the previous one, while inserting a new deployable\ncreate the breed in the backend\n\nvamp inspect breed frontend:${OLD} | \\\nvamp generate breed --deployable mycompany/frontend:${NEW} frontend:${NEW} --stdin | \\\nvamp create breed --stdin\n\nOnce you have the new breed stored, you can insert it into a running deployment at the right position, i.e:\n\nget a blueprint from a running deployment with inspect and --as_blueprint\ngenerate a new blueprint with generate while inserting a new breed\ndeploying the result with deploy\n\nvamp inspect deployment $DEPLOYMENT --as_blueprint | \\\nvamp generate blueprint --cluster frontend --breed frontend:${NEW} --stdin | \\\nvamp deploy --deployment $DEPLOYMENT --stdin\n\n------","id":5},{"path":"/documentation/how vamp works/architecture-and-components","date":"2016-09-13T09:00:00+00:00","title":"Architecture and components","content":"\nArchitecture \nVamp and the Vamp Gateway Agent require specific elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation. There is no set architecture required for running Vamp and every use case or specific combination of tools and platforms can have its own set up.\n\n Example topology\nThe below diagram should be used more as an overview than required architecture. For example, Mesos/Marathon stack is included even though it is not a hard dependency.\n\nVamp components\n\nVamp consists of server- and client-side components that work together with elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation.\n\n Vamp UI  \nThe Vamp UI is a graphical web interface for managing Vamp in a web browser. Packaged with Vamp.\n\nVamp CLI  \nThe Vamp CLI is a command line interface for managing Vamp and providing integration with (shell) scripts.\n\n Vamp  \nVamp is the main API endpoint, business logic and service coordinator. Vamp talks to the configured container manager (Docker, Marathon, Kubernetes etc.) and synchronizes it with Vamp Gateway Agent (VGA)  via ZooKeeper, etcd or Consul. Vamp uses Elasticsearch for artifact persistence and to store events (e.g. changes in deployments). Typically, there should be one Vamp instance and one or more VGA instances.  \n\nVamp workflows\nVamp workflows are small applications (for example using JavaScript or your own containers) that automate changes of the running system, and its deployments and gateways. We have included a few workflows out of the box, such as health and metrics, which are used by the Vamp UI to report system status and to enable autoscaling and self-healing.\n\n Vamp Gateway Agent (VGA)  \nVamp Gateway Agent (VGA) reads the HAProxy configuration from ZooKeeper, etcd or Consul and reloads HAProxy on each configuration change with as close to zero client request interruptions as possible. Typically, there should be one Vamp instance and one or more VGA instances.     \nLogs from HAProxy are read over socket and pushed to Logstash over UDP.  VGA will handle and recover from ZooKeeper, etcd, Consul and Logstash outages without interrupting the HAProxy process and client requests.  ","id":6},{"path":"/documentation/how vamp works/events-and-metrics","date":"2016-09-13T09:00:00+00:00","title":"Events and metrics","content":"HAProxy (VGA) generates logs and makes them accessible via open socket - check the HAProxy configuration (github.com/magneticio - haproxy.cfg) of log.\n\nVGA listens on log socket and any new messages are forwarded to the Logstash instance.\nLog format is configurable in Vamp configuration vamp.gateway-driver.haproxy (github.com/magneticio - reference.conf).\n\nFor an effective feedback loop, HTTP/TCP logs should be collected, stored and analyzed\nCollection and storing is done by a combination of HAProxy, VGA and Logstash setup\nLogs can be stored in Elasticsearch and later analysed and visualised by Kibana.\n\nLogstash\n\n{{ note title=\"Note!\" }}\nLogstash is listening on UDP port, but in principle any other listener can receive logs forwarded by VGA. \nDifferent VGAs can use different Logstash instances.\n{{ /note }}\n\nIn general, for each HTTP/TCP request to HAProxy, several log messages are created (e.g. for gateway, service and instance level).\nUsing one of the simplest Logstash configurations should be sufficient for dozens of requests per second - or even more.\nThis also depends on whether ELK is used for custom application/service logs etc.\n\nYou can transform logs to plain JSON, which can be parsed easily later on (e.g. for Kibana visualization), with this Logstash configuration together with the default vamp.gateway-driver.haproxy log format (github.com/magneticio - logstash.conf).\n\n{{ tip title=\"Examples\" }}\n\nDifferent Logstash/Elasticsearch setups: (elastic.co - Deploying and Scaling Logstash).\nLogstash command line parameter (/github.com/magneticio - Logstash section).\n{{ /tip }}\n\n Kibana\n   \n  Vamp can be configured to create Kibana searches, visualisations and dashboards automatically with the vamp.gateway-driver.kibana.enabled configuration parameter.\n  Vamp will do this by inserting ES documents to the Kibana index, so only the URL to access ES is needed (by default reusing the same as for persistence).  \n\n{{ note title=\"What next?\" }}\n{{ /note }}","id":7},{"path":"/documentation/how vamp works/persistence-key-value-store","date":"2016-09-13T09:00:00+00:00","title":"Persistence and key-value (KV) store","content":"\nPersistence \nVamp uses Elasticsearch (ES) as main persistence (e.g. for artifacts and events). \nVamp is not demanding in ES resources, so a small ES installation is sufficient for Vamp indexes (index names are configurable). Vamp can also use an existing ES cluster.\n\n Key-value (KV) store\nVamp depends on a key-value (KV) store for non-direct communication between Vamp and instances of the Vamp Gateway Agent (VGA). There is no direct connection between Vamp and the VGA instances - all communication is done by managing specific KV in the store.  Currently we support:\n\nZooKeeper (apache.org - zookeeper).  \nVamp and VGAs can use an existing DC/OS ZooKeeper cluster.\netcd (coreos.com - etcd)  \nVamp and VGAs can use an existing Kubernetes etcd cluster.\nConsul (consul.io)\n\n{{ note title=\"What next?\" }}\nRead about routing and load balancing\n{{ /note }}\n\n","id":8},{"path":"/documentation/how vamp works/requirements","date":"2016-09-13T09:00:00+00:00","title":"Requirements","content":"\nVamp's components work together with elements in your architecture to handle orchetstration, routing, persistence and metrics aggregation. To achieve this, Vamp requires access to a container scheduler, key value store, Elastic Search and HAproxy.\n\nContainer scheduler  (orchestration)\nVamp talks directly to your choice of container scheduler. Currently we support DC/OS, Kubernetes and Rancher.\n\n Key value store\nVamp depends on a key-value (KV) store for non-direct communication between Vamp and instances of the Vamp Gateway Agent (VGA). There is no direct connection between Vamp and the VGA instances, all communication is done by managing specific KV in the store.  When Vamp needs to update the HAProxy configuration (e.g. when a new service has been deployed) Vamp will generate the new configuration and store it in the KV store. The VGAs read specific valuea and reload HAProxy instances accordingly.\nCurrently we support:\n\nZooKeeper (apache.org - zookeeper).  \nVamp and VGAs can use an existing DC/OS ZooKeeper cluster.\netcd (coreos.com - etcd)  \nVamp and VGAs can use an existing Kubernetes etcd cluster.\nConsul (consul.io)\n\nElastic Search (persistence and metrics)\nVamp uses Elastic Search (ES) for persistence (e.g. for artifacts and events) and for aggregating the metrics used by Vamp workflows and the Vamp UI. As Vamp is not demanding in ES resources, it can comfortably work with an existing ES cluster.  \nCurrently we use Logstash to format and send data to Elastic Search, but you could also opt for an alternative solution.\n\n HAproxy  (routing)\nEach Vamp Gateway Agent (VGA) requires its own instance of HAproxy. This is a hard requirement, so to keep things simple we provide a Docker container with both Vamp Gateway Agent (VGA) and HAproxy (hub.docker.com - magneticio/vamp-gateway-agent).  \n\n{{ note title=\"What next?\" }}\nFind out about using Vamp\n{{ /note }}\n\n","id":9},{"path":"/documentation/how vamp works/routing-and-load-balancing","date":"2016-09-13T09:00:00+00:00","title":"Routing and load balancing","content":"\n{{ note title=\"What next?\" }}\nRead about how Vamp works with events and metrics\n{{ /note }}\n","id":10},{"path":"/documentation/how vamp works/service-discovery","date":"2016-09-13T09:00:00+00:00","title":"Service discovery","content":"\nHow does Vamp do service discovery?\n\nVamp uses a service discovery pattern called server-side service discovery. This pattern allows service discovery without the need to change your code or run any other daemon or agent. The provided link explains the general pro’s and cons in good detail. In addition to service discovery, Vamp also functions as a service registry.\n\nFor Vamp, we recognise the following benefits of this pattern:\n\nNo code injection needed.\nNo extra libraries or agents needed.\nplatform/language agnostic: it’s just HTTP.\nEasy integration using ENV variables.\n\n Create and publish a service\n\n{{ note title=\"Note\" }}\nServices do not register themselves. They are explicitly created, registered in the Vamp database and provisioned on the load balancer.\n{{ /note }}\n\nServices are created and published as follows:\n\nThe user describes a service and its desired endpoint port in the Vamp DSL.\nThe service is deployed to the configured container manager by Vamp.\nVamp instructs Vamp Gateway Agent (via ZooKeeper, etcd or Consul) to set up service endpoints.\nVamp Gateway Agent takes care of configuring HAProxy, making the services available.\n\nAfter this, you can scale the service up/down or in/out either by hand or using Vamp’s auto scaling functionality. The endpoint is stable.\n\nDiscovering a service\n\nSo, how does one service find a dependent service? Services are found by just referencing them in the DSL. Take a look at the following example:\n","id":11},{"path":"/documentation/how vamp works/what-to-choose","date":"2016-09-13T09:00:00+00:00","title":"What to choose?","content":"\n","id":12},{"path":"/documentation/installation/azure-container-service","date":"2016-09-30T12:00:00+00:00","title":"Azure Container Service","content":"\nTested against\nThis guide has been tested on \n\n Requirements\n\nBefore we begin...\n\n Standard install\n\n{{ note title=\"What next?\" }}\n\nOnce you have Vamp up and running you can jump into the getting started tutorials.\nThings still not running? We're here to help →\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}","id":13},{"path":"/documentation/installation/configure-elastic-stack","date":"2016-09-13T09:00:00+00:00","title":"Configure Elastic Stack","content":"\n\nConfigure Logstash\nIn general, for each HTTP/TCP request to HAProxy, several log messages are created (e.g. for gateway, service and instance level).\nUsing one of the simplest Logstash configurations should be sufficient for dozens of requests per second - or even more.\nThis also depends on whether ELK is used for custom application/service logs etc.\n\nYou can transform logs to plain JSON, which can be parsed easily later on (e.g. for Kibana visualization), with this Logstash configuration together with the default vamp.gateway-driver.haproxy log format (github.com/magneticio - logstash.conf).\n\n{{ tip title=\"Examples\" }}\n\nDifferent Logstash/Elasticsearch setups: (elastic.co - Deploying and Scaling Logstash).\nLogstash command line parameter (/github.com/magneticio - Logstash section).\n{{ /tip }}\n\n Configure Vamp for \n\n{{ note title=\"What next?\" }}\nConfigure Vamp\nSet container driver\nFollow the getting started tutorials\n{{ /note }}\n\n","id":14},{"path":"/documentation/installation/configure-vamp","date":"2016-09-13T09:00:00+00:00","title":"Configure Vamp","content":"\nVamp can be configured using one or a combination of:\n\nthe Vamp application.conf HOCON file (github.com/typesafehub - config)\nenvironment variables\nsystem properties\n\nFor example:\n\nexport VAMPINFOMESSAGE=Hello # overriding Vamp info message (vamp.info.message)\n\njava -Dvamp.gateway-driver.host=localhost \\\n     -Dlogback.configurationFile=logback.xml \\\n     -Dconfig.file=application.conf \\\n     -jar vamp.jar\n\nThe Vamp application.conf file\n\nThe Vamp application.conf consists of the following sections. All sections are nested inside a parent vamp {} tag.\n\nrest-api\npersistence\ncontainer-drivers\ngateway-driver\noperation\n\n rest-api\nConfigure the port, host name and interface that Vamp runs on using the rest-api.port \n\nvamp {\n  rest-api {\n    interface = 0.0.0.0\n    host = localhost\n    port = 8080\n    response-timeout = 10 seconds # HTTP response time out\n  }\n}    \n\npersistence\n\nVamp uses Elasticsearch for persistence and ZooKeeper (apache.org - ZooKeeper), etcd (coreos.com  - etcd documentation) or Consul (consul.io) for key-value store (keeping HAProxy configuration). \n\nvamp {\n  persistence {\n    response-timeout = 5 seconds\n\n    database {\n      type: \"elasticsearch\"  elasticsearch or in-memory (no persistence)\n      elasticsearch.url = ${vamp.pulse.elasticsearch.url}\n    }\n\n    key-value-store {\n    \n      type = \"zookeeper\"    # zookeeper, etcd or consul\n      base-path = \"/vamp\"   # base path for keys, e.g. /vamp/...\n\n      zookeeper {\n        servers = \"192.168.99.100:2181\"\n      }\n\n      etcd {\n        url = \"http://192.168.99.100:2379\"\n      }\n\n      consul {\n        url = \"http://192.168.99.100:8500\"\n      }\n    }\n  }\n}\n\nzookeeper, etcd or consul configuration is needed based on the type, e.g. if type = \"zookeeper\" then only zookeeper.servers should be set.\n\nContainer drivers\n\nVamp can be configured to work with the following container drivers:\n\nDocker\nMesos/Marathon\nKubernetes\nRancher\n\n Docker\nVamp can talk directly to a Docker daemon and its driver is configured by default. This is useful for local testing, Docker Swarm support is coming soon.\nVamp can even run inside Docker while deploying to Docker.\n\nInstall Docker as per Docker's installation manual (docs.docker.com - install Docker engine)\nCheck the DOCKER_* environment variables Vamp uses to connect to Docker, i.e.\n\n        DOCKER_HOST=tcp://192.168.99.100:2376\n    export DOCKERMACHINENAME=default\n    DOCKERTLSVERIFY=1\n    DOCKERCERTPATH=/Users/tim/.docker/machine/machines/default\n    \nIf Vamp can't find these environment variables, it falls back to using the unix:///var/run/docker.sock Unix socket for communicating with Docker.\nUpdate the container-driver section in Vamp's config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf:\n\n        ...\n    container-driver {\n      type = \"docker\"\n      response-timeout = 30 # seconds, timeout for container operations\n    }\n    ...\n        See Vamp configuration for details of the the Vamp application.conf file\n(Re)start Vamp by restarting the Java process by hand.   \n\nMesos/Marathon\nVamp can use the full power of Marathon running on either a DCOS cluster or custom Mesos cluster. You can use Vamp's DSL, or you can pass native Marathon options by using a dialect in a blueprint.  \n\nSet up a DCOS cluster using Mesosphere's assisted install on AWS (mesosphere.com - product).  \nIf you prefer, you can build your own Mesos/Marathon cluster. Here are some tutorials and scripts to help you get started:\n\n  Mesos Ansible playbook (github.com/mhamrah - ansible mesos playbook)\n  Mesos Vagrant (github.com/everpeace - vagrant-mesos)\n  Mesos Terraform (github.com/ContainerSolutions - Terraform Mesos)\n\nWhichever way you set up Marathon, in the end you should be able to see something like this:  \n\nMake a note of the Marathon endpoint (host:port) and update the container-driver section in Vamp's config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf. Set the \"url\" option to the Marathon endpoint.\n\n        ...\n    container-driver {\n      type = \"marathon\"\n      url = \"http://marathonhost:marathonport\" \n      response-timeout = 30  seconds, timeout for container operations\n    }\n    ...\n    (Re)start Vamp by restarting the Java process by hand.   \n\nKubernetes\nSpecify Kubernetes as the container driver in the Vamp application.conf file.   \n\nTaken from the example application.conf file for Kubernetes (github.com/magneticio - vamp-kubernetes):\n\n  ...\n  container-driver {\n\n    type = \"kubernetes\"\n\n    kubernetes {\n      url = \"https://kubernetes\"\n      service-type = \"LoadBalancer\"\n    }\n    ...\n\n Rancher\n\nSpecify Rancher as the container driver in the Vamp application.conf file.   \n\nTaken from the example application.conf file for Rancher (github.com/magneticio - vamp-rancher):\n\n  ...\n  container-driver.type = \"rancher\"\n  ...\n\ngateway-driver\n\nThe gateway-driver section configures how traffic should be routed through Vamp Gateway Agent. See the below example on how to configure this:\n\nvamp {\n  gateway-driver {\n    host: \"10.193.238.26\"               Vamp Gateway Agent / Haproxy, internal IP.\n    response-timeout: 30 seconds\n\n    haproxy {\n      ip: 127.0.0.1                    # HAProxy backend server IP\n\n      template: \"\"                     # Path to template file, if not specified default will be used\n\n      virtual-hosts {\n        ip: \"127.0.0.1\"                # IP, if virtual hosts are enabled\n        port: 40800                    # Port, if virtual hosts are enabled\n      }\n    }\n  }\n}  \n\nThe reason for the need to configure vamp.gateway-driver.host is that when services are deployed, they need to be able to find Vamp Gateway Agent in their respective networks. This can be a totally different network than where Vamp is running.\nLet's use an example: frontend and backend service, frontend depends on backend - in Vamp DSL that would be 2 clusters (assuming the same deployment).\nThere are different ways how frontend can discover its dependency backend, and to make things simpler Vamp supports using specific environment parameters.\n \n","id":15},{"path":"/documentation/installation/dcos","date":"2016-09-30T12:00:00+00:00","title":"DC/OS 1.7 and 1.8","content":"\nOverview\n\nThere are different ways to install Vamp on DC.OS. On this page we start out with the most common setup, but if you are interested in doing a custom install or working with public and private nodes you should jump to that section.\n\nStandard install\nCustom install\nPublic and private nodes\n\n Standard install\nThis setup will run Vamp, Mesos and Marathon, together with Zookeeper, Elasticsearch and Logstash on DC/OS. If you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\nTested against\nThis guide has been tested on both 1.7 and the latest 1.8 version of DC/OS.\n\n Requirements\nBefore you start you need to have a DC/OS cluster up and running, as well as the its CLI configured to use it. We assume you have it up and running on http://dcos.example.com/.\nSetting up DC/OS is outside the scope of this document, for that you need to refer to the official documentation:\n\nhttps://dcos.io/docs/1.7/administration/installing/\nhttps://dcos.io/docs/1.7/usage/cli/\nhttps://dcos.io/docs/1.8/administration/installing/\nhttps://dcos.io/docs/1.8/usage/cli/\n\nStep 1: Install Elasticsearch + Logstash\n\nMesos, Marathon and ZooKeeper are all installed by DC/OS. In addition to these, Vamp requires Elasticsearch and Logstash for metrics collection and aggregation.\n\nYou could install Elasticsearch on DC/OS by following the Mesos Elasticsearch documentation (mesos-elasticsearch - Elasticsearch Mesos Framework).\nHowever, Vamp will also need Logstash (not currently available as a DC/OS package) with a specific Vamp Logstash configuration (github.com/magneticio - Vamp Docker logstash.conf).  \n\nTo make life easier, we have created compatible Docker images for a Vamp Elastic Stack (hub.docker.com - magneticio elastic) that you can use with the Mesos elasticsearch documentation (mesos-elasticsearch - How to install on Marathon).\nOur advice is to use our custom Elasticsearch+Logstash Docker image. Let's get started!\n\nCreate elasticsearch.json with the following content:\n\n{\n  \"id\": \"elasticsearch\",\n  \"instances\": 1,\n  \"cpus\": 0.2,\n  \"mem\": 1024.0,\n  \"container\": {\n    \"docker\": {\n      \"image\": \"magneticio/elastic:2.2\",\n      \"network\": \"HOST\",\n      \"forcePullImage\": true\n    }\n  },\n  \"healthChecks\": [\n    {\n      \"protocol\": \"TCP\",\n      \"gracePeriodSeconds\": 30,\n      \"intervalSeconds\": 10,\n      \"timeoutSeconds\": 5,\n      \"port\": 9200,\n      \"maxConsecutiveFailures\": 0\n    }\n  ]\n}\n\nThis will run the container with 1G of RAM and a basic health check on the elasticsearch port.\n\nUsing the CLI we can install this in our cluster:\n\n$ dcos marathon app add elasticsearch.json\n\nIf you get no error message you should now be able to see it being deployed:\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    0/1    0/0      scale       DOCKER   None  \n\nOnce it's fully up and running you should see all tasks and health checks being up:\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    1/1    1/1       ---        DOCKER   None  \n\n Step 2: Deploy Vamp\n\nOnce you have elasticsearch up and running it's time to move on to Vamp.\n\nCreate vamp.json with the following content:\n\n{\n  \"id\": \"vamp/vamp\",\n  \"instances\": 1,\n  \"cpus\": 0.5,\n  \"mem\": 1024,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"magneticio/vamp:katana-dcos\",\n      \"network\": \"BRIDGE\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8080,\n          \"hostPort\": 0,\n          \"name\": \"vip0\",\n          \"labels\": {\n            \"VIP_0\": \"10.20.0.100:8080\"\n          }\n        }\n      ],\n      \"forcePullImage\": true\n    }\n  },\n  \"labels\": {\n    \"DCOSSERVICENAME\": \"vamp\",\n    \"DCOSSERVICESCHEME\": \"http\",\n    \"DCOSSERVICEPORT_INDEX\": \"0\"\n  },\n  \"env\": {\n    \"VAMPWAITFOR\": \"http://elasticsearch.marathon.mesos:9200/.kibana\",\n    \"VAMPPERSISTENCEDATABASEELASTICSEARCHURL\": \"http://elasticsearch.marathon.mesos:9200\",\n    \"VAMPGATEWAYDRIVERLOGSTASHHOST\": \"elasticsearch.marathon.mesos\",\n    \"VAMPWORKFLOWDRIVERVAMPURL\": \"http://10.20.0.100:8080\",\n    \"VAMPPULSEELASTICSEARCH_URL\": \"http://elasticsearch.marathon.mesos:9200\"\n  },  \n  \"healthChecks\": [\n    {\n      \"protocol\": \"TCP\",\n      \"gracePeriodSeconds\": 30,\n      \"intervalSeconds\": 10,\n      \"timeoutSeconds\": 5,\n      \"portIndex\": 0,\n      \"maxConsecutiveFailures\": 0\n    }\n  ]\n}\n\nThis service definition will download our Vamp container and spin it up in your DC/OS cluster on a private node in bridge networking mode. It will also configure the apporiate labels for the AdminRouter to expose the UI through DC/OS, as well as an internal VIP for other applications to talk to Vamp, adjusting some defaults to work inside DC/OS, and finally a health check for monitoring.\n\nDeploy it with the CLI, like with did with elasticsearch:\n\n$ dcos marathon app add vamp.json\n\n$ dcos marathon app list\nID              MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD   \n/elasticsearch  1024  0.2    1/1    1/1       ---        DOCKER   None  \n/vamp/vamp      1024  0.5    0/1    0/0      scale       DOCKER   None  \n\nIt will take a minute for Vamp to deploy all its components, you can see that by looking in the \"tasks\" column, where Vamp is listed as 0/1. Run the list command again and you should see all the components coming online:\n\n$ dcos marathon app list\nID                        MEM   CPUS  TASKS  HEALTH  DEPLOYMENT  CONTAINER  CMD\n/elasticsearch            1024  0.2    1/1    1/1       ---        DOCKER   None\n/vamp/vamp                1024  0.5    1/1    1/1       ---        DOCKER   None\n/vamp/vamp-gateway-agent  256   0.2    3/3    ---       ---        DOCKER   ['--storeType=zookeeper', '--storeConnection=zk-1.zk:2181', '--storeKey=/vamp/haproxy/1.6', '--logstash=elasticsearch.marathon.mesos:10001']\n/vamp/workflow-health      64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-kibana      64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-metrics     64   0.1    1/1    ---       ---        DOCKER   None\n/vamp/workflow-vga         64   0.1    1/1    ---       ---        DOCKER   None\n\nVamp has now spun up all it's components and you should be able to access the ui by opening http://dcos.example.com/service/vamp/ in your browser.\n\nNow you're ready to follow our Vamp getting started tutorials.\nThings still not running? We're here to help →\n\n NB If you need help you can also find us on [Gitter] (https://gitter.im/magneticio/vamp)\n\nCustom install\n\nThe Vamp DC/OS Docker image (github.com/magneticio - Vamp DC/OS) contains configuration (github.com/magneticio - Vamp DC/OS configuration) that can be overridden for specific needs by:\n\nMaking a new Docker image based on the Vamp DC/OS image\nUsing environment variables\n\n Example 1 - Remove the metrics and health workflows by Vamp configuration and keep the kibana workflow:\n\nvamp.lifter.artifact.resources = [\n    \"breeds/kibana.js\", \"workflows/kibana.yml\"\n  ]\n\nor doing the same using Marathon JSON\n\n\"env\": {\n  \"VAMPLIFTERARTIFACT_RESOURCES\": \"[\\\"breeds/kibana.js\\\",\\\"workflows/kibana.yml\\\"]\"\n}\n\nExample 2 - Avoid automatic deployment of Vamp Gateway Agent\n\nRemove vga-marathon breed and workflow from vamp.lifter.artifact.files:\n\nvamp.lifter.artifact.files = []\n\nor using Marathon JSON\n\n\"env\": {\n  \"VAMPLIFTERARTIFACT_FILES\": \"[]\"\n}\n\n Public and private nodes\n\nRunning Vamp on public Mesos agent node(s) and disabling automatic Vamp Gateway Agent deployments (but keeping other default workflows) can be done with the following Marathon JSON:\n\n{\n  \"id\": \"vamp/vamp\",\n  \"instances\": 1,\n  \"cpus\": 0.5,\n  \"mem\": 1024,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"magneticio/vamp:katana-dcos\",\n      \"network\": \"BRIDGE\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 8080,\n          \"hostPort\": 0,\n          \"name\": \"vip0\",\n          \"labels\": {\n            \"VIP_0\": \"10.20.0.100:8080\"\n          }\n        }\n      ],\n      \"forcePullImage\": true\n    }\n  },\n  \"labels\": {\n    \"DCOSSERVICENAME\": \"vamp\",\n    \"DCOSSERVICESCHEME\": \"http\",\n    \"DCOSSERVICEPORT_INDEX\": \"0\"\n  },\n  \"env\": {\n    \"VAMPLIFTERARTIFACT_FILES\": \"[\\\"breeds/health.js\\\",\\\"workflows/health.yml\\\",\\\"breeds/metrics.js\\\",\\\"workflows/metrics.yml\\\",\\\"breeds/kibana.js\\\",\\\"workflows/kibana.yml\\\"]\",\n    \"VAMPWAITFOR\": \"http://elasticsearch.marathon.mesos:9200/.kibana\",\n    \"VAMPPERSISTENCEDATABASEELASTICSEARCHURL\": \"http://elasticsearch.marathon.mesos:9200\",\n    \"VAMPGATEWAYDRIVERLOGSTASHHOST\": \"elasticsearch.marathon.mesos\",\n    \"VAMPWORKFLOWDRIVERVAMPURL\": \"http://vamp-vamp.marathon.mesos:8080\",\n    \"VAMPPULSEELASTICSEARCH_URL\": \"http://elasticsearch.marathon.mesos:9200\"\n  },\n  \"acceptedResourceRoles\": [\n    \"slave_public\"\n  ],\n  \"healthChecks\": [\n    {\n      \"protocol\": \"TCP\",\n      \"gracePeriodSeconds\": 30,\n      \"intervalSeconds\": 10,\n      \"timeoutSeconds\": 5,\n      \"portIndex\": 0,\n      \"maxConsecutiveFailures\": 0\n    }\n  ]\n}\n\nDeploying Vamp Gateway Agent on all public and private Mesos agent nodes through Marathon JSON - NB replace $INSTANCES (e.g. to be the same as total number of Mesos agent nodes) and optionally other parameters:\n\n{\n  \"id\": \"vamp/vamp-gateway-agent\",\n  \"instances\": $INSTANCES,\n  \"cpus\": 0.2,\n  \"mem\": 256.0,\n  \"container\": {\n    \"type\": \"DOCKER\",\n    \"docker\": {\n      \"image\": \"magneticio/vamp-gateway-agent:katana\",\n      \"network\": \"HOST\",\n      \"privileged\": true,\n      \"forcePullImage\": true\n    }\n  },\n  \"args\": [\n    \"--storeType=zookeeper\",\n    \"--storeConnection=zk-1.zk:2181\",\n    \"--storeKey=/vamp/gateways/haproxy/1.6\",\n    \"--logstash=elasticsearch.marathon.mesos:10001\"\n  ],\n  \"constraints\": [\n    [\n      \"hostname\",\n      \"UNIQUE\"\n    ]\n  ],\n  \"acceptedResourceRoles\": [\n    \"slave_public\",\n    \"*\"\n  ]\n}\n\n{{ note title=\"What next?\" }}\n\nOnce you have Vamp up and running you can follow our getting started tutorials.\nChcek the Vamp documentation\nThings still not running? We're here to help →\n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n{{ /note }}","id":16},{"path":"/documentation/installation/hello-world","date":"2016-09-13T09:00:00+00:00","title":"Hello world","content":"\nThe Vamp hello world setup will run Mesos, Marathon (mesosphere.github.io - Marathon) and Vamp inside a local Docker container with Vamp's Marathon driver.  We will do this in three simple steps (although it's really just one docker run command). You can use the hello world setup to work through the getting started tutorials and try out some of Vamp's core features.\n\n{{ note }}\nThis hello world set up is designed for demo purposes only - it is not production grade.\n{{ /note }}\n\nStep 1: Get Docker\n\nPlease install one of the following for your platform/architecture\n\nDocker 1.9.x (Linux) or higher (Vamp works with Docker 1.12 too), OR\n[Docker Toolbox 1.12.x] (https://github.com/docker/toolbox/releases) if on Mac OS X 10.8+ or Windows 7+ \n\nVamp hello world on Docker for Mac or Windows is currently not supported. We're working on this so please check back. \n\n Step 2: Run Vamp\n\nUse the instructions below to start the magneticio/vamp-docker:0.9.0-marathon container, taking care to pass in the right parameters. \n\nLinux\n\nA typical command would be:\ndocker run --privileged \\\n           --net=host \\\n           -v /var/run/docker.sock:/var/run/docker.sock \\\n           -v $(which docker):/bin/docker \\\n           -v \"/sys/fs/cgroup:/sys/fs/cgroup\" \\\n           -e \"DOCKERHOSTIP=hostname -I | awk '{print $1;}'\" \\\n           magneticio/vamp-docker:0.9.0\n\nMounting volumes is important. Read this great article about starting Docker containers from/within another Docker container.\n\n Mac OS X 10.8+ or Windows 7+\n\nIf you installed Docker Toolbox, please use the Docker Quickstart Terminal. We don't currently support Kitematic. A typical command on Mac OS X running Docker Toolbox would be:\ndocker run --net=host \\\n           -v /var/run/docker.sock:/var/run/docker.sock \\\n           -v docker-machine ssh default \"which docker\":/bin/docker \\\n           -v \"/sys/fs/cgroup:/sys/fs/cgroup\" \\\n           -e \"DOCKERHOSTIP=docker-machine ip default\" \\\n           magneticio/vamp-docker:0.9.0\n\nStep 3: Check Vamp is up and running\n\nAfter some downloading and booting, your Docker log will show Vamp has launched and report something like:\n\n...Bound to /0.0.0.0:8080\n\nNow check if Vamp is home on http://{docker-machine ip default}:8080/ and you're ready for the Vamp getting started tutorials\n\nExposed services:\n\nHAProxy statistics | http://localhost:1988 (username/password: haproxy) \nHAProxy statistics http://localhost:1988 (username/password: haproxy)\nElasticsearch HTTP http://localhost:9200\nKibana http://localhost:5601\nSense http://localhost:5601/app/sense\nMesos http://localhost:5050\nMarathon http://localhost:9090 (Note that the Marathon port is 9090 and not the default 8080).\nChronos http://localhost:4400\nVamp http://localhost:8080\n\nIf you run on Docker machine, use docker-machine ip default instead of localhost.\n\n{{ note title=\"Note\" }}\nThis set up runs all of Vamp's components in one container. This is definitely not ideal, but works fine for kicking the tires.\nYou will run into cpu, memory and storage issues pretty soon though. Also, random ports are assigned by Vamp which you might not have exposed on either Docker or your Docker Toolbox Vagrant box.  \n{{ /note }}\n\n What next?\n\nNow you're all set to follow our getting started tutorials.\nThings still not running? We're here to help →\nRemember, this is not a production grade setup!\n\nIf you need help you can also find us on [Gitter] (https://gitter.im/magneticio/vamp)","id":17},{"path":"/documentation/installation/","date":"2016-09-13T09:00:00+00:00","title":"Installation","content":"Before you get Vamp up and running on your architecture, it is helpful to understand how vamp works and the role of each component and its preferred location in a typical architecture.  \n\nIf you need help you can find us on [Gitter] (https://gitter.im/magneticio/vamp)\n\nRequirements\n\nVamp requirements\n\n Install Vamp\n\nDC/OS 1.7 and 1.8\nMesos/Marathon\nKubernetes 1.2\nRancher\nAzure Container Service\n\nConfiguration\n\nConfigure Vamp\nSet container driver\nConfigure Elastic Stack \n\n Try Vamp\n\nWe've put together a hello world walkthrough to let you try out some of Vamp's core features in a local docker container. You can use this to work through the getting started tutorials.\n\n","id":18},{"path":"/documentation/installation/kubernetes","date":"2016-10-04T09:00:00+00:00","title":"Kubernetes 1.x","content":"\n{{ note title=\"Note!\" }}\nKubernetes support is still in Alpha.\n{{ /note }}\n\nTested against\nThis guide has been tested on Kubernetes 1.2 and 1.3. Minikube can also be used. (github.com - minikube) \n\n Requirements\n\nGoogle Container Engine cluster\nKey-value store (like ZooKeeper, Consul or etcd)\nElasticsearch and Logstash\n\nBefore we begin...\nIt is advisable to try out the official Quickstart for Google Container Engine tutorial first (google.com - container engine quickstart).  \n\n Standard install\nThe standard install will run Vamp together with etcd, Elasticsearch and Logstash on Google container engine and kubernetes. (We will also deploy our demo Sava application to give you something to play around on).   \n\nStep 1: Create a new GKE cluster:\n\nThe simple way to create a new GKE cluster:\n\nopen Google Cloud Shell\nset a zone, e.g. gcloud config set compute/zone europe-west1-b\ncreate a cluster vamp using default parameters: gcloud container clusters create vamp\n\nAfter the (new) Kubernetes cluster is setup, we are going to continue with the installation using the Kubernetes CLI kubectl.\nYou can use kubectl directly from the Google Cloud Shell, e.g. to check the Kubernetes client and server version:\n\n$ kubectl version\n\n Step 2: Deploy etcd, Elasticsearch and Logstash\n\nLet's deploy etcd - the installation is based on this tutorial (github.com/coreos - etcd on Kubernetes).\nExecute: \n\n$ kubectl create \\\n        -f https://raw.githubusercontent.com/magneticio/vamp-docker/master/vamp-kubernetes/etcd.yml\n\nDeploy Elasticsearch and Logstash with a proper Vamp Logstash configuration (github.com/magneticio - elastic):\n\n$ kubectl run elastic --image=magneticio/elastic:2.2\n$ kubectl expose deployment elastic --protocol=TCP --port=9200 --name=elasticsearch\n$ ubectl expose deployment elastic --protocol=UDP --port=10001 --name=logstash\n$ kubectl expose deployment elastic --protocol=TCP --port=5601 --name=kibana\n{{ note title=\"Note!\" }}\nThis is not a production grade setup. You would also need to take care of persistence and running multiple replicas of each pod.\n{{ /note }}\n\nStep 3: Run Vamp\n\nLet's run Vamp gateway agent as a daemon set first:\n$ kubectl create \\\n        -f https://raw.githubusercontent.com/magneticio/vamp-docker/master/vamp-kubernetes/vga.yml\n\nTo deploy Vamp, execute:\n\n$ kubectl run vamp --image=magneticio/vamp:0.9.0-kubernetes\n$ kubectl expose deployment vamp --protocol=TCP --port=8080 --name=vamp --type=\"LoadBalancer\"\n\nThe Vamp image uses the following configuration (github.com/magneticio - Vamp kubernetes configuration).\n\nWait a bit until Vamp is running and check out the Kubernetes services:\n\n$ kubectl get services\n\nThe output should be similar to this:\n\nNAME                 CLUSTER-IP     EXTERNAL-IP      PORT(S)             AGE\nelasticsearch        10.3.242.188   none           9200/TCP            4m\netcd-client          10.3.247.112   none           2379/TCP            4m\netcd0                10.3.251.13    none           2379/TCP,2380/TCP   4m\netcd1                10.3.251.103   none           2379/TCP,2380/TCP   4m\netcd2                10.3.250.20    none           2379/TCP,2380/TCP   4m\nkubernetes           10.3.240.1     none           443/TCP             5m\nlogstash             10.3.254.16    none           10001/UDP           4m\nvamp                 10.3.242.93    146.148.118.45   8080/TCP            2m\nvamp-gateway-agent   10.3.254.234   146.148.22.145   80/TCP              2m\n\nNotice that the Vamp UI is exposed (in this example) on http://146.148.118.45:8080\n\n Step 4: Deploy the Sava demo application\n\n","id":19},{"path":"/documentation/installation/mesos-marathon","date":"2016-09-30T12:00:00+00:00","title":"Mesos/Marathon","content":"\nVamp can use the full power of Marathon running on either a DCOS cluster or custom Mesos cluster. You can use Vamp's DSL, or you can pass native Marathon options by using a dialect in a blueprint.  \n\nInstall\nThe instructions included on the DC/OS installation page will also work with Mesos/Marathon.\n\n set container driver\nSee set Mesos/Marathon as the container driver\n\n","id":20},{"path":"/documentation/installation/rancher","date":"2016-09-13T09:00:00+00:00","title":"Rancher","content":"\n{{ note title=\"Note!\" }}\nRancher support is still in Alpha.\n{{ /note }}\n\nTested against\nThis guide has been tested on Rancher version 1.1.x.\n\n Requirements\n\nRancher up and running\nKey-value store like ZooKeeper, Consul or etcd\nElasticsearch and Logstash\n\nBefore we begin...\nIt is advisable to try out the official Rancher Quick Start Guide tutorial first (rancher.com - quick start guide).  \n\n Standard install\nThe standard install will run Vamp together with Consul, Elasticsearch and Logstash on Rancher. (We'll also deploy our demo Sava application to give you something to play around on).\n\nIf you want to make a setup on your local VM based Docker, it's advisable to increase default VM memory size from 1GB to 4GB.\n\nStep 1: Run Rancher locally\nBased on the official Rancher quickstart tutorial, these are a few simple steps to run Rancher locally:\n$ docker run -d --restart=always -p 8080:8080 rancher/server\nThe Rancher UI is exposed on port 8080, so go to http://SERVER_IP:8080 - for instance http://192.168.99.100:8080, http://localhost:8080 or something similar depending on your Docker setup.\n  \nFollow the instructions on the screen to add a new Rancher host:\n\nclick on \"Add Host\" and then on \"Save\". \nYou should get instructions (bullet point 5) to run an agent Docker image:\n$ docker run \\\n  -d --privileged \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /var/lib/rancher:/var/lib/rancher \\\n  rancher/agent:v1.0.1 \\\n  http://192.168.99.100:8080/v1/scripts/E78EF5848B989FD4DA77:1466265600000:SYqIvhPgzKLonp8r0erqgpsi7pQ\n\nGo to Add Stack and create a new stack vamp (lowercase).   \n\n Step 2: Install Consul, Elasticsearch and Logstash\nWe can now install the other dependencies.\n\nConsul\n\nUse your newly created vamp stack and go to Add Service:\n\nName ⇒ consul\nSelect Image ⇒ gliderlabs/consul-server\nSet Command ⇒ -server -bootstrap\nGo to Networking tab\nUnder Hostname select Set a specific hostname: and enter consul\nClick the Create button\n\n Elasticsearch and Logstash\n\n Our custom Docker image magneticio/elastic:2.2 contains Elasticsearch, Logstash and Kibana with the proper Logstash configuration for Vamp. More details can be found on the github project page (github.com/magneticio - elastic).\n\nUse the vamp stack and go to Add Service:\n\nName ⇒ elastic\nSelect Image ⇒ magneticio/elastic:2.2\nGo to Networking tab\nUnder Hostname select Set a specific hostname: and enter elastic\nClick on Create button\n\nStep 3: Run Vamp\n\nFirst we'll run the Vamp Gateway Agent: \n\nUse the vamp stack and go to Add Service:\n\nSet scale to Always run one instance of this container on every host\nName ⇒ vamp-gateway-agent\nSelect Image ⇒ magneticio/vamp-gateway-agent:0.9.0\nSet Command ⇒ --storeType=consul --storeConnection=consul:8500 --storeKey=/vamp/haproxy/1.6 --logstash=elastic:10001\nGo to Networking tab\nUnder Hostname select Set a specific hostname: and enter vamp-gateway-agent\nClick on Create button\n\nNow let's find a Rancher API endpoint that can be accessed from running container:\n\nGo to the API page and find the endpoint, e.g. http://192.168.99.100:8080/v1/projects/1a5\nGo to the Infrastructure/Containers and find the IP address of rancher/server, e.g. 172.17.0.2\nThe Rancher API endpoint should be then http://IP_ADDRESS:PORT/PATH based on values we have, e.g. http://172.17.0.2:8080/v1/projects/1a5\n\nNow we can deploy Vamp:\n\nUse the vamp stack and go to Add Service:\n\nName ⇒ vamp\nSelect Image ⇒ magneticio/vamp:0.9.0-rancher\nGo to Add environment variable VAMPCONTAINERDRIVERRANCHERURL with value of Rancher API endpoint, e.g. http://172.17.0.2:8080/v1/projects/1a5\nGo to Networking tab\nUnder Hostname select Set a specific hostname: and enter vamp\nClick the Create button\nGo to Add Load Balancer (click arrow next to Add Service button label)\nChoose a name (e.g. vamp-lb), Source IP/Port ⇒ 9090, Default Target Port ⇒ 8080 and Target Service ⇒ vamp\n\nIf you go to http://SERVER_IP:9090 (e.g http://192.168.99.100:9090), you should get the Vamp UI.  \nYou should also notice that Vamp Gateway Agent is running (one instance on each node) and additional Vamp workflows.\n\nTo access HAProxy stats:\n\nGo to Add Load Balancer (click arrow next to Add Service)\nChoose aname (e.g. vamp-gateway-agent-lb), Source IP/Port ⇒ 1988, Default Target Port ⇒ 1988 and Target Service ⇒ vamp-gateway-agent\nUse the following username/password: haproxy for the HAProxy stats page\n\n Step 4: Deploy the Sava demo application\n\nLet's deploy our sava demo application:\n\n","id":21},{"path":"/documentation/installation/set-container-driver","date":"2016-09-30T12:00:00+00:00","title":"Set container driver","content":"\nVamp can be configured to work with the following container drivers:\n\nDocker\nMesos/Marathon\nKubernetes\nRancher\n\nDocker\nVamp can talk directly to a Docker daemon and its driver is configured by default. This is useful for local testing, Docker Swarm support is coming soon.\nVamp can even run inside Docker while deploying to Docker.\n\nInstall Docker as per Docker's installation manual (docs.docker.com - install Docker engine)\nCheck the DOCKER_* environment variables Vamp uses to connect to Docker, i.e.\n\n        DOCKER_HOST=tcp://192.168.99.100:2376\n    export DOCKERMACHINENAME=default\n    DOCKERTLSVERIFY=1\n    DOCKERCERTPATH=/Users/tim/.docker/machine/machines/default\n    \nIf Vamp can't find these environment variables, it falls back to using the unix:///var/run/docker.sock Unix socket for communicating with Docker.\nUpdate the container-driver section in Vamp's config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf:\n\n        ...\n    container-driver {\n      type = \"docker\"\n      response-timeout = 30  seconds, timeout for container operations\n    }\n    ...\n        See Vamp configuration for details of the the Vamp application.conf file\n(Re)start Vamp by restarting the Java process by hand.   \n\nMesos/Marathon\nVamp can use the full power of Marathon running on either a DCOS cluster or custom Mesos cluster. You can use Vamp's DSL, or you can pass native Marathon options by using a dialect in a blueprint.  \n\nSet up a DCOS cluster using Mesosphere's assisted install on AWS (mesosphere.com - product).  \nIf you prefer, you can build your own Mesos/Marathon cluster. Here are some tutorials and scripts to help you get started:\n\n  Mesos Ansible playbook (github.com/mhamrah - ansible mesos playbook)\n  Mesos Vagrant (github.com/everpeace - vagrant-mesos)\n  Mesos Terraform (github.com/ContainerSolutions - Terraform Mesos)\n\nWhichever way you set up Marathon, in the end you should be able to see something like this:  \n\nMake a note of the Marathon endpoint (host:port) and update the container-driver section in Vamp's config file. If you use a package installer like yum or apt-get you can find this file in /usr/share/vamp/conf/application.conf. Set the \"url\" option to the Marathon endpoint.\n\n        ...\n    container-driver {\n      type = \"marathon\"\n      url = \"http://marathonhost:marathonport\" \n      response-timeout = 30  seconds, timeout for container operations\n    }\n    ...\n    (Re)start Vamp by restarting the Java process by hand.   \n\nKubernetes\nSpecify Kubernetes as the container driver in the Vamp application.conf file.   \n\nTaken from the example application.conf file for Kubernetes (github.com/magneticio - vamp-kubernetes):\n\n  ...\n  container-driver {\n\n    type = \"kubernetes\"\n\n    kubernetes {\n      url = \"https://kubernetes\"\n      service-type = \"LoadBalancer\"\n    }\n    ...\n\n Rancher\n\nSpecify Rancher as the container driver in the Vamp application.conf file.   \n\nTaken from the example application.conf file for Rancher (github.com/magneticio - vamp-rancher):\n\n  ...\n  container-driver.type = \"rancher\"\n  ...\n\n{{ note title=\"What next?\" }}\nConfigure Vamp\nConfigure Elastic Stack\nFollow the getting started tutorials\n{{ /note }}","id":22},{"path":"/documentation/release-notes","title":"","date":"","content":"---\ndate: 2016-09-13T09:00:00+00:00\ntitle: Release notes\n---","id":23},{"path":"/documentation/tutorials/deploy-your-first-blueprint","date":"2016-09-13T09:00:00+00:00","title":"Deploy your first blueprint","content":"Overview\n\nIf everything went to plan, you should have your Vamp installation up and running. If not, please follow the Vamp hello world quick setup steps. Now we're ready to check out some of Vamp's features. \n\n In this tutorial we will:  \n\nDeploy a monolith, using either the Vamp UI or the Vamp API\nCheck out the deployed application  \nGet some metrics on the running application  \nChange the scale and load-balancing\nChaos monkey!    \n\nIn depth\n\n Step 1: Deploy a monolith\n\nImagine you or the company you work for still use monolithic applications. I know, it sounds far fetched...\nThis application is conveniently called Sava monolith and is at version 1.0.  \n\nYou've managed to wrap your monolith in a Docker container, which lives in the Docker hub under magneticio/sava:1.0.0. Your app normally runs on port 8080 but you want to expose it under port 9050 in this case. Let's deploy this through Vamp using the following simple blueprint. Don't worry too much about what means what: we'll get there.\n\n","id":24},{"path":"/documentation/tutorials/","date":"2016-09-13T09:00:00+00:00","title":"Tutorials","content":"\nGetting started\nWe’ve created a set of showcase applications, services and corresponding blueprints that demonstrate Vamp’s core features - together we call them “Sava”. Sava is a mythical vampire from Serbia (wiki), but in our case it is a Github repo full of examples to help us demonstrate Vamp.\nYou can work with Sava in the Vamp hello world setup (or any other Vamp installation).\n\nDeploy your first blueprint\nRun a canary release\nSplit a monolith into services\nMerge a changed topology\n","id":25},{"path":"/documentation/tutorials/merge-and-delete","date":"2016-09-13T09:00:00+00:00","title":"Merge and delete","content":"\nOverview\n\nIn the previous tutorial we \"over-engineered\" our service based solution a bit (on purpose of course). We don't really need two backends services, so in this tutorial we will introduce our newly engineered solution and transition to it using Vamp blueprints and canary releasing methods.\n\n In this tutorial we will:\nGet some background and theory on merging services\nPrepare our blueprint\nTransition from blueprints to deployments (and back)\nDelete parts of the deployment\nAnswer the all important question when would I use this?\n\nIn depth\n\n step 1: Get some background and theory\n\nWhat we are going to do is create a new blueprint that is completely valid by itself and merge it\nwith our already running deployment. This might sound strange at first, but it makes sense. Why? Merging will enable us to slowly move from the previous solution to the next solution. Once moved over, we can\nremove any parts we no longer need, i.e. the former \"over-engineered\" topology.\n\nIn the diagram above, this is visualized as follows:\n\nWe have a running deployment (the blue circle with the \"1\"). To this we introduce a new blueprint\nwhich is merged with the running deployment (the pink circle with the \"2\").\nAt a point, both are active as we are transitioning from blue to pink.\nOnce we are fully on pink, we actively remove/decommission the blue part.\n\nIs this the same as a blue/green release? Yes, but we like pink better ;o)\n\nStep 2: Prepare our blueprint\n\nThe below blueprint describes our more reasonable service topology. Again, this blueprint is completely\nvalid by itself. You could just deploy it somewhere separately and not merge it with our over-engineered\ntopology. Notice the following:\n\nThe blueprint only has one backend cluster with one service.\nThe blueprint does not specify a gateway using the gateways key because we are going to use the gateway already present and configured in the running deployment. However, it would be perfectly correct to specify the old gateway - the gateway would be updated as well.\n\n","id":26},{"path":"/documentation/tutorials/run-a-canary-release","date":"2016-09-13T09:00:00+00:00","title":"Run a canary release","content":"\nOverview\n\nIn the previous tutorial we deployed our app sava 1.0. If you haven't walked through that part already, please do so before continuing. \n\nNow let's say we have a new version of this great application that we want to canary release into production. We have it containerised as magneticio/sava:1.1.0 and are ready to go. \n\n In this tutorial we will:\n\nPrepare our blueprint\nDeploy the new version of our application next to the old one\nUse conditions to target specific groups\nLearn a bit more about conditions\n\nIn depth\n\n Step 1: Prepare our blueprint\n\nVamp allows you to do canary releases using blueprints. Take a look at the YAML example below. It is quite similar to the blueprint we initially used to deploy sava 1.0.0. However, there are two big differences.\n\nThe services key holds a list of breeds: one for v1.0.0 and one for v1.1.0 of our app. Breeds are Vamp's way of describing static artifacts that can be used in blueprints.\nWe've added the routing key which holds the weight of each service as a percentage of all requests. Notice we assigned 50% to our current version 1.0.0 and 50% to the new version 1.1.0 We could also start with a 100% to 0% split, a 99% to 1% split or whatever combination you want as long as all percentages add up to 100% in total.\n\n","id":27},{"path":"/documentation/tutorials/split-a-monolith","date":"2016-09-13T09:00:00+00:00","title":"Split a monolith","content":"Overview\n\nIn the previous tutorial we did some basic canary releasing on two versions of a monolithic application. Very nice, but Vamp isn't\ncalled the Very Awesome Microservices Platform for nothing. The next step is to split our monolithic Sava application into separate services.\n\n In this tutorial we will:\n\ndefine a new service topology\nlearn about environment variables and service discovery\n\nIn depth\n\n Step 1: Define a new service topology\n\nTo prove our point, we are going to slightly \"over-engineer\" our services solution. This will also help\nus demonstrate how we can later remove parts of our solution using Vamp. For now, we'll split the\nmonolith into a topology of one frontend and two separate backend services. After our engineers\nare done with coding, we can catch this new topology in the following blueprint. Please notice a couple\nof things:\n\nWe now have three clusters: sava, backend1 and backend2. Each cluster could have multiple\nservices on which we could do separate canary releases and set separate filters.\nThe sava cluster has explicit dependencies on the two backends. Vamp will make sure these dependencies\nare checked and rolled out in the right order.\nUsing environment_variables we connect the dynamically assigned ports and hostnames of the backend\nservices to the \"customer facing\" sava service.\nWe've change the gateway port to 9060 so it doesn't collide with the  monolithic deployment.\n\n","id":28},{"path":"/documentation/using vamp/artifacts","date":"2016-09-13T09:00:00+00:00","title":"Artifacts","content":"\nVamp has a few basic entities or artifacts you can work with, these can be classed as static resource descriptions and dynamic runtime entities. Note that API actions on static resource descriptions are mostly synchronous, while API actions on dynamic runtime entities are largely asychronous.\n\nStatic resource descriptions\n\nBreeds describe single services and their dependencies.  Read more...\nBlueprints are, well, blueprints! They describe how breeds work in runtime and what properties they should have.  Read more...  \n\n Dynamic runtime entities\n\nDeployments are running blueprints. You can have many deployments from one blueprint and perform actions on each at runtime. Plus, you can turn any running deployment into a blueprint.  Read more...  \nGateways are the \"stable\" routing endpoint - defined by a port (incoming) and routes (outgoing).  Read more... \nWorkflows are apps (services) deployed on cluster, used for dynamically changing the runtime configuration (e.g. SLA, scaling, condition weight update).  Read more...\n\nWorking across multiple teams\n\nIn larger companies with multiple teams working together on a large project, all required information is often not available at the same time. To facilitate this style of working, Vamp allows you to set placeholders. Placeholders let you communicate with other teams using simple references and gradually build up a complicated deployment. Vamp will only check references at deployment time, this means:\n\nBreeds can be referenced in blueprints before they exist \nYou do not need to know the contents of an SLA when you reference it.\nYou can reference a variable that someone else should fill in.\n\nRead more about referencing artifacts and environment variables.\n\n{{ note title=\"What next?\" }}\nRead about Vamp breeds\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":29},{"path":"/documentation/using vamp/blueprints","date":"2016-09-13T09:00:00+00:00","title":"Blueprints","content":"\nBlueprints are execution plans - they describe how your services should be hooked up and what their topology should look like at runtime. This means you reference your breeds (or define them inline) and add runtime configuration to them.\n\nBlueprints allow you to add the following extra properties:\n\nGateways: a stable port where the service can be reached.\nClusters and services: a cluster is a grouping of services with one purpose, i.e. two versions (a/b) of one service.\nEnvironment variables: a list of variables (interpolated or not) to be made available at runtime.\nDialects: a dialect is a set of native commands for the underlying container platform, i.e. Docker or Mesosphere Marathon.\nScale: the CPU and memory and the amount of instance allocate to a service.\nConditions: how traffic should be directed based on HTTP and/or TCP properties.\nSLA and escalations: SLA definition that controls autoscaling.\n\nExample - key concepts of blueprints\n\n","id":30},{"path":"/documentation/using vamp/breeds","date":"2016-09-13T09:00:00+00:00","title":"Breeds ","content":"Breeds are static descriptions of applications and services available for deployment. Each breed is described by the DSL in YAML notation or JSON, whatever you like. This description includes name, version, available parameters, dependencies etc.\nTo a certain degree, you could compare a breed to a Maven artifact or a Ruby Gem description.\n\nBreeds allow you to set the following properties:\n\nDeployable: the name of actual container or command that should be run.\nPorts: a map of ports your container exposes.\nEnvironment variables: a list of variables (interpolated or not) to be made available at runtime.\nDependencies: a list of other breeds this breed depends on.\n\nDeployable\n\nDeployables are pointers to the actual artifacts that get deployed. Vamp supports Docker containers or can support any other artifacts supported by your container manager. \n\n Example breed - deploy a Docker container\n\n","id":31},{"path":"/documentation/using vamp/conditions","date":"2016-09-13T09:00:00+00:00","title":"Conditions","content":"\nCreating conditions is quite easy. Checking Headers, Cookies, Hosts etc. is all possible.\nUnder the hood, Vamp uses Haproxy's ACL's (cbonte.github.io/haproxy-dconv - 7.1 ACL basics) and you can use the exact ACL definition right in the blueprint in the condition field of a condition.\n\nHowever, ACL's can be somewhat opaque and cryptic. That's why Vamp has a set of convenient \"short codes\"\nto address common use cases. Currently, we support the following:\n\n| description           | syntax                       | example                  |\n| ----------------------|:----------------------------:|:------------------------:|\n| match user agent      | user-agent == value          | user-agent == Firefox    |\n| mismatch user agent   | user-agent != value          | user-agent != Firefox    |\n| match host            | host == value                | host == localhost        |\n| mismatch host         | host != value                | host != localhost       |\n| has cookie            | has cookie value             | has cookie vamp          |\n| misses cookie         | misses cookie value          | misses cookie vamp       |\n| has header            | has header value             | has header ETag          |\n| misses header         | misses header value          | misses header ETag       |\n| match cookie value    | cookie name has value    | cookie vamp has 12345    |\n| mismatch cookie value | cookie name misses value | cookie vamp misses 12345 |\n| header has value      | header name has value   | header vamp has 12345    |\n| header misses value   | header name misses value | header vamp misses 12345 |\n\nAdditional syntax examples: github.com/magneticio/vamp - ConditionDefinitionParserSpec.scala.\n\nVamp is also quite flexible when it comes to the exact syntax. This means the following are all equivalent:\n\nIn order to specify plain HAProxy ACL, ACL needs to be between { }:\n\ncondition: \" hdr_sub(user-agent) Chrome \"\n\nHaving multiple conditions in a condition is perfectly possible. For example, the following condition would first check whether the string \"Chrome\" exists in the User-Agent header of a\nrequest and then it would check whether the request has the header\n\"X-VAMP-MY-COOL-HEADER\". So any request matching both conditions would go to this service.\n\n","id":32},{"path":"/documentation/using vamp/deployments","date":"2016-09-13T09:00:00+00:00","title":"Deployments","content":"\nA deployment is a \"running\" blueprint. Over time, new blueprints can be merged with existing deployments or parts of the running blueprint can be removed from it. Each deployment can be exported as a blueprint and \ncopy / pasted to another environment, or even to the same environment to function as a clone.\n\nCreate a deployment\n\nYou can create a deployment in the following ways:\n\nSend a POST request to the /deployments endpoint.\nUse the UI to deploy a blueprint using the \"deploy\" button on the \"blueprints\" tab.\nUse the CLI vamp deploy command  \n $ vamp deploy my_blueprint.\n\nThe name of the deployment will be automatically assigned as a UUID (e.g. 123e4567-e89b-12d3-a456-426655440000).\n\n Vamp deployment process\n\nOnce we have issued the deployment, Vamp will do the following:\n\nUpdate Vamps internal model.\nIssue and monitor deployment commands to the container platform.\nUpdate the ZooKeeper entry.\nStart collecting metrics.\nMonitor the container platform for changes.\n\nVamp will add runtime information to the deployment model, like start times, resolved ports etc.\n\nDeployment scenarios\n\nA common Vamp deployment scenario is to introduce a new version of the service to an existing cluster, this is what we call a merge. After testing/migration is done, the old or new version can be removed from the cluster, simply called a removal. Let's look at each in turn.\n\n Merge\n\nMerging of new services is performed as a deployment update. You can merge in many ways:\n\nSend a PUT request to the /deployments/{deployment_name} endpoint.\nUse the UI to update a deployment using the \"Edit deployment\" button. \nUse the CLI with a combination of the vamp merge and vamp deploy commands.\n\nIf a service already exists then only the gateways and scale will be updated. Otherwise a new service will be added. If a new cluster doesn't exist in the deployment, it will be added.\n\nLet's deploy a simple service:\n\n","id":33},{"path":"/documentation/using vamp/environment-variables","date":"2016-09-13T09:00:00+00:00","title":"Environment variables ","content":"\nBreeds and blueprints can include lists of environment variables that will be injected into the container at runtime. You set environment variables with the environment_variables keyword or its shorter version env, e.g. both examples below are equivalent.\n\n","id":34},{"path":"/documentation/using vamp/escalations","date":"2016-09-13T09:00:00+00:00","title":"Escalations","content":"\nAn escalation is a workflow triggered by an escalation event. Vamp checks for these escalation events using a continuous background process with a configurable interval time. If the events match the escalation handlers defined in the DSL, the action is executed.\n\nAgain: escalation events can be generated by third party systems and they will be handled in the same manner as events created by Vamp SLA workflows. \n\nEscalation handlers\n\nAny escalation that is triggered should be handled by an escalation handler\n\nVamp ships with the following set of escalation handlers - scaleinstances, scalecpu and scale_memory. These handlers can be composed into intricate escalation systems.\n\n scale_instances   \nScales up the number of running instances. It is applied only to the first service in the cluster (old or \"A\" version). You can set upper limits to how far you want to scale out or in, effectively guaranteeing a minimum set of running instances. This is very much like AWS auto-scaling.  \nExample - scale_instances\n","id":35},{"path":"/documentation/using vamp/events","date":"2016-09-13T09:00:00+00:00","title":"Events","content":"\nVamp collects events on all running services. Interaction with the API also creates events, like updating blueprints or deleting a deployment. Furthermore, Vamp allows third party applications to create events and trigger Vamp actions.\n\nAll events are stored and retrieved using the Event API that is part of Vamp.\n\nExample - JSON \"deployment create\" event\n\n{\n  \"tags\": [\n    \"deployments\",\n    \"deployments:6ce79a33-5dca-4eb2-b072-d5f65e4eef8a\",\n    \"archiving\",\n    \"archiving:create\"\n  ],\n  \"value\": \"name: sava\",\n  \"timestamp\": \"2015-04-21T09:15:42Z\"\n}\n\n Basic event rules\n\nAll events stick to some basic rules:\n\nAll data in Vamp are events. \nValues can be any JSON object or it can be empty.\nTimestamps are in ISO8601/RFC3339.\nTimestamps are optional. If not provided, Vamp will insert the current time.\nTimestamps are inclusive for querying.\nEvents can be tagged with metadata. A simple tag is just single string.\nQuerying data by tag assumes \"AND\" behaviour when multiple tags are supplied, i.e. [\"one\", \"two\"] would only fetch records that are tagged with both.\nSupported event aggregations are: average, min, max and count.\n\nHow tags are organised\n\nIn all of Vamp's components we follow a REST (resource oriented) schema, for instance:\n/deployments/{deployment_name} \n/deployments/{deploymentname}/clusters/{clustername}/services/{service_name}\nTagging is done using a very similar schema: \"{resourcegroup}\", \"{resourcegroup}:{name}\". Some examples:\n\n\"deployments\", \"deployments:{deployment_name}\"\n\"deployments\", \"deployments:{deploymentname}\", \"clusters\", \"clusters:{clustername}\", \"services\", \"services\",services:{service_name} \"\n\nThis schema allows querying per group and per specific name. Getting all events related to all deployments is done by using tag \"deployments\". Getting events for specific deployment \"deployments:{deployment_name}\".\n\n Query events using tags\n\nUsing the tags schema and timestamps, you can do some powerful queries. Either use an exact timestamp or use special range query operators, described on the elastic.co site (elastic.co - Range query).\n\n{{ note title=\"Note!\" }}\nthe default page size for a set of returned events is 30.\n{{ /note }}\n\nExample queries\n\nGet all events\nResponse time for a cluster\nCurrent sessions for a service\nll known events for a service\n \n Example 1\nGet all events\n\nThe below query gets ALL metrics events up till now, taking into regard the pagination.\n\n{{ note title=\"Note!\" }}\nGET request with body - similar to approach used by Elasticsearch.\n{{ /note }}\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"metrics\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\nExample 2 \nResponse time for a cluster\n\nThe below query gets the most recent response time events for the \"frontend\" cluster in the \"d9b42796-d8f6-431b-9230-9d316defaf6d\" deployment.\n\nNotice the \"gateways:UUID\", \"metrics:responseTime\" and \"gateways\" tags. This means \"give me the response time of this specific gateway at the gateway level\". The response will echo back the events in the time range with the original set of tags associated with the events. \n\nGET /api/v1/events\n\n{\n  \"tags\": [\"routes:d9b42796-d8f6-431b-9230-9d316defaf6dfrontend8080\",\"metrics:rtime\",\"route\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\n[\n    {\n        \"tags\": [\n            \"gateways\",\n            \"gateways:d9b42796-d8f6-431b-9230-9d316defaf6dfrontend8080\",\n            \"metrics:rate\",\n            \"metrics\",\n            \"gateway\"\n        ],\n        \"value\": 0,\n        \"timestamp\": \"2015-06-08T10:28:35.001Z\",\n        \"type\": \"gateway-metric\"\n    },\n    {\n        \"tags\": [\n            \"gateways\",\n            \"gateways:d9b42796-d8f6-431b-9230-9d316defaf6dfrontend8080\",\n            \"metrics:rate\",\n            \"metrics\",\n            \"gateway\"\n        ],\n        \"value\": 0,\n        \"timestamp\": \"2015-06-08T10:28:32.001Z\",\n        \"type\": \"gateway-metric\"\n    }\n]    \n\n Example 3\nCurrent sessions for a service\n\nAnother example is getting the current sessions for a specific service, in this case the monarch_front:0.2 service that is part of the 214615ec-d5e4-473e-a98e-8aa4998b16f4 deployment and lives in the frontend cluster.\n\nNotice we made the search more specific by specifying the \"services\" and then \"service:SERVICE NAME\" tag.\nAlso, we are using relative timestamps: anything later or equal (lte) than \"now\".\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"routes:214615ec-d5e4-473e-a98e-8aa4998b16f4frontend8080\",\"metrics:scur\",\"services:monarch_front:0.2\",\"service\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\nExample 4\nAll known events for a service\n\nThis below query gives you all the events we have for a specific service, in this case the same service as in example 2. In this way you can get a quick \"health snapshot\" of service, server, cluster or deployment.\n\nNotice we made the search less specific by just providing the \"metrics\" tag and not telling the API which specific one we want.\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"routes:214615ec-d5e4-473e-a98e-8aa4998b16f4frontend8080\",\"metrics\",\"services:monarch_front:0.2\",\"service\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\n Server-sent events (SSE)\n\nEvents can be streamed back directly from Vamp.\n\nGET /api/v1/events/stream\n\nIn order to narrow down (filter) events, list of tags could be provided in the request body.\n\n{\n  \"tags\": [\"routes:214615ec-d5e4-473e-a98e-8aa4998b16f4frontend8080\",\"metrics\"]\n}\n\nGET method can be also used with tag parameter (may be more convenient):\n\nGET /api/v1/events/stream?tag=archiving&tag=breeds\n\nArchiving\n\nAll changes in artifacts (creation, update or deletion) triggered by REST API calls are archived. We store the type of event and the original representation of the artifact. It's a bit like a Git log. \n\nHere is an example event:\n\n{\n  \"tags\": [\n    \"deployments\",\n    \"deployments:6ce79a33-5dca-4eb2-b072-d5f65e4eef8a\",\n    \"archiving\",\n    \"archiving:delete\"\n  ],\n  \"value\": \"\",\n  \"timestamp\": \"2015-04-21T09:17:31Z\",\n  \"type\": \"\"\n}\n\nSearching through the archive is 100% the same as searching for events. The same tagging scheme applies.\nThe following query gives back the last set of delete actions executed in the Vamp API, regardless of the artifact type.\n\nGET /api/v1/events\n\n{\n  \"tags\": [\"archiving\",\"archiving:delete\"],\n    \"timestamp\" : {\n      \"lte\" : \"now\"\n    }\n}\n\n{{ note title=\"What next?\" }}\nRead about Vamp SLA (Service Level Agreement)\nCheck the API documentation\nTry Vamp\n{{ /note }}\n","id":36},{"path":"/documentation/using vamp/gateways","date":"2016-09-13T09:00:00+00:00","title":"Gateways","content":"\nGateways are dynmic runtime entities in the Vamp eco-system. They represent load balancer rules to deployment, cluster and service instances.\n\nThere are two types of gateways:\n\nInternal gateways: created automatically for each deployment cluster, updated using the gateway/deployment API\nExternal gateways: explicitly declared either in a deployment blueprint or using the gateway API\n\nThis is an example of automatically created gateway for deployment vamp, cluster sava and port port.\nCluster contains 2 services sava:1.0.0 and sava:1.1.0 with 2 running instances each. \n","id":37},{"path":"/documentation/using vamp/references","date":"2016-09-13T09:00:00+00:00","title":"Referencing artifacts","content":"\nWith any artifact, Vamp allows you to either use an inline notation or reference the artifact by name. For references, you use the reference keyword or its shorter version ref. Think of it like either using actual values or pointers to a value. This has a big impact on how complex or simple you can make any blueprint, breed or deployment. It also impacts how much knowledge you need to have of all the different artifacts that are used in a typical deployment or blueprint.\n\nVamp assumes that referenced artifcats (the breed called my_breed in the example below) is available to load from its datastore at deploy time. This goes for all basic artifacts in Vamp: SLA's, gateways, conditions, escalations, etc.\n\nExample - reference notation\n\ninline notation\n\n","id":38},{"path":"/documentation/using vamp/sla","date":"2016-09-13T09:00:00+00:00","title":"SLA (Service Level Agreement)","content":"\nSLA stands for \"Service Level Agreement\". Vamp uses it to define a pre-described set of boundaries to a service and the actions that should take place once the service crosses those boundaries. In essence, an SLA and its associated escalation is a workflow that is checked and controlled by Vamp based on the runtime behaviour of a service. SLAs and escalations are defined with the VAMP DSL.\n\nThe SLA event system\n\nYou can define an SLA for each cluster in a blueprint. A common example would be to check if the average response time of the cluster (averaged across all services) is higher or lower than a certain threshold. Under the hood, an SLA workflow creates two distinct events. These are are sent from Vamp and stored to Elasticsearch.\n\nEscalate for a specific deployment and cluster  \ne.g. if the response time is higher than the upper threshold.\nDeEscalate for a specific deployment and cluster  \ne.g. if the response time is lower than the lower threshold.\n\nSLA monitoring is a continuous background process with a configurable interval time. On each run an SLA workflow is executed for each deployment & cluster that has an SLA defined. Within the same SLA definition it's possible to define a list of escalations. Escalations are triggered by escalation events (Escalate/DeEscalate).\n\nThis means escalation events can be generated by the third party systems by sending them to Elasticsearch. This would allow scaling up or down to be triggered by basically any system that can POST a piece of JSON.\n\nSLA's are in essence pieces of code inside Vamp that stick to this event model and can use, if they want, the metrics and event data streaming out of Elasticsearch to make decisions on how things are and should be running.\n\n SLA types\n\nVamp currently ships with the following SLA types:\n\nresponsetimesliding_window\n\nResponse time with sliding window \n\nThe responsetimesliding_window SLA triggers events based on response times. \n\n Example - SLA defined inline in a blueprint.\n\nNotice the SLA is defined at the cluster level and acts on the first service in the cluster.\n\nNotice how the SLA is defined separately from the escalations. This is key to how Vamp approaches SLA's and how modular and extendable the system is.\n\n","id":39},{"path":"/documentation/using vamp/sticky-sessions","date":"2016-09-13T09:00:00+00:00","title":"Sticky Sessions","content":"\nVamp supports route and instance level sticky sessions.\n\nRoute Level\n\nA common use case is when the end users have to have the same experience in A/B testing setup thus they should get the same service always (either A or B).\n\n","id":40},{"path":"/documentation/using vamp/virtual-hosts","date":"2016-09-13T09:00:00+00:00","title":"Virtual Hosts","content":"\nVamp can be configured to support virtual host via HAProxy:\n\nvamp.operation.gateway {\n    virtual-hosts = true\n    virtual-hosts-domain = \"vamp\"\n}\n\nExample - Virtual hosts\n \n PUT ${VAMP_URL}/api/v1/deployments/runner with body:\n\n","id":41},{"path":"/documentation/using vamp/workflows","date":"2016-09-13T09:00:00+00:00","title":"Workflows ","content":"\nA \"workflow\" is an automated change of the running system and its deployments and gateways. \nChanging the number of running instances based on metrics (e.g. SLA) is an example of a workflow. \nA workflow can be seen as a recipe or solution, however it has a more generic meaning not just related to \"problematic\" situations.\n\nAnother example is a workflow that will decide automatically if a new version, when doing a canary release, should be accepted or not. \nFor instance, push the route up to 50% of traffic to the new version, compare metrics over some time (e.g. frequency of 5xx errors, response time), change to 100% and remove the old version. \nThis workflow could define the rate of the transitions (e.g. 5% - 10% - 25%, ...) as well.\n\nRationale\n\nWorkflows allow closing the feedback loop: deploy, measure, react.\nVamp workflows are based on running separate services (breeds) and in its simplest form scripting can be used - e.g. application/javascript breeds. \nScripting allows experimentation with different features and if the feature is common and generic enough, it could be supported later in Vamp DSL.\nSince workflows are running breeds in similar way as in deployments (blueprints), all other breed features are supported - ports, environment variables etc.\n\n Workflow API\n\nEach workflow is represented as an artifact and they follow basic CRUD operation patterns as any other artifact:\n  /api/v1/workflows\n\nEach workflow has to have:\n\n name\n breed - either reference or inline definition, similar to blueprints\n schedule \n scale - optional\n environment_variables (or env)- overrides breed environment variables\n arguments- Docker arguments, overrides default configuration arguments and breed arguments\n\nExample:\n\n","id":42},{"path":"/index.json","title":"","date":"","content":"---\ndate: 2016-08-02T01:13:07+02:00\ntype: json\nurl: index.json\n---","id":43},{"path":"/","date":"2016-09-13T09:00:00+00:00","title":"Canary releasing and autoscaling for microservice systems","type":"index","content":"try Vamp\nlearn more\n\n--------","id":44},{"path":"/instructions/","date":"2016-08-01T12:53:48+02:00","title":"Instructions for creating content","weight":"10","content":"\nAdding content to an existing section (menu)\n\nLet's create our first content file for your documentation. \nOpen a terminal and add the following command for each new file you want to add. \nReplace section-name with a general term that describes your document in detail.\n\nhugo new section-name/filename.md\n\nVisitors of your website will find the final document under www.example.com/section-name/filename/.\n\nTo be properly added to the menu, file should contains menu section, e.g.:\n\nmenu:\n  main:\n    parent: Examples\n    identifier: Example 1\n    weight: 10\n\nCheck out content of Examples.\n\n Creating section (menu)\n\nOpen config.toml file and append new section (menu) definition.\nLet's add Getting started submenu. Add the following after the Examples definition (end of the file):\n\n[[menu.main]]\n\tname   = \"Getting started\"\n\turl    = \"getting-started/\"\n\tweight = 20\n\nFields:\nname - displayed name\nurl - section URL, should be the same as directory name\nweight - order of the submenu\n\nNow let's add the index page:\n\nhugo new getting-started/index.md\n\nAdding a new API version\n\nAPI section (menu) is specific because of custom rendering.\nIn order to ad a new API version docs, go to /content/api and copy (branch) one of the existing versions.\nFor instance let's copy 0.9.0 to 0.9.1 directory.\n\n$ tree content/api\n\ncontent/api\n├── 0.8.5\n│   ├── example1.md\n│   └── index.md\n├── 0.9.0\n│   ├── example2.md\n│   └── index.md\n├── 0.9.1\n│   ├── example2.md\n│   └── index.md\n└── master\n    ├── example1.md\n    ├── example2.md\n    └── index.md\n\nNow change all 0.9.0 occurrences in 0.9.1 files to 0.9.1. \nThis will also prevent Hugo to complain about duplicate page identifiers.\nIf you check API menu, 0.9.1 version should also appear (you may need to rerun hugo serve --watch).\n\n Table of contents\n\nYou maybe noticed that the menu on the left contains a small table of contents of the current page. All h2 tags (## Headline in Markdown) will be added automatically.\n\nAdmonitions\n\nAdmonition is a handy feature that adds block-styled side content to your documentation, for example hints, notes or warnings. It can be enabled by using the corresponding shortcodes inside your content:\n\n{{/* note title=\"Note\" */}}\nNothing to see here, move along.\n{{/* /note */}}\n\nThis will print the following block:\n\n{{ note title=\"Note\" }}\nNothing to see here, move along.\n{{ /note }}\n\nThe shortcode adds a neutral color for the note class and a red color for the warning class. You can also add a custom title:\n\n{{/* warning title=\"Don't try this at home\" */}}\nNothing to see here, move along.\n{{/* /warning */}}\n\nThis will print the following block:\n\n{{ warning title=\"Don't try this at home\" }}\nNothing to see here, move along.\n{{ /warning }}\n\nUsing the tip keyword makes a tip box\n\n{{ tip title=\"Don't try this at home\" }}\nNothing to see here, move along.\n{{ /tip }}\n\n","id":45},{"path":"/resources/community","date":"2016-09-13T09:00:00+00:00","title":"Join the Vamp community","content":"Vamp is an open source project, actively developed by Magnetic.io. We encourage anyone to pitch in with pull requests, bug reports etc. \n\nContribute to Vamp \nVamp is split into separate repos and projects. Check the source on Github for an overview of all key repos (github.com - magneticio).   \nFeel free to contribute with Github pull requests.\n\n Submit change or feature requests \nLet us know your change or feature requests.  \nSubmit an issue on github tagged \"feature proposal\". \n\nReport a bug \nif you find  bug, please report it!  \nSubmit an issue on github, including details of the environment you are running Vamp in.\n","id":46},{"path":"/resources/downloads/","date":"2016-09-13T09:00:00+00:00","title":"Downloads","content":"\nBinaries\nVamp\nVamp Gateway Agent (VGA)\nVamp CLI\n\n Homebrew\nVamp CLI for MacOS X\n\nDocker images\nVamp Gateway Agent (VGA) and HAProxy\nVamp workflow agent\n\n Build from source\nBuild Vamp\nBuild Vamp Gateway Agent (VGA)\n  \n--------","id":47},{"path":"/support","date":"2016-09-13T09:00:00+00:00","title":"support   ","content":"Vamp is an open source project, actively developed by Magnetic.io. Vamp is Apache 2.0 licensed.\n\nCommunity support\nIf you have a question about Vamp, please check the Vamp documentation first  - we're always adding new resources, tutorials and examples.\n\n Bug reports\nIf you found a bug, please report it! Create an issue on GitHub and provide as much info as you can, specifically the version of Vamp you are running and the container driver you are using.\n\nGitter\nYou can post questions directly to us on our public Gitter channel  \n\n Twitter\nYou can also follow us on Twitter: @vamp_io\n\nProfessional support\nFor extended support, pricing information on the Vamp Enterprise Edition (EE), professional services or consultancy you can contact us at info@magnetic.io or call +31(0)88 555 33 99\n\n{{ note title=\"What next?\" }}\nTry Vamp\nLearn how Vamp works\nGet your teeth into the Vamp documentation\n{{ /note }}\n","id":48},{"path":"/why use vamp/enterprise-edition","date":"2016-09-13T09:00:00+00:00","title":"Vamp Enterprise Edition","content":"\nFor features, pricing and availability of our commercial Vamp Enterprise Edition (EE), please contact info@magnetic.io or call +31(0)88 555 33 99\n","id":49},{"path":"/why use vamp/faq","date":"2016-09-13T09:00:00+00:00","title":"frequently asked questions","content":"\nVamp and your company\nWhy use Vamp?   \nhow do you safeguard business goals after deployment?   \n\nVamp challenges you to rethink how you develop and release microservice based applications.   \n\n     \nWho is already using Vamp and how?\n\nWhat is canary testing/releasing?\n\nWill Vamp make that much difference to my company?  \nVamp brings clarity and flexibility to microservice upgrades and deployment monitoring. \n\n------","id":50},{"path":"/why use vamp/feature-list","date":"2016-09-13T09:00:00+00:00","title":"Feature list","content":"\nVamp 0.9.0 (Beta release) includes: \n\nContainer-scheduler agnostic API\nPercentage and condition based programmable routing\nYAML based configuration blueprints with support for dependencies, clusters and environment variables\nGraphical UI and dashboard\nIntegrated javascript-based workflow system \nMetric-driven autoscaling, canary releasing and other optimisation and automation patterns\nAutomatic loadbalancing for autoscaled services\nAPI gateway routing features like conditional rewrites\nCLI for integration with common CI/CD pipelines\nOpen source (Apache 2.0)\nEvent API and server-side events (SSE) stream\nMulti-level metric aggregation\nPort-based, virtual host names or external service (consul etc) based service discovery support\nLightweight design to run in high-available mission-critical architectures\nIntegrates with ELK stack (Elastic Search, Logstash, Kibana) for custom Kibana dashboards\nVamp Runner provides automated integration and workflows testing \n\nDid you know?\n\nVamp is platform agnostic\nVamp is functionality agnostic, but functions well in an API centric, event driven and stateless environment. \nVamp is not a strict container platform, but uses the power of container platforms under the hood.\nVamp is written in Scala, Go and ReactJS \nVamp includes clear SLA management and service level enforcement out of the box\nVamp is an open source project, actively developed by Magnetic.io\n\n{{ note title=\"What next?\" }}\nTry Vamp\nUse cases: Vamp solutions to practical problems\nWhat Vamp offers compared to other tools and services\nHow Vamp works\n{{ /note }}","id":51},{"path":"/why use vamp/get-started","date":"2016-09-13T09:00:00+00:00","title":"Get started","content":"\nTry Vamp\nTo make the most of your 'my first Vamp' experience, we suggest you start by installing our single, all-in-one Vamp Docker Hello World package. This will set up everything you need to play around with Vamp on a local or remote dev machine - the container package includes Mesos/Marathon and Elastic Stack (ELK), as well as all the necessary Vamp components. We have some nice tutorials to get a feel for the supernatural powers of Vamp.\n\n Install a production-grade Vamp setup\nOf course our Hello World package is no production-grade setup. We suggest your next step should be to understand the Vamp architecture and then find the Vamp version for your favorite container scheduler. We support most common container schedulers, so you should be able to find one to your liking in our installation docs. If you're still not sure which container scheduler to work with, our 'what to choose' guide can help you make an informed decision.\n\nFine tune and integrate\nAfter you've successfully installed a production-grade Vamp on your preferred container-cluster manager/scheduler (if you need help here, find us in our public Gitter channel), it's time to either dive into the ways you can use Vamp or investigate how you can configure and fine-tune Vamp to match your specific requirements. It might also be interesting to integrate Vamp into your CI pipeline to create a CD pipeline with Vamp's canary-releasing features. You can check out our CLI and REST API documentation for integrations.\n\n Get your teeth into the fun stuff!\nAt this point you've become a real Vamp guru. The next step could be to start playing around with our Vamp Runner tool to investigate typical recipes, such as automated canary-releasing, auto-scaling and more. You can use the JavaScript-based workflows in the recipes as a reference to create your own recipes and workflows. Once you've created some cool workflows and recipes we would of course like to hear from you!\n\n{{ note title=\"What next?\" }}\nTry the Vamp Docker Hello World package and tutorials\nRead about the Vamp architecture\nCheck the installation docs\n{{ /note }}","id":52},{"path":"/why use vamp/","date":"2016-09-13T09:00:00+00:00","title":"Why use Vamp?","content":"\nWe recognise the pain and risk involved with delivering microservice applications.  We've been there too - facing downtime and unexpected issues while transitioning from one release to the next. \nIn microservice architectures, these concerns can quickly multiply. It's all too easy to get stuck dealing with the added complexities and miss out on the potential benefits. \n\nWhat is Vamp?\n\nVamp is an open source, self-hosted platform for managing (micro)service oriented architectures that rely on container technology. Vamp provides a DSL to describe services, their dependencies and required runtime environments in blueprints and a runtime/execution engine to deploy these blueprints (similar to AWS Cloudformation). Planned deployments and running services can be managed from your choice of Vamp interface - graphical UI, command line interface or RESTful API. \n\nAfter deployment, Vamp workflows continue to monitor running applications and can act automatically based on defined SLAs.  You can use Vamp to orchestrate complex deployment patterns, such as architecture level A/B testing and canary releases. Vamp will take care of all the heavy lifting, such as route updates, metrics collection and service discovery.\n\n Vamp facts\n\nVamp is platform agnostic\nVamp is functionality agnostic, but functions well in an API centric, event driven and stateless environment. \nVamp is not a strict container platform, but uses the power of container platforms under the hood.\nVamp is written in Scala, Go and ReactJS \nVamp includes clear SLA management and service level enforcement out of the box\nVamp is an open source project, actively developed by Magnetic.io\n\n{{ note title=\"What next?\" }}\nTry Vamp\nUse cases: Vamp solutions to practical problems\nWhat Vamp offers compared to other tools and services\nHow Vamp works\n{{ /note }}\n\n","id":53},{"path":"/why use vamp/see-vamp-in-action","date":"2016-09-13T09:00:00+00:00","title":"See Vamp in action","content":"\n","id":54},{"path":"/why use vamp/use cases/create-responsive-website","date":"2016-09-13T09:00:00+00:00","title":"Canary test and release a responsive frontend","content":"\n“We need to upgrade our website frontend to make it responsive”\n  \nDeveloping a responsive web frontend is often a major undertaking, requiring a large investment of hours and extensive testing. Until you go live, it's difficult to predict how the upgrade will be received by users - will it actually improve important metrics, will it work on all browser, devices and resolutions, etc.?   \n\nBut why develop this new responsive frontend in one go, having to go for the dreaded and risky big-bang release? Using Vamp you can apply a canary release to introduce the new frontend to a selected cohort of users, browsers and/or devices. This would require a minimal investment of development and delivering real usage data:\n\nStart small: Build the new frontend for only one specific browser/resolution first to measure effectiveness. Vamp can deploy the new responsive frontend and route a percentage of supported users with this specific browser and screen-resolution there. All other users will continue to see the old version of your website.\nOptimise: With the new responsive frontend in the hands of real users with this specific browser/resolution, you can measure actual data and optimise accordingly without negatively affecting the majority of your users.\nScale up: Once you are satisified with the performance of the new frontend, you can use Vamp to scale up the release, developing and canary releasing one browser/resolution at a time. Of course other cohort combinations are also possible, Vamp is open and supports all HAproxy ACL rules.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to resolve client-side incompatibilities after an upgrade\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":55},{"path":"/why use vamp/use cases/","date":"2016-09-13T09:00:00+00:00","title":"use cases","content":"\nThe integrated deployment, routing and workflow features of Vamp support a broad range of scenarios and industry verticals. We specifically see powerful use cases in the areas of testing in production, migrating to microservices, and realtime system optimisation. In this section we describe specific scenarios and how Vamp can effectively solve these.\n\nTesting in production \nUse canary testing and releasing to introduce a responsive web frontend. Read more ...\nResolve client-side incompatibilities after an upgrade. Read more ...\nA/B test architectural changes in production. Read more ...\n\n Migrating to microservices\nMove from VM based monoliths to modern microservices. Read more ...\n\nRealtime system optimisation\n\nWhat would happen if...... Simulate and test auto scaling behaviour. Read more ...\nSelf-healing and self-optimising. Read more ...\n\nWe're always interested to hear specific use-cases. If you have one to share, send us an email at info@magnetic.io","id":56},{"path":"/why use vamp/use cases/modernise-architecture","date":"2016-09-13T09:00:00+00:00","title":"Test and modernise architecture","content":"\n\"We want to switch to a NoSQL database for our microservices, but don't know which solution will run best for our purposes\"\n\nWith multiple NoSQL database options available, it's hard to know which is the best fit for your specific circumstances. You can try things out in a test lab, but the real test comes when you go live with production load.\n\nWhy guess? Using Vamp you could A/B test different versions of your services with different NoSQL backends, in production, and then use real data to make an informed and data-driven decision.   \n\nDeploy two versions: Vamp can deploy multiple versions of your architecture, each with a different database solution (or other configuration settings), then distribute incoming traffic across each.\nStress test: Use the metrics reported by Vamp to measure which option performs best in production.\nKeep the best performing option: Once you have made your decision, Vamp can route all traffic to your chosen architecture. Services from the alternative options will be drained to ensure customer experience is not impacted by the test.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to simulate and test scaling behaviour\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":57},{"path":"/why use vamp/use cases/refactor-monolithic-to-microsystems","date":"2016-09-13T09:00:00+00:00","title":"Move from monoliths and VM's to microservices","content":"\n“We want to move to microservices, but we can’t upgrade all components at once and want to do a gradual migration”\n  \nRefactoring a monolithic application to a microservice architecture is a major project. A big bang style re-write to upgrade all components at once is a risky approach and requires a large investment in development, testing and refactoring.\n\nWhy not work incrementally? Using Vamp's routing you could introduce new services for specific application tiers like the frontend or business logic layers, and move traffic with specific conditions to these new services. These services in turn can connect to your legacy systems again, using Vamp's proxying. A typical example would be introducing an angular based frontend or node.js based API microservice. You can send 2% of your incoming traffic to this new microservice frontend, which in turn connects with the legacy backend system. This way you can test your new microservices in a small and controlled way and avoid a big bang release. You can introduce new services one by one, test them in production and increase traffic until you migrated your entire application from a monolith to microservices.\n\nStart small: You can build e.g. one new frontend component. Vamp will deploy this to run alongside the legacy monolithic system.\nActivate smart routing: Vamp can route traffic behind the scenes, so a small percentage of visitors is sent to the new frontend service, while the new frontend is routed by Vamp to the legacy backend. You can continue transferring components from the legacy monolithic system to new microservices and Vamp can adapt the routing as you go.\nRemove legacy components: Once all services have been transferred from the legacy monolith, you can start removing components from the legacy system.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to test and modernise architecture\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":58},{"path":"/why use vamp/use cases/resolve-incompatibilities-after-upgrade","date":"2016-09-13T09:00:00+00:00","title":"Resolve client-side incompatibilities after an upgrade","content":"\n“We upgraded our website self-management portal, but our biggest client is running an unsupported old browser version”\n  \nLeaving an important client unable to access your services after a major upgrade is a big and potentially costly problem. The traditional response would be to rollback the upgrade asap - if that's even possible.  \n\nWhy rollback? Using Vamp's smart conditional routing you could send specific clients or browser-versions to an older version of your portal while others can enjoy the benefits of your new upgraded portal. Because Vamp supports SLA based autoscaling for (Docker) containers, you can deploy the old version on the same infrastructure as the new version is running on. This also avoids having to provision costly over dimensioned DTAP environments for only a small user-base, leveraging your existing infrastructure efficiently.\n\nRe-deploy: Vamp can (re)deploy a (containerised) compatible version of your portal to run side-by-side with the upgraded version.\nActivate smart routing: Vamp can route all users with e.g. a specific IP, browser or location to a compatible version of the portal. Other clients will continue to see the new upgraded portal.\nResolve the incompatibility: Once the client upgrades to a compatible browser Vamp can automatically route them to the new portal version.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp to move from monoliths and VMs to microservices\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":59},{"path":"/why use vamp/use cases/self-healing-and-self-optimising","date":"2016-09-13T09:00:00+00:00","title":"Self-healing and self-optimising","content":"\n\"Our website traffic can be unpredictable, it's hard to plan and dimension the exact resources we're going to need to run within SLA's\"\n\nWhy overdimension your whole system? Using Vamp you can auto-scale individual services based on clearly defined SLAs (Service Level Agreements). It's also easy to create advanced workflows for up and down scaling, based on your application or business specific requirements. Vamp can also make sure that unhealthy and failing services are corrected based on clearly defined metrics and treshholds.\n\nSet SLAs: You can define SLA metrics, tresholds and escalation workflows. You can do this in Vamp YAML blueprints, modify our packaged workflows, or create your own workflow scripts for advanced use-cases.\nOptimise: Vamp workflows can automatically optimise your running system based on metrics that are relevant to your application or services.\nSleep easy: Vamp will track troughs and spikes in activity and automatically scale services up and down to match your SLAs. All scaling events will be logged. Unhealthy services can be healed by Vamp.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp for service discovery\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":60},{"path":"/why use vamp/use cases/service-discovery","date":"2016-09-13T09:00:00+00:00","title":"Service discovery","content":"\nVamp uses a service discovery pattern called server-side service discovery. This pattern allows service discovery without the need to change your code or run any other service-discovery daemon or agent. In addition to service discovery, Vamp also functions as a service registry. We recognise the following benefits of this pattern:\n\nNo code injection needed\nNo extra libraries or agents needed\nPlatform/language agnostic: it’s just HTTP\nEasy integration using ENV variables\n\nVamp doesn't need point-to-point wiring. Vamp uses environment variables that resolve to service endpoints Vamp automatically sets up and exposes. Even though Vamp provides this type of service discovery, it does not put any constraint on other possible solutions. For instance services can use their own approach specific approach using service registry, self-registration etc. Vamp can also integrate with common service discovery solutions like Consul and read from these to setup the required routing automatically.\n\nSmartStack\nVamp can automatically deploy a VampGatewayAgent(VGA)+HAproxy on every node of your container cluster. This creates a so-called Layer 7 intra service network mesh which enables you to create a \"SmartStack\", an automated service discovery and registration framework originally coined and developed by AirBnB. More on the history and advantages of the SmartStack approach can be read here (nerds.airbnb.com - SmartStack: Service Discovery in the Cloud).\n\n{{ note title=\"What next?\" }}\nRead more about Vamp service discovery\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}\n","id":61},{"path":"/why use vamp/use cases/simulate-and-test-scaling-behaviour","date":"2016-09-13T09:00:00+00:00","title":"Simulate and test scaling behaviour","content":"\n\"How would our system react if... the number of users increased x10 ... the response time of a service increased with 20 seconds ... an entire tier of our application would be killed ...\"  \nYour company might dream of overnight success, but what if it actually happened? Stress tests rarely cater to extreme real world circumstances and usage patterns, and are often done on systems that are not identical to production environments. It's not uncommon the bottleneck sits in the system generating the load itself, so it's difficult to predict how your microservices would actually scale or to know if your planned responses will really help.\n\nWhy not find out for sure? Using Vamp you can test your services and applications against difficult to predict or simulate situations, mocking all kinds of metrics, and then validate and optimise the workflows that handle the responses, like for example auto up and down scaling. With the same workflows as would be running in production, on the same infrastructure, with the same settings.\n\nMock required load: Vamp can simulate (mock) high-stress situations for any kind of metric your system needs to respond to, without actually having to generate real traffic.\nOptimise: You can optimise your resource allocation and autoscaling configurations based on real validated behaviour under stress.\nIterate until you're certain: Vamp can repeat the tests until you're confident with the outcome. Then you can use the same scaling and optimising workflows in production.\n\n{{ note title=\"What next?\" }}\nRead about using Vamp for self healing and self optimising\nSee how Vamp measures up to common platforms, tools and frameworks  \nFind out how Vamp works\n{{ /note }}","id":62},{"path":"/why use vamp/vamp-compared-to/frameworks-and-tools","date":"2016-09-13T09:00:00+00:00","title":"Frameworks and tools","content":"\nVamp compared to CI/CD tools\nSpinnaker, Jenkins, Wercker, Travis, Bamboo    \nVamp closes the loop between the development and operations elements of a CI/CD pipeline by enabling the controlled introduction of a deployable into production (canary-test and canary-release) and feeding back runtime technical and business metrics to power automated optimisation workflows (such as autoscalers). Vamp integrates with CI/CD tools like Travis, Jenkins or Wercker to canary-release and scale the built deployables they provide. The initial deployment setup is defined in a YAML blueprint (e.g. deployable details, required resources, routing filters) and is typically provided by the CI tool as a template to the Vamp API. Vamp will then run, canary-release, monitor and scale the deployment based on the filters and conditions specified in the blueprint.\n\n Vamp compared to feature toggle frameworks\nLaunchDarkly, Togglz, Petri  \nFeature toggle frameworks use code level feature toggles to conditionally test new functionality in an application or service. Tools such as LaunchDarkly and Togglz work with these toggles to enable, for example, A/B testing and canary functionality. While there are many cases for using feature toggles, there are also times when feature toggling isn't the smartest choice.\nVamp allows controlled testing of new features without the need to adjust your code. To achieve this, Vamp controls traffic routing towards and between your applications and services based on blueprint descriptions of individual (micro)services and their dependencies. This makes sense on an application code level, offering increased security and reduced technical debt compared to maintaining toggles in your code, and provides a mature alternative for cases when feature toggles are not the appropriate choice.\n\nVamp compared to configuration management and provisioning tools\nPuppet, Ansible, Chef, Terraform    \nThe responsbilities of configuration management and infrastructure provisioning tools are often stretched to cover container deployment features. These tools were not intended for handling container deployments or for the dynamic management and routing traffic over these containers. Vamp has been designed and developed from the ground up specifically to fit these use cases.  \n\n Vamp compared to custom built solutions\nBuilding and maintaining a scalable and robust enterprise-grade system for canary-testing and releasing is not trivial. Vamp delivers programmable routing and automatic load balancing, deployment orchestration and workflows, as well as a powerful event system, REST API, graphical UI, integration testing tools and a CLI.  \n\nVamp compared to A/B and MVT testing tools\nOptimizely, VisualWebsiteOptimizer, Google Analytics, Planout  \nVamp enables canary testing versions of applications, effectively providing A/B and MVT testing of applications and services by deploying two or more versions of an application or service and dividing incoming traffic between the running versions. Vamp doesn't have a built-in analytics engine though, so the analysing of the relevant metrics needs to be done with a specific Vamp workflow or an external analytics engine. Results can be fed back to Vamp to automatically update routing rules and deployments to push a winning version to a full production release. Because of the flexible programmable routing and use of environment variables, Vamp can be used to canary test almost everything, from content and business logic to configuration settings and architectural changes.  \n\n Vamp compared to DevOps tools\nDeis, Flynn, Dokku  \nVamp has no ambition to provide a Heroku-like environment for containers. Vamp integrates programmable routing and load balancing, container deployments and orchestration to enable canary testing and canary releasing features. Vamp also adds metrics-driven workflows for auto-scaling and other optimisations. Vamp sees business as a first class citizen in DevOps teams, providing a graphical UI and tools for non-technical roles.   \n\n{{ note title=\"What next?\" }}\nTry Vamp\nUse cases -  some Vamp solutions to practical problems\nFind out how Vamp works\n{{ /note }}\n\n","id":63},{"path":"/why use vamp/vamp-compared-to/paas-and-container-systems","date":"2016-09-13T09:00:00+00:00","title":"PaaS and container systems","content":"\nVamp compared to container schedulers and container clouds (CPaaS)\nDocker Swarm, DC/OS, Mesos/Marathon, Kubernetes, Nomad, Rancher, AWS ECS, Azure CS, Mantl, Apollo  \nContainer cluster managers and schedulers like Marathon, DC/OS, Kubernetes, Nomad or Docker Swarm provide great features to run containers in clustered setups. What they don't provide are features to manage the lifecycle of a microservices or container based system. How to do continuous delivery, how to gradually introduce and upgrade versions in a controlled and risk-free way, how to aggregate metrics and how to use these metrics to optimise and scale your running system. Vamp adds these features on top of well-used container schedulers by dynamically managing routing and load balancing, deployment automation and metric driven workflows. Vamp also adds handy features like dependencies, ordering of deployments and resource management.\n\n Vamp compared to PaaS systems\nCloud foundry, OpenStack, IBM Bluemix, Openshift  \nVamp adds an experimentation layer to PaaS infrastructures by providing canary-releasing features that integrate with common PaaS proxies like HAProxy. For continuous delivery and auto-scaling features, Vamp integrates with common container-schedulers included in PaaS systems, like Kubernetes in Openshift V3.   \n\n{{ note title=\"What next?\" }}\nRead about Vamp compared to common frameworks and tools\nTry Vamp\nFind out how Vamp works\n{{ /note }}\n\n","id":64},{"path":"/why use vamp/vamp-compared-to/proxies-and-load-balancers","date":"2016-09-13T09:00:00+00:00","title":"Proxies and load balancers","content":"\nVamp compared to software based proxies and load balancers\nHAproxy, NGINX, linkerd, Traefik   \nVamp adds programmable routing (percentage and condition based) and load balancing to the battle-tested HAProxy proxy, as well as a REST API, graphical UI and CLI.  This means you can use Vamp together with all common container-schedulers to provide continuous delivery and auto-scaling features using automatic load balancing and clustering of scaled out container instances. By default Vamp is packaged with HAProxy, but you could also integrate the Vamp magic with other programmable proxies such as NGINX, linkerd or Traefik.\n\n{{ note title=\"What next?\" }}\nRead about Vamp compared to PaaS and container systems\nTry Vamp\nFind out how Vamp works\n{{ /note }}\n\n","id":65}]